<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://xxyr.cc").hostname,root:"/",scheme:"Gemini",version:"7.6.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"manual"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="《Python深度学习》笔记"><meta property="og:type" content="article"><meta property="og:title" content="《Python深度学习》笔记"><meta property="og:url" content="https://xxyr.cc/post/DeepLearning/deep-learning-notes/index.html"><meta property="og:site_name" content="不负骤雨"><meta property="og:description" content="《Python深度学习》笔记"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/05/16/iweKFoO9k862ufD.png"><meta property="og:image" content="https://i.loli.net/2020/05/16/zomZXBxty67UqEN.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/18/Yf6u1H.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/18/YhClMF.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/18/YhipjS.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/19/Y4DYO1.jpg"><meta property="og:image" content="https://i.loli.net/2020/05/16/zomZXBxty67UqEN.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/20/YTstRf.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/20/YTsgzT.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/21/YbAsqs.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/21/YbA6Zn.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/21/YqbxS0.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/21/YqqSyT.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/17/Y20vQK.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/17/Y2yrUe.png"><meta property="og:image" content="https://s1.ax1x.com/2020/05/17/Y24DV1.png"><meta property="article:published_time" content="2020-05-14T12:47:17.000Z"><meta property="article:modified_time" content="2020-05-30T06:40:44.181Z"><meta property="article:author" content="不负骤雨"><meta property="article:tag" content="Python"><meta property="article:tag" content="Deep Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://i.loli.net/2020/05/16/iweKFoO9k862ufD.png"><link rel="canonical" href="https://xxyr.cc/post/DeepLearning/deep-learning-notes/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>《Python深度学习》笔记 | 不负骤雨</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" style="margin:10px"><div class="container use-motion"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">不负骤雨</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">围城</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">14</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">16</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://xxyr.cc/post/DeepLearning/deep-learning-notes/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210207-162535-0914.jpg"><meta itemprop="name" content="不负骤雨"><meta itemprop="description" content="reading coding keeping"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="不负骤雨"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">《Python深度学习》笔记</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-05-14 20:47:17" itemprop="dateCreated datePublished" datetime="2020-05-14T20:47:17+08:00">2020-05-14</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-05-30 14:40:44" itemprop="dateModified" datetime="2020-05-30T14:40:44+08:00">2020-05-30</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span> </a></span></span><span id="/post/DeepLearning/deep-learning-notes/" class="post-meta-item leancloud_visitors" data-flag-title="《Python深度学习》笔记" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论次数：</span> <a title="valine" href="/post/DeepLearning/deep-learning-notes/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/post/DeepLearning/deep-learning-notes/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>《Python深度学习》笔记<a id="more"></a><br><a href="#jump">Windows下的环境搭建</a>，<br>英文电子版<a href="https://livebook.manning.com/book/deep-learning-with-python/" target="_blank" rel="noopener">https://livebook.manning.com/book/deep-learning-with-python/</a><br>Keras中文文档：<a href="https://keras.io/zh/" target="_blank" rel="noopener">https://keras.io/zh/</a></p><h2 id="Chapter-1-基本概念"><a href="#Chapter-1-基本概念" class="headerlink" title="Chapter 1 基本概念"></a>Chapter 1 基本概念</h2><p>首先理清人工智能、机器学习和深度学习的概念和关系，<br>人工智能&gt;机器学习&gt;深度学习<br>深度学习是机器学习的子集，而机器学习则是人工智能的子集</p><h3 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h3><p><strong>简洁定义</strong>：将通常由人类完成的智力任务自动化。<br><strong>发展</strong>：<em>符号主义人工智能</em>（symbolic AI）→<em>机器学习</em>（machine learning）<br><strong>符号主义人工智能</strong>：编写足够多的明确规则来处理知识，用于解决定义明确的逻辑问题，如下国际象棋，但对于图像分类、语音识别等难以给出明确规则的复杂、模糊的问题将无法解决。</p><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><p><img src="https://i.loli.net/2020/05/16/iweKFoO9k862ufD.png" alt=""><br>　　机器学习将某个任务相关的示例输入机器学习系统，系统从中找到统计结构，最终找到规则将任务自动化，其与经典程序设计不同如上图。<br>　　机器学习的三个要素如下：<br>　　（1）输入数据点。<br>　　（2）预期输出的示例。<br>　　（3）衡量算法效果好坏的方法。<br>　　机器学习和深度学习的核心问题在于<strong>有意义地变换数据</strong> ，即学习输入数据的有用<strong>表示</strong>（表征数据或将数据编码），如彩色图像可以编码为RGB（红-绿-蓝）格式或HSV（色相-饱和度-明度）格式，在应对不同任务时，不同的表示方式将会产生很大的差异，所以机器学习模型的目的就是为输入数据寻找合适的表示。<br>　　<strong>定义</strong>：在预先定义好的可能性空间中（假设空间），利用反馈信号的指引来寻找输入数据的有用表示。</p><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><p>　　深度学习强调从连续的<strong>层</strong>中进行学习，这些层对应越来越有意义的<strong>表示</strong>。<br>　　模型包含的层数成为模型的<strong>深度</strong>，这些分层总是通过<strong>神经网络</strong>（neural network）模型学习得到。<br>　　<strong>定义</strong>：学习数据表示的多级方法，相当于多级信息蒸馏操作。<br>　　<strong>深度学习工作原理</strong>：<br>　　神经网络每层对输入数据所做的具体操作保存在该层的<strong>权重</strong>中，其本质是一串数字，每层实现的变换由其权重来<strong>参数化</strong>，<strong>学习</strong>即为每层找到一组权重值，使得该网络能够将每个示例输入与其目标正确地一一对应。<br>　　输入网络预测值和真实目标值通过<strong>损失函数</strong>计算一个距离值，利用这个距离值作为反馈信号通过<strong>优化器</strong>实现<strong>反向传播算法</strong>来对权重值进行微调，以降低当前示例的损失值，随着示例的增多，损失值逐渐降低，输出值与目标值尽可能接近。如下图。<br><img src="https://i.loli.net/2020/05/16/zomZXBxty67UqEN.png" alt="深度学习工作原理图"></p><h2 id="Chapter-2-神经网络的数学基础"><a href="#Chapter-2-神经网络的数学基础" class="headerlink" title="Chapter 2 神经网络的数学基础"></a>Chapter 2 神经网络的数学基础</h2><h3 id="张量（tensor）"><a href="#张量（tensor）" class="headerlink" title="张量（tensor）"></a>张量（tensor）</h3><p>定义：又叫多维Numpy数组，作为机器学习的基本数据结构，是一个数据容器，包含的数据几乎总是数值数据，如矩阵是二维张量，张量是矩阵向任意维度的推广[ 张量的<strong>维度</strong>（dimension）通常叫做<strong>轴</strong>（axis）]</p><h3 id="标量（scalar）"><a href="#标量（scalar）" class="headerlink" title="标量（scalar）"></a>标量（scalar）</h3><p>定义：仅包含一个数字的张量，又叫标量张量、零维张量、0D张量。在Numpy数组中，一个float32或float64的数字就是一个标量。<br>标量张量有0个轴（ndim == 0），轴的个数又叫<strong>阶</strong>（rank）。如下代码查看一个标量张量的轴的个数：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array(<span class="number">12</span>)</span><br><span class="line">print(x.ndim)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="向量（vector）"><a href="#向量（vector）" class="headerlink" title="向量（vector）"></a>向量（vector）</h3><p>定义：数字组成的数组，又叫一维张量或1D张量，只有一个轴，如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>])</span><br><span class="line">print(x.ndim)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure><p>该向量有5个元素，为5D向量，有1个轴，沿着轴有5个维度。而5D张量有5个轴，沿着某个轴可能有任意个维度。</p><h3 id="矩阵（matrix）"><a href="#矩阵（matrix）" class="headerlink" title="矩阵（matrix）"></a>矩阵（matrix）</h3><p>定义：多个向量组成的数组，又叫二维张量或2D张量，有2个轴，如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>]])</span><br><span class="line">print(x.ndim)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><p>第一个轴上的元素叫行，[12, 3, 22, 121, 4]是x的第一行，第二个轴上的元素叫列，[12, 12, 12]是x的第一列。</p><h3 id="3D张量与更高维张量"><a href="#3D张量与更高维张量" class="headerlink" title="3D张量与更高维张量"></a>3D张量与更高维张量</h3><p>定义：多个矩阵组成的数组，有3个轴，如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[[<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>]],</span><br><span class="line">            [[<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>]]])</span><br><span class="line">print(x.ndim)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><p>深度学习一般处理0D到4D张量，处理视频数据可能会遇到5D张量。</p><h3 id="张量的关键属性"><a href="#张量的关键属性" class="headerlink" title="张量的关键属性"></a>张量的关键属性</h3><ul><li><strong>轴的个数</strong>（阶、维度）：Python库中为ndim。</li><li><strong>形状</strong>：张量沿某个轴的维度大小（元素个数），如前面的3D张量的形状为（3， 3， 5）。</li><li><strong>数据类型</strong>：张量所包含数据的类型，Python库中为dtype，如float32、float64、unit8等，极少数情况有字符（char）张量，Numpy等大多数库都不存在字符串张量，因为<span style="border-bottom:2px solid red">张量存储在预先分配的连续内存段中</span>，而字符串长度可变，无法用这种方式存储。</li></ul><p>代码示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line">print(train_images.shape)</span><br><span class="line">print(train_images.ndim);</span><br><span class="line">print(train_images.dtype);</span><br><span class="line"></span><br><span class="line"><span class="comment">#该3D张量中的第四个数字</span></span><br><span class="line">digit = train_images[<span class="number">4</span>]</span><br><span class="line">plt.imshow(digit, cmap=plt.cm.binary)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="number">3</span></span><br><span class="line">uint8</span><br><span class="line"></span><br><span class="line"><span class="comment">#由上可见，train_images是一个由8位整数组成的3维张量，即60000个28*28整数矩阵组成的数组，每个矩阵是一张灰度图像。</span></span><br></pre></td></tr></table></figure><p><img src="https://s1.ax1x.com/2020/05/18/Yf6u1H.png" alt="该3D张量中的第四个数字"></p><h3 id="张量切片"><a href="#张量切片" class="headerlink" title="张量切片"></a>张量切片</h3><p>类似于Python的切片操作，示例:</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#选择第10~100个数字（不包括100），将其放在形为（90，28，28）的数组中</span></span><br><span class="line">my_slice = train_images[<span class="number">10</span>:<span class="number">100</span>]     <span class="comment">#[10:100, 0:28, 0:28]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#可沿着张量轴在任意两个索引间进行选择，也可使用负索引</span></span><br><span class="line">my_slice = train_images[, <span class="number">14</span>:, <span class="number">14</span>:]       <span class="comment">#右下角14*14</span></span><br><span class="line">my_slice = train_images[, <span class="number">7</span>:<span class="number">-7</span>, <span class="number">7</span>:<span class="number">-7</span>]       <span class="comment">#中心14*14</span></span><br></pre></td></tr></table></figure><h3 id="数据批量"><a href="#数据批量" class="headerlink" title="数据批量"></a>数据批量</h3><p>　　所有数据张量的第一个轴称为<strong>样本轴</strong>或<strong>0轴</strong>（samples axis）。<br>　　深度学习模型会将数据集拆分成小批量，形如<code>batch = train_images[:128]</code>。<br>　　对于这种批量张量，第一个轴为<strong>批量轴</strong>（batch axis）或<strong>批量维度</strong>（batch dimension）。</p><h3 id="现实世界中的数据张量"><a href="#现实世界中的数据张量" class="headerlink" title="现实世界中的数据张量"></a>现实世界中的数据张量</h3><ul><li><strong>向量数据</strong>：2D 张量，形状为 (samples, features) 。</li><li><strong>时间序列数据或序列数据</strong>：3D 张量，形状为 (samples, timesteps, features) 。</li><li><strong>图像</strong>：4D张量，形状为 (samples, height, width, channels) 或 (samples, channels,<br>height, width) 。</li><li><strong>视频</strong>：5D张量，形状为 (samples, frames, height, width, channels) 或 (samples,<br>frames, channels, height, width)</li></ul><h4 id="向量数据"><a href="#向量数据" class="headerlink" title="向量数据"></a>向量数据</h4><p>　　常见的数据。每个数据点被编码为一个向量，因此一个数据批量就被编码为 2D 张量（即向量组成的数组），其中第一个轴是<strong>样本轴</strong>，第二个轴是<strong>特征轴</strong> <span style="border-bottom:2px solid red">(samples, features)</span>。如<br>　　人口统计数据集，其中包括每个人的年龄、邮编和收入。每个人可以表示为包含 3 个值的向量，而整个数据集包含 100 000 个人，因此可以存储在形状为 (100000, 3) 的 2D张量中。</p><h4 id="时间序列数据或序列数据"><a href="#时间序列数据或序列数据" class="headerlink" title="时间序列数据或序列数据"></a>时间序列数据或序列数据</h4><p>　　当时间对于数据很重要时，将数据存储在带有时间轴的3D张量中。每个样本被编码为一个向量序列（即2D张量），因此一个数据批量就被编码为一个3D张量 <span style="border-bottom:2px solid red">(samples, timesteps, features)</span>。如图：<br><img src="https://s1.ax1x.com/2020/05/18/YhClMF.png" alt="">　　依据惯例，时间轴始终是第二个轴。<br>　　例如股票价格数据集：<br>　　要构建一个股票价格数据集：每一分钟，我们将股票的当前价格，前一分钟的最高价格和前一分钟的最低价格保存下来，那么就被编码为一个3D向量。整个交易日就编码为一个有390个3D向量的2D张量（390，3）。250天的数据就编码为3D张量（250，390，3）。</p><h4 id="图像数据"><a href="#图像数据" class="headerlink" title="图像数据"></a>图像数据</h4><p>　　图像通常具有三个维度：<strong>高度</strong>、<strong>宽度</strong>和<strong>颜色深度</strong>。虽然灰度图像（比如 MNIST 数字图像）只有一个颜色通道，因此可以保存在 2D 张量中，但按照惯例，图像张量始终都是 3D 张量，灰度图像的彩色通道只有一维。因此，如果图像大小为 256×256，那么 128 张灰度图像组成的批量可以保存在一个形状为 (128, 256, 256, 1) 的张量中，而 128 张彩色图像组成的批量则可以保存在一个形状为 (128, 256, 256, 3) 的张量中。<br><img src="https://s1.ax1x.com/2020/05/18/YhipjS.png" alt="">　　图像张量的形状有两种约定：通道在后（channels-last）的约定（在 TensorFlow 中使用）和通道在前（channels-first）的约定（在 Theano 中使用）。<br>　　Google 的 TensorFlow 机器学习框架将颜色深度轴放在最后：<span style="border-bottom:2px solid red"> (samples, height, width, color_depth)</span>,例如(128, 256, 256, 3) 。<br>　　Theano将图像深度轴放在批量轴之后：<span style="border-bottom:2px solid red">(samples, color_depth, height, width)</span>，例如(128, 3, 256, 256)。<br>　　Keras 框架同时支持这两种格式。</p><h4 id="视频数据"><a href="#视频数据" class="headerlink" title="视频数据"></a>视频数据</h4><p>　　视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧，每一帧都是一张彩色图像。由于每一帧都可以保存在一个形状为 (height, width, color_depth) 的 3D 张量中，因此一系列帧可以保存在一个形状为 (frames, height, width,color_depth) 的 4D 张量中，而不同视频组成的批量则可以保存在一个 5D 张量中，其形状为<span style="border-bottom:2px solid red">(samples, frames, height, width, color_depth) </span>。<br>　　例如，一个以每秒 4 帧采样的 60 秒 YouTube 视频片段，视频尺寸为 144×256，这个视频共有 240 帧。4 个这样的视频片段组成的批量将保存在形状为 (4, 240, 144, 256, 3)的张量中。总共有 106 168 320 个值！如果张量的数据类型（ dtype ）是 float32 ，每个值都是32 位，那么这个张量共有 405MB。现实生活中遇到的视频要小得多，因为它们不以float32 格式存储，而且通常被大大压缩，比如 MPEG 格式。</p><h3 id="张量运算——神经网络的“齿轮”"><a href="#张量运算——神经网络的“齿轮”" class="headerlink" title="张量运算——神经网络的“齿轮”"></a>张量运算——神经网络的“齿轮”</h3><p>　　本节和下一节主要是高等数学和线性代数在神经网络中的运用，下面只简单提一下这些数学知识在神经网络中的相关概念。</p><h4 id="逐元素运算（element-wise）"><a href="#逐元素运算（element-wise）" class="headerlink" title="逐元素运算（element-wise）"></a>逐元素运算（element-wise）</h4><p>　　运算独立地应用于张两种地每个元素，适合大规模并行实现。如：</p><ul><li>relu运算：relu(x) == max(x, 0)</li><li>四则运算（前提是运算对象形状相同）</li></ul><h4 id="广播（broadcast）"><a href="#广播（broadcast）" class="headerlink" title="广播（broadcast）"></a>广播（broadcast）</h4><p>　　两个形状不同地张量相加，较小地张量会被广播，以匹配较大的张量。如：<br>　　x形状为（32，10），y形状为（10，），则x+y会为y添加空的第一个轴（广播轴）→（1，10），再沿新轴重复32次→（32，10）。<br>　　但以上过程不会在运算中实际发生，只是想象的思维模型。</p><h4 id="张量点积（tensor-product）"><a href="#张量点积（tensor-product）" class="headerlink" title="张量点积（tensor product）"></a>张量点积（tensor product）</h4><p>　　类似于线性代数中矩阵的乘法，Keras和Numpy中使用<code>numpy.dot(x, y)</code>实现。<br>　　形如(a, b, c, d).(d, e)-&gt;(a, b, c, e)，<img src="https://s1.ax1x.com/2020/05/19/Y4DYO1.jpg" alt=""></p><h4 id="张量变形（tensor-reshaping）"><a href="#张量变形（tensor-reshaping）" class="headerlink" title="张量变形（tensor reshaping）"></a>张量变形（tensor reshaping）</h4><p>　　改变张量的行列，但元素总数不变。<code>x.reshape((2,6))</code><br>　　行列互换称为<strong>转置</strong>（transposition）<code>numpy.transpose(x)</code></p><h4 id="张量运算的几何解释"><a href="#张量运算的几何解释" class="headerlink" title="张量运算的几何解释"></a>张量运算的几何解释</h4><p>　　类似高数中的向量变换以及更高维的延申。</p><h3 id="基于梯度的优化——神经网络的“引擎”"><a href="#基于梯度的优化——神经网络的“引擎”" class="headerlink" title="基于梯度的优化——神经网络的“引擎”"></a>基于梯度的优化——神经网络的“引擎”</h3><p>　　对每个神经层用下述方法对输入数据进行变换。<br>　　<code>output = relu(dot(W, input) + b)</code><br>　　在这个表达式中，W 和b 都是张量，均为该层的属性。它们被称为该层的<strong>权重</strong>（weight）或<strong>可训练参数</strong>（trainable parameter），分别对应kernel 和bias 属性。这些权重包含网络从观察训练数据中学到的信息。<br>　　一开始，这些权重矩阵取较小的随机值，这一步叫作<strong>随机初始化</strong>（random initialization）。当然，W 和b 都是随机的，relu(dot(W, input) + b) 肯定不会得到任何有用的表示。虽然得到的表示是没有意义的，但这是一个起点。下一步则是根据反馈信号逐渐调节这些权重。这个逐渐调节的过程叫作<strong>训练</strong>，也就是机器学习中的<strong>学习</strong>。<br>　　上述过程发生在一个<strong>训练循环</strong>（training loop）内，其具体过程如下。必要时一直重复这些步骤。</p><ul><li>抽取训练样本x 和对应目标y 组成的数据批量。</li><li>在x 上运行网络［这一步叫作前向传播（forward pass）］，得到预测值y_pred。</li><li>计算网络在这批数据上的损失，用于衡量y_pred 和y 之间的距离。</li><li>更新网络的所有权重，使网络在这批数据上的损失略微下降。</li></ul><p>　　最终得到的网络在训练数据上的损失非常小，即预测值y_pred 和预期目标y 之间的距离非常小。<br><span style="border-bottom:2px solid red">详情参见：<a href="https://www.ituring.com.cn/book/tupubarticle/23177" target="_blank" rel="noopener">https://www.ituring.com.cn/book/tupubarticle/23177</a></span></p><h2 id="Chapter-3-神经网络入门"><a href="#Chapter-3-神经网络入门" class="headerlink" title="Chapter 3 神经网络入门"></a>Chapter 3 神经网络入门</h2><p>　　这里再次引用第一章的深度学习工作原理图。<br><img src="https://i.loli.net/2020/05/16/zomZXBxty67UqEN.png" alt="深度学习工作原理图"></p><h3 id="层：深度学习的基础组件"><a href="#层：深度学习的基础组件" class="headerlink" title="层：深度学习的基础组件"></a>层：深度学习的基础组件</h3><p>　　<strong>层</strong>是神经网络的基本数据结构，层的状态即层的<strong>权重</strong>，权重是利用<strong>随机梯度下降</strong>学到的一个或多个张量。以下是几种不同的层及其应用场景。</p><ul><li><strong>密集连接层</strong>（densely connected layer）：又叫<strong>全连接层</strong>（fully connected layer）和<strong>密集层</strong>（dense layer），用于处理保存简单向量的2D张量，形状为 (samples, features)，对应Keras的Dense类。</li><li><strong>循环层</strong>（recurrent layer）：用于处理保存序列数据的3D张量，形状为 (samples, timesteps, features)，对应Keras的LSTM层。</li><li><strong>二维卷积层</strong>：用于处理保存图像数据的4D张量，形状为 (samples, height, width, channels)，对应Keras的Conv2D。</li></ul><h3 id="模型：层构成的网络"><a href="#模型：层构成的网络" class="headerlink" title="模型：层构成的网络"></a>模型：层构成的网络</h3><p>　　构建深度学习模型就是将相互兼容的多个层拼接在一起，以建立有用的数据变换流程。深度学习模型是层构成的有向无环图。<br>　　这里的层兼容性（layer compatibility）指每一层只接受特定形状的输入张量并返回特定形状的输出张量。<br>　　一些常见的网络拓扑结构如下:<br>　　（1）线性网络<br>　　（2）双分支（two-branch）网络<br>　　（3）多头（multihead）网络<br>　　（4）Inception模块</p><h3 id="损失函数与优化器：配置学习过程的关键"><a href="#损失函数与优化器：配置学习过程的关键" class="headerlink" title="损失函数与优化器：配置学习过程的关键"></a>损失函数与优化器：配置学习过程的关键</h3><ul><li>损失函数(目标函数)：训练过程中将其最小化，能够衡量当前任务是否已成功完成。</li><li>优化器：决定如何根据损失函数对网络进行更新，执行随机梯度下降（SGD： stochastic gradient descent）的某个变体。</li></ul><p>　　注意：具有多个输出的神经网络可能有多个损失函数，但梯度下降过程必须基于单个标量损失值。因此要将所有损失函数取平均变为一个标量值。</p><h3 id="Keras开发流程"><a href="#Keras开发流程" class="headerlink" title="Keras开发流程"></a>Keras开发流程</h3><ul><li>定义训练数据：输入张量和目标张量。</li><li>定义层组成的网络（或模型），将输入映射到目标。</li><li>配置学习过程：选择损失函数，优化器和需要监控的指标。</li><li>调用模型的fit()方法迭代训练数据。</li></ul><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models,layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用Sequential类定义模型</span></span><br><span class="line"><span class="comment"># model = models.Sequential()</span></span><br><span class="line"><span class="comment"># model.add(layers.Dense(32, activation='relu', input_shape=(784,)))</span></span><br><span class="line"><span class="comment"># model.add(layers.Dense(10, activation='softmax'))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#函数式API定义模型</span></span><br><span class="line">input_tensor = layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">x = layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(input_tensor)</span><br><span class="line">output_tensor = layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)(x)</span><br><span class="line">model = models.Model(inputs=input_tensor, outputs=output_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment">#编译</span></span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),       <span class="comment">#优化器</span></span><br><span class="line">              loss=<span class="string">'mse'</span>,                                   <span class="comment">#损失函数</span></span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])                         <span class="comment">#评估函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#迭代训练数据</span></span><br><span class="line">model.fit(input_tensor, target_tensor, batch_size=<span class="number">128</span>, epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="电影评论分类：二分类问题"><a href="#电影评论分类：二分类问题" class="headerlink" title="电影评论分类：二分类问题"></a>电影评论分类：二分类问题</h3><p>注意：</p><ul><li>隐藏单元越多（更高维的表示空间），网络越能学习更复杂的表示，但这会使网络的计算代价更大，并且可能导致学习到不好的模式（这种模式可以提高训练数据的性能，但不能提高测试数据的性能）。</li><li>sigmoid函数将任意值压缩到[0, 1]区间内。</li><li>relu（rectified linear unit，整流线性单元）函数，将所有负值归零。</li><li>激活函数：也叫非线性，为了得到更丰富的假设空间，充分利用多层表示的优势。</li><li>对于二分类这种最后输出概率值的问题，损失函数最优解为binary_crossentropy（二元交叉熵）。</li></ul><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#二分类问题</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models, layers</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 1 加载数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#train_data和train_labels为评论组成的列表，而每条评论为单词索引组成的列表</span></span><br><span class="line"><span class="comment">#test_data和test_labels为0、1组成的列表，代表负面和正面</span></span><br><span class="line"><span class="comment">#num_words=10000表示仅保留训练数据中前10000个最常出现的单词</span></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># word_index = imdb.get_word_index()                                   #将单词映射为整数索引的字典</span></span><br><span class="line"><span class="comment"># reverse_word_index = dict(                                           #键值颠倒，整数索引映射为单词</span></span><br><span class="line"><span class="comment">#     [(value, key) for (key, value) in word_index.items()])</span></span><br><span class="line"><span class="comment"># decoded_review = ' '.join(                                           #评论解码，索引减去3，</span></span><br><span class="line"><span class="comment">#     [reverse_word_index.get(i - 3, '?') for i in train_data[0]])     #因为0、1、2为保留索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 2 处理数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#对列表进行one-hot编码，转化为0、1组成的向量。</span></span><br><span class="line"><span class="comment">#将整数序列编码为二进制矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))                    <span class="comment">#创建一个零矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)                              <span class="comment">#训练数据向量化</span></span><br><span class="line">x_test = vectorize_sequences(test_data)                                <span class="comment">#测试数据向量化</span></span><br><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)                   <span class="comment">#训练标签向量化</span></span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)                     <span class="comment">#测试标签向量化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#留出10000个样本作验证集</span></span><br><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line">y_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 3 定义模型/构建网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#采用3层全连接层，16为隐藏单元个数，即维度，activation为激活函数</span></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 4 编译模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置优化器，损失函数，评估函数</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'acc'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 5 训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用512个样本组成的小批量，对所有样本进行20次迭代，</span></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图</span></span><br><span class="line"></span><br><span class="line">history_dict = history.history</span><br><span class="line">loss_values = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">val_loss_values = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss_values) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss_values, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss_values, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.clf()</span><br><span class="line">acc_values = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">val_acc_values = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc_values, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc_values, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><table><thead><tr><th><img src="https://s1.ax1x.com/2020/05/20/YTstRf.png" alt="训练损失和验证损失"></th><th align="center"><img src="https://s1.ax1x.com/2020/05/20/YTsgzT.png" alt="训练精度和验证精度"></th></tr></thead></table><p>　　由上图可见，每轮训练损失在降低，训练精度在上升，符合预期，但验证损失和验证精度并非如此，模型在训练数据上表现更好，但不一定在从未见过的数据上表现更好，这种现象成为<strong>过拟合</strong>（overfit），详见Chapter 4。<br>　　下面是一种简单的训练方法：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">results = model.evaluate(x_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>results</span><br><span class="line">[<span class="number">0.29506705965518953</span>, <span class="number">0.884119987487793</span>]</span><br></pre></td></tr></table></figure><p>　　用训练好的网络进行预测。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>model.predict(x_test)</span><br><span class="line">array([[ <span class="number">0.98006207</span>]</span><br><span class="line">       [ <span class="number">0.99758697</span>]</span><br><span class="line">       [ <span class="number">0.99975556</span>]</span><br><span class="line">       ...,</span><br><span class="line">       [ <span class="number">0.82167041</span>]</span><br><span class="line">       [ <span class="number">0.02885115</span>]</span><br><span class="line">       [ <span class="number">0.65371346</span>]], dtype=float32)</span><br></pre></td></tr></table></figure><h3 id="新闻分类：多分类问题"><a href="#新闻分类：多分类问题" class="headerlink" title="新闻分类：多分类问题"></a>新闻分类：多分类问题</h3><ul><li>单标签、多分类（single-label，multiclass classification）：每个数据点只能划分到一个类别。</li><li>多标签、多分类（multilabel，multiclass classification）：每个数据点能划分到多个类别。</li></ul><p>注意：</p><ul><li>编码数据中将标签向量化的两种方法：转化为整数张量或进行one-hot编码。注意两种方法所使用的<strong>损失函数</strong>可能会有所差别。<ul><li>通过分类编码（也称为one-hot编码），使用categorical_crossentropy作为损失函数对标签进行编码。</li><li>将标签编码为整数，使用sparse_categorical_crossentropy作为损失函数对标签进行编码。</li></ul></li><li>如果要在N个类别中对数据点进行分类，则网络最后一层应为大小为N的Dense层。</li><li>在单标签，多类分类问题中，网络应以softmax激活结束，这样可以输出在N个类别上的概率分布。</li><li><strong>分类交叉熵</strong>几乎总是针对多分类问题使用的损失函数，它使网络输出的概率分布与目标的真实分布之间的距离最小化。</li><li>如果需要将数据划分为大量类别，则应避免使用较小的中间层，而在网络中导致信息瓶颈（永久地丢失信息）。</li></ul><p>本例使用路透社数据集。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多分类问题</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models, layers</span><br><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(</span><br><span class="line">    num_words=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br><span class="line"><span class="comment">#分类编码</span></span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br><span class="line"><span class="comment">#将标签转化为整数张量</span></span><br><span class="line"><span class="comment"># y_train = np.array(train_labels)</span></span><br><span class="line"><span class="comment"># y_test = np.array(test_labels)</span></span><br><span class="line"></span><br><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,      <span class="comment">#分类交叉熵</span></span><br><span class="line">              metrics=[<span class="string">'acc'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><table><thead><tr><th><img src="https://s1.ax1x.com/2020/05/21/YbAsqs.png" alt="训练损失和验证损失"></th><th align="center"><img src="https://s1.ax1x.com/2020/05/21/YbA6Zn.png" alt="训练精度和验证精度"></th></tr></thead></table><p>由上图可见，网络在训练9轮后开始过拟合，重新进行训练并进行评估。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">9</span>,</span><br><span class="line">          batch_size=<span class="number">512</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br><span class="line">results = model.evaluate(x_test, one_hot_test_labels)</span><br></pre></td></tr></table></figure><p>在新数据上进行评估。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">predictions = model.predict(x_test)</span><br></pre></td></tr></table></figure><h3 id="预测房价：回归问题"><a href="#预测房价：回归问题" class="headerlink" title="预测房价：回归问题"></a>预测房价：回归问题</h3><p>前面两种分类问题目标是预测输入数据点所对应的单一离散的标签，而回归问题预测一个连续值而不是离散的标签，如根据气象数据预测明天的气温。<br>本例采用波士顿房价数据集，每个样本有多个数据特征，如犯罪率、每个住宅的平均房间数等。目标是房屋价格的中位数，单位千美元。<br>注意：</p><ul><li>本例的每个特征几乎都有不同的取值范围，应当对每个特征值做标准化，如减去特征平均值再除以标准差。</li><li>回归问题常用的损失函数为均方误差（MSE，mean squared error），预测值与目标值之差的平方。</li><li>回归问题使用的评估指标（metrics）为平均绝对误差（MAE，mean absolute error）预测值与目标值之差的绝对值。</li><li>若可用数据很少，使用K折验证可以可靠地评估模型。</li><li>若可用的训练数据很少，最好使用隐藏层较少（1到2个）的网络，避免严重的过拟合</li></ul><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#回归问题</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models, layers</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment">#特征值标准化</span></span><br><span class="line">mean = train_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">train_data -= mean</span><br><span class="line">std = train_data.std(axis=<span class="number">0</span>)</span><br><span class="line">train_data /= std</span><br><span class="line"></span><br><span class="line">mean1 = test_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">test_data -= mean1</span><br><span class="line">std1 = test_data.std(axis=<span class="number">0</span>)</span><br><span class="line">test_data /= std1</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为要将同一个模型多次实例化，故用一个函数来构建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                           input_shape=(train_data.shape[<span class="number">1</span>],)))</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))                                      <span class="comment">#线性层，标量回归</span></span><br><span class="line">    model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'mse'</span>, metrics=[<span class="string">'mae'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment">#K折验证</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line"><span class="comment"># num_epochs = 100</span></span><br><span class="line"><span class="comment"># all_scores = []</span></span><br><span class="line">num_epochs = <span class="number">500</span></span><br><span class="line">all_mae_histories = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">'processing fold #'</span>, i)</span><br><span class="line">    <span class="comment">#准备验证数据：第k个分区的数据</span></span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#准备训练数据：其他所有分区的数据</span></span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[:i * num_val_samples],</span><br><span class="line">         train_data[(i + <span class="number">1</span>) * num_val_samples:]],</span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[:i * num_val_samples],</span><br><span class="line">         train_targets[(i + <span class="number">1</span>) * num_val_samples:]],</span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    model = build_model()</span><br><span class="line">    <span class="comment"># model.fit(partial_train_data, partial_train_targets,</span></span><br><span class="line">    <span class="comment">#           epochs=num_epochs, batch_size=1, verbose=0)</span></span><br><span class="line">    <span class="comment"># val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)</span></span><br><span class="line">    <span class="comment"># all_scores.append(val_mae)</span></span><br><span class="line">    <span class="comment"># mean_scores = np.mean(all_scores)</span></span><br><span class="line"></span><br><span class="line">    history = model.fit(partial_train_data, partial_train_targets,</span><br><span class="line">                        validation_data=(val_data, val_targets),</span><br><span class="line">                        epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">    print(history.history.keys())</span><br><span class="line">    mae_history = history.history[<span class="string">'val_mae'</span>]</span><br><span class="line">    all_mae_histories.append(mae_history)</span><br><span class="line"></span><br><span class="line">average_mae_history = [</span><br><span class="line">    np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> all_mae_histories]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)]</span><br><span class="line"></span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(average_mae_history) + <span class="number">1</span>), average_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#将每个数据点替换为前面数据点的指数移动平均值，以得到光滑曲线。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">  smoothed_points = []</span><br><span class="line">  <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">    <span class="keyword">if</span> smoothed_points:</span><br><span class="line">      previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">      smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      smoothed_points.append(point)</span><br><span class="line">  <span class="keyword">return</span> smoothed_points</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除前10个数据点</span></span><br><span class="line">smooth_mae_history = smooth_curve(average_mae_history[<span class="number">10</span>:])</span><br><span class="line"></span><br><span class="line">plt.clf()</span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(smooth_mae_history) + <span class="number">1</span>), smooth_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><table><thead><tr><th><img src="https://s1.ax1x.com/2020/05/21/YqbxS0.png" alt="每轮的验证MAE"></th><th align="center"><img src="https://s1.ax1x.com/2020/05/21/YqqSyT.png" alt="重绘后的每轮的验证MAE"></th></tr></thead></table><p>由上图可知，验证MAE在80轮后不再显著降低，之后开始过拟合，调整参数在训练数据上训练参数。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model = build_model()</span><br><span class="line">model.fit(train_data, train_targets,</span><br><span class="line">          epochs=<span class="number">80</span>, batch_size=<span class="number">16</span>, verbose=<span class="number">0</span>)</span><br><span class="line">test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_mae_score</span><br><span class="line"><span class="number">2.6759588718414307</span></span><br></pre></td></tr></table></figure><h2 id="Chapter-4-机器学习基础"><a href="#Chapter-4-机器学习基础" class="headerlink" title="Chapter 4 机器学习基础"></a>Chapter 4 机器学习基础</h2><h3 id="机器学习的四个分支"><a href="#机器学习的四个分支" class="headerlink" title="机器学习的四个分支"></a>机器学习的四个分支</h3><h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h4><p>最常见的机器学习类型，给定一组样本（通常由人工标注），学习将输入数据映射到已知目标。主要包括分类和回归问题，如光学字符识别、语音识别、图像分类以及语言翻译。另外包括一些主要的变体如下</p><h4 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h4><h4 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h4><h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><h3 id="机器学习通用工作流程"><a href="#机器学习通用工作流程" class="headerlink" title="机器学习通用工作流程"></a>机器学习通用工作流程</h3><h4 id="定义问题，收集数据集"><a href="#定义问题，收集数据集" class="headerlink" title="定义问题，收集数据集"></a>定义问题，收集数据集</h4><h4 id="选择衡量成功的指标"><a href="#选择衡量成功的指标" class="headerlink" title="选择衡量成功的指标"></a>选择衡量成功的指标</h4><h4 id="确定评估方法"><a href="#确定评估方法" class="headerlink" title="确定评估方法"></a>确定评估方法</h4><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><h4 id="优化模型"><a href="#优化模型" class="headerlink" title="优化模型"></a>优化模型</h4><h4 id="模型正则化与调节超参数"><a href="#模型正则化与调节超参数" class="headerlink" title="模型正则化与调节超参数"></a>模型正则化与调节超参数</h4><h2 id="Chapter-5-卷积神经网络"><a href="#Chapter-5-卷积神经网络" class="headerlink" title="Chapter 5 卷积神经网络"></a>Chapter 5 卷积神经网络</h2><p>一个简单的卷积神经网络示例。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#卷积神经网络</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models, layers</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">train_images = train_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">test_images = test_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># model.summary()</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(train_images, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_acc</span><br><span class="line"><span class="number">0.9919000267982483</span></span><br></pre></td></tr></table></figure><h2 id="Windows下的环境搭建"><a href="#Windows下的环境搭建" class="headerlink" title="Windows下的环境搭建"></a><span id="jump">Windows下的环境搭建</span></h2><p>由于懒得装双系统，虚拟机先不说配置不够，搭起来感觉坑很多，遂直接在Windows系统下搭建。</p><h3 id="安装CUDA和cuDNN"><a href="#安装CUDA和cuDNN" class="headerlink" title="安装CUDA和cuDNN"></a>安装CUDA和cuDNN</h3><blockquote><p>CUDA（Compute Unified Device Architecture）：NVIDIA用于自家GPU的并行计算框架，本质是一个工具包（ToolKit）。</p></blockquote><blockquote><p>cuDNN（CUDA Deep Neural Network library）：是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。用GPU训练模型，cuDNN不是必须的，但是一般会采用这个加速库。</p></blockquote><ol><li><p>查看显卡支持的CUDA版本<br>NVIDIA控制面板&gt;帮助&gt;系统信息&gt;组件，如图支持10.2版本<br><img src="https://s1.ax1x.com/2020/05/17/Y20vQK.png" alt=""></p></li><li><p>安装对应版本的CUDA</p><blockquote><p>下载地址：<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-downloads</a></p></blockquote><blockquote><p>安装完成后打开cmd，输入命令：<code>nvcc -V</code>，如图</p><img src="https://s1.ax1x.com/2020/05/17/Y2yrUe.png" style="margin-left:0" alt=""></blockquote></li><li><p>编译CUDA（暂时未编译）</p></li><li><p>安装cuDNN</p><blockquote><p>下载地址：<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a></p></blockquote><blockquote><p>这里需要注册账户才可以下载，可以看到支持的对应CUDA版本<br>将解压后的文件复制到CUDA安装路径中，这里是默认路径<br><code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1</code></p></blockquote></li></ol><h3 id="安装TensorFlow和Keras"><a href="#安装TensorFlow和Keras" class="headerlink" title="安装TensorFlow和Keras"></a>安装TensorFlow和Keras</h3><p>由于本机已经安装过Python3.7，管理多版本Python有点麻烦，故直接<strong>使用pip安装</strong>而不是Anaconda<br>在用pip安装前，<strong>确保pypi源更换为国内镜像</strong>，否则速度超级慢</p><blockquote><p>方法为在C盘的user目录下新建一个pip文件夹，如：C:\Users\xx\pip，在pip文件夹内新建一个pip.ini文件，内容如下：<br></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn</span><br></pre></td></tr></table></figure><p></p></blockquote><blockquote><p>常用国内镜像：<br></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple&#x2F;   # 清华大学</span><br><span class="line">https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;     # 阿里云</span><br><span class="line">https:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;             # 豆瓣</span><br><span class="line">https:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple&#x2F;    # 中国科学技术大学</span><br><span class="line">https:&#x2F;&#x2F;pypi.hustunique.com&#x2F;                # 华中科技大学</span><br></pre></td></tr></table></figure><p></p></blockquote><ol><li>安装Tensorflow<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">pip install tensorflow==<span class="number">2.1</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">pip install tensorflow-gpu==<span class="number">2.1</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>安装完成后在python环境中运行<figure class="highlight elm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">tf</span><span class="selector-class">.test</span><span class="selector-class">.is_gpu_available</span>()</span><br></pre></td></tr></table></figure>可以查看到GPU信息即安装成功。<br>Tensorflow作为Keras的后端,TensorFlow与CUDA和cuDNN各版本对应如下：<br><img src="https://s1.ax1x.com/2020/05/17/Y24DV1.png" alt=""></li><li>安装Keras<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">pip <span class="keyword">install</span> keras</span><br></pre></td></tr></table></figure></li></ol><h3 id="安装其他组件"><a href="#安装其他组件" class="headerlink" title="安装其他组件"></a>安装其他组件</h3><h4 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h4><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">python -m pip <span class="keyword">install</span> matplotlib</span><br></pre></td></tr></table></figure></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>不负骤雨</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://xxyr.cc/post/DeepLearning/deep-learning-notes/" title="《Python深度学习》笔记">https://xxyr.cc/post/DeepLearning/deep-learning-notes/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python/" rel="tag"># Python</a> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/post/Tips/common-commands/" rel="prev" title="常用命令"><i class="fa fa-chevron-left"></i> 常用命令</a></div><div class="post-nav-item"><a href="/post/Python/effective-python/" rel="next" title="Pythonic Coding">Pythonic Coding <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-1-基本概念"><span class="nav-number">1.</span> <span class="nav-text">Chapter 1 基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#人工智能"><span class="nav-number">1.1.</span> <span class="nav-text">人工智能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习"><span class="nav-number">1.2.</span> <span class="nav-text">机器学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深度学习"><span class="nav-number">1.3.</span> <span class="nav-text">深度学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-2-神经网络的数学基础"><span class="nav-number">2.</span> <span class="nav-text">Chapter 2 神经网络的数学基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#张量（tensor）"><span class="nav-number">2.1.</span> <span class="nav-text">张量（tensor）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#标量（scalar）"><span class="nav-number">2.2.</span> <span class="nav-text">标量（scalar）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#向量（vector）"><span class="nav-number">2.3.</span> <span class="nav-text">向量（vector）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵（matrix）"><span class="nav-number">2.4.</span> <span class="nav-text">矩阵（matrix）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3D张量与更高维张量"><span class="nav-number">2.5.</span> <span class="nav-text">3D张量与更高维张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量的关键属性"><span class="nav-number">2.6.</span> <span class="nav-text">张量的关键属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量切片"><span class="nav-number">2.7.</span> <span class="nav-text">张量切片</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据批量"><span class="nav-number">2.8.</span> <span class="nav-text">数据批量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#现实世界中的数据张量"><span class="nav-number">2.9.</span> <span class="nav-text">现实世界中的数据张量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#向量数据"><span class="nav-number">2.9.1.</span> <span class="nav-text">向量数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#时间序列数据或序列数据"><span class="nav-number">2.9.2.</span> <span class="nav-text">时间序列数据或序列数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图像数据"><span class="nav-number">2.9.3.</span> <span class="nav-text">图像数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#视频数据"><span class="nav-number">2.9.4.</span> <span class="nav-text">视频数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量运算——神经网络的“齿轮”"><span class="nav-number">2.10.</span> <span class="nav-text">张量运算——神经网络的“齿轮”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#逐元素运算（element-wise）"><span class="nav-number">2.10.1.</span> <span class="nav-text">逐元素运算（element-wise）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#广播（broadcast）"><span class="nav-number">2.10.2.</span> <span class="nav-text">广播（broadcast）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#张量点积（tensor-product）"><span class="nav-number">2.10.3.</span> <span class="nav-text">张量点积（tensor product）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#张量变形（tensor-reshaping）"><span class="nav-number">2.10.4.</span> <span class="nav-text">张量变形（tensor reshaping）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#张量运算的几何解释"><span class="nav-number">2.10.5.</span> <span class="nav-text">张量运算的几何解释</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于梯度的优化——神经网络的“引擎”"><span class="nav-number">2.11.</span> <span class="nav-text">基于梯度的优化——神经网络的“引擎”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-3-神经网络入门"><span class="nav-number">3.</span> <span class="nav-text">Chapter 3 神经网络入门</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#层：深度学习的基础组件"><span class="nav-number">3.1.</span> <span class="nav-text">层：深度学习的基础组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型：层构成的网络"><span class="nav-number">3.2.</span> <span class="nav-text">模型：层构成的网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数与优化器：配置学习过程的关键"><span class="nav-number">3.3.</span> <span class="nav-text">损失函数与优化器：配置学习过程的关键</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Keras开发流程"><span class="nav-number">3.4.</span> <span class="nav-text">Keras开发流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#电影评论分类：二分类问题"><span class="nav-number">3.5.</span> <span class="nav-text">电影评论分类：二分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#新闻分类：多分类问题"><span class="nav-number">3.6.</span> <span class="nav-text">新闻分类：多分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预测房价：回归问题"><span class="nav-number">3.7.</span> <span class="nav-text">预测房价：回归问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-4-机器学习基础"><span class="nav-number">4.</span> <span class="nav-text">Chapter 4 机器学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习的四个分支"><span class="nav-number">4.1.</span> <span class="nav-text">机器学习的四个分支</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#监督学习"><span class="nav-number">4.1.1.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#无监督学习"><span class="nav-number">4.1.2.</span> <span class="nav-text">无监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自监督学习"><span class="nav-number">4.1.3.</span> <span class="nav-text">自监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#强化学习"><span class="nav-number">4.1.4.</span> <span class="nav-text">强化学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习通用工作流程"><span class="nav-number">4.2.</span> <span class="nav-text">机器学习通用工作流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义问题，收集数据集"><span class="nav-number">4.2.1.</span> <span class="nav-text">定义问题，收集数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#选择衡量成功的指标"><span class="nav-number">4.2.2.</span> <span class="nav-text">选择衡量成功的指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#确定评估方法"><span class="nav-number">4.2.3.</span> <span class="nav-text">确定评估方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#准备数据"><span class="nav-number">4.2.4.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化模型"><span class="nav-number">4.2.5.</span> <span class="nav-text">优化模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型正则化与调节超参数"><span class="nav-number">4.2.6.</span> <span class="nav-text">模型正则化与调节超参数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-5-卷积神经网络"><span class="nav-number">5.</span> <span class="nav-text">Chapter 5 卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Windows下的环境搭建"><span class="nav-number">6.</span> <span class="nav-text">Windows下的环境搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装CUDA和cuDNN"><span class="nav-number">6.1.</span> <span class="nav-text">安装CUDA和cuDNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装TensorFlow和Keras"><span class="nav-number">6.2.</span> <span class="nav-text">安装TensorFlow和Keras</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装其他组件"><span class="nav-number">6.3.</span> <span class="nav-text">安装其他组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#matplotlib"><span class="nav-number">6.3.1.</span> <span class="nav-text">matplotlib</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="不负骤雨" src="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210207-162535-0914.jpg"><p class="site-author-name" itemprop="name">不负骤雨</p><div class="site-description" itemprop="description">reading coding keeping</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">文章</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zhishui1?tab=repositories" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhishui1?tab&#x3D;repositories" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/349434614" title="BiliBili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;349434614" rel="noopener" target="_blank"><i class="fa fa-fw fa-custom bili"></i></a> </span><span class="links-of-author-item"><a href="https://steamcommunity.com/id/zhishui_x" title="Steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;zhishui_x" rel="noopener" target="_blank"><i class="fa fa-fw fa-steam"></i></a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">不负骤雨</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">246k</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: true,
      notify: true,
      appId: 'hOTS2YD4rcQy3T7bS94tywjn-gzGzoHsz',
      appKey: '4fQPAOtQk7tqm6wJ8kJxODvT',
      placeholder: "Just go go",
      avatar: 'wavatar',
      meta: guest,
      pageSize: '10' || 10,
      visitor: true,
      lang: 'zh-cn' || 'zh-cn',
      path: location.pathname,
      recordIP: true,
      serverURLs: ''
    });
  }, window.Valine);
});</script></body></html>