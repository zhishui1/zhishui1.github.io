<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://xxyr.cc").hostname,root:"/",scheme:"Gemini",version:"7.6.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"manual"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="《Python3网络爬虫开发实战》笔记"><meta property="og:type" content="article"><meta property="og:title" content="Python爬虫基础"><meta property="og:url" content="https://xxyr.cc/post/Python/python-spider/index.html"><meta property="og:site_name" content="不负骤雨"><meta property="og:description" content="《Python3网络爬虫开发实战》笔记"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skJuQI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skt3GQ.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/sktHsI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skNbp4.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skUWCD.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skaVxJ.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skavFK.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skyP9x.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skykjO.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skh0nx.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skhWjI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skhhut.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/11/s89S0K.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/12/sJ0Uaj.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/12/sJ26Lq.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/17/sse47d.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/18/sy04PI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/18/sywvjO.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/18/sysBwD.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/18/s61te0.png"><meta property="article:published_time" content="2020-08-31T03:43:55.000Z"><meta property="article:modified_time" content="2021-01-19T09:28:47.684Z"><meta property="article:author" content="不负骤雨"><meta property="article:tag" content="Python"><meta property="article:tag" content="Spider"><meta property="article:tag" content="爬虫"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://s3.ax1x.com/2021/01/05/skJuQI.png"><link rel="canonical" href="https://xxyr.cc/post/Python/python-spider/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>Python爬虫基础 | 不负骤雨</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" style="margin:10px"><div class="container use-motion"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">不负骤雨</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">围城</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">13</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">14</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://xxyr.cc/post/Python/python-spider/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://i.loli.net/2019/12/15/ev4RZy7Iakn9WHl.jpg"><meta itemprop="name" content="不负骤雨"><meta itemprop="description" content="reading coding keeping"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="不负骤雨"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Python爬虫基础</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-08-31 11:43:55" itemprop="dateCreated datePublished" datetime="2020-08-31T11:43:55+08:00">2020-08-31</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-01-19 17:28:47" itemprop="dateModified" datetime="2021-01-19T17:28:47+08:00">2021-01-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span> </a></span></span><span id="/post/Python/python-spider/" class="post-meta-item leancloud_visitors" data-flag-title="Python爬虫基础" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论次数：</span> <a title="valine" href="/post/Python/python-spider/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/post/Python/python-spider/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>《Python3网络爬虫开发实战》笔记<a id="more"></a></p><h2 id="Ch-1-爬虫基础"><a href="#Ch-1-爬虫基础" class="headerlink" title="Ch 1 爬虫基础"></a>Ch 1 爬虫基础</h2><h3 id="HTTP基本原理"><a href="#HTTP基本原理" class="headerlink" title="HTTP基本原理"></a>HTTP基本原理</h3><h4 id="URI和URL"><a href="#URI和URL" class="headerlink" title="URI和URL"></a>URI和URL</h4><p>URI的全称为 Uniform Resource Identifier，即统一资源标志符。<br>URL的全称为 Universal Resource Locator，即统一资源定位符。<br>URN的全称为 Universal Resource Name，即统一资源名称。<br>三者关系为：<br><img src="https://s3.ax1x.com/2021/01/05/skJuQI.png" alt=""></p><h4 id="超文本"><a href="#超文本" class="headerlink" title="超文本"></a>超文本</h4><p>HypeText</p><h4 id="HTTP和HTTPS"><a href="#HTTP和HTTPS" class="headerlink" title="HTTP和HTTPS"></a>HTTP和HTTPS</h4><p>HTTP的全称是Hyper Text Transfer Protocol，超文本传输协议。<br>HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的HTTP通道，简单讲是 HTTP 的安全版，即HTTP下加入 SSL层，简称为HTTPS。</p><h4 id="HTTP请求过程"><a href="#HTTP请求过程" class="headerlink" title="HTTP请求过程"></a>HTTP请求过程</h4><h4 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h4><ul><li><p>请求方法<br><img src="https://s3.ax1x.com/2021/01/05/skt3GQ.png" alt=""></p></li><li><p>请求网址<br>URL</p></li><li><p>请求头<br>用来说明服务器要使用的附加信息。<br><img src="https://s3.ax1x.com/2021/01/05/sktHsI.png" alt=""></p><p>因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。</p></li><li><p>请求体<br>一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。<br><img src="https://s3.ax1x.com/2021/01/05/skNbp4.png" alt=""></p><p>在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type， 不然可能会导致POST提交后无法正常响应。</p></li></ul><h4 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h4><ul><li><p>响应状态码<br>响应状态码表示服务器的响应状态。<br><img src="https://s3.ax1x.com/2021/01/05/skUWCD.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skaVxJ.png" alt=""></p></li><li><p>响应头<br>响应头包含了服务器对请求的应答信息。<br><img src="https://s3.ax1x.com/2021/01/05/skavFK.png" alt=""></p></li><li><p>响应体<br>响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。</p></li></ul><h3 id="网页基础"><a href="#网页基础" class="headerlink" title="网页基础"></a>网页基础</h3><h4 id="网页的组成"><a href="#网页的组成" class="headerlink" title="网页的组成"></a>网页的组成</h4><p>网页可以分为三大部分——HTML,CSS和JavaScript。如果把网页比作一个人的话， HTML相于骨架，JavaScript相当于肌肉，CSS相当于皮肤。</p><ul><li>HTML<br>HTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。</li><li>CSS<br>CSS，全称叫作Cascading Style Sheets，即层叠样式表。</li><li>JavaScript<br>JavaScript，简称JS，是一种脚本语言，实现了一种实时、动态、交互的页面功能。</li></ul><h4 id="网页的结构"><a href="#网页的结构" class="headerlink" title="网页的结构"></a>网页的结构</h4><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="节点树及节点间关系"><a href="#节点树及节点间关系" class="headerlink" title="节点树及节点间关系"></a>节点树及节点间关系</h4><p><img src="https://s3.ax1x.com/2021/01/05/skyP9x.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skykjO.png" alt=""></p><h4 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h4><p>常用三种方法：根据id（#）、根据class（.）以及标签名（h1）进行筛选。<br>嵌套选择：</p><ul><li>各选择器之间加空格代表嵌套关系，如div #container为先选择一个div节点，在选择其内部id为container的节点。</li><li>不加空格代表并列关系，如div#container为选择id为container的div节点。</li></ul><p>css选择器还有一些其他语法规则，具体如表2-4所示。<br><img src="https://s3.ax1x.com/2021/01/05/skh0nx.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skhWjI.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skhhut.png" alt=""></p><h3 id="爬虫的基本原理"><a href="#爬虫的基本原理" class="headerlink" title="爬虫的基本原理"></a>爬虫的基本原理</h3><h4 id="爬虫概述"><a href="#爬虫概述" class="headerlink" title="爬虫概述"></a>爬虫概述</h4><ul><li>获取网页</li><li>提取信息</li><li>保存数据</li><li>自动化程序</li></ul><h4 id="能抓怎样的数据"><a href="#能抓怎样的数据" class="headerlink" title="能抓怎样的数据"></a>能抓怎样的数据</h4><p>HTML代码、JSON文件、二进制数据等</p><h4 id="JavaScript渲染页面"><a href="#JavaScript渲染页面" class="headerlink" title="JavaScript渲染页面"></a>JavaScript渲染页面</h4><p>通过分析其后台Ajax接口，或使用Selenium、Splash库来模拟JavaScript渲染。</p><h3 id="会话和Cookies"><a href="#会话和Cookies" class="headerlink" title="会话和Cookies"></a>会话和Cookies</h3><h4 id="静态网页和动态网页"><a href="#静态网页和动态网页" class="headerlink" title="静态网页和动态网页"></a>静态网页和动态网页</h4><h4 id="无状态HTTP"><a href="#无状态HTTP" class="headerlink" title="无状态HTTP"></a>无状态HTTP</h4><p>HTTP连接本身是无状态的。</p><ul><li><p>会话<br>Web中，会话对象用来存储特定用户会话所需的属性及配置信息。</p></li><li><p>Cookies<br>Cookies指某些网站为了辨别用户身份、进行会话跟踪而存储在用户本地终端上的数据。</p></li><li><p>会话维持<br>当客户端第一次请求服务器时，服务器会返回一个请求头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把Cookie保 存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies放到请求头一起提交给服务器，“Cookies携带了会话ID信息，服务器检查该Cookies即可找到对应的会话是什么，然后再判断会话来以此来辨认用户状态。</p></li><li><p>属性结构</p><ul><li>Name：该Cookie的名称。一旦创建，该名称便不可更改。</li><li>Value：该Cookie的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。</li><li>Domain：可以访问该 Cookie 的域名 。 例如，如果设置为 . zhihu.com ，则所有以 zh ihu .com 结尾的域名都可以访问该 Cookie。</li><li>Max Age：该Cookie失效的时间，单位为秒，也常和Expires一起使用，通过它可以计算出其有效时间。Max Age如果为正数，则该Cookie在Max Age 秒之后失效。如果为负数，则关闭浏览器时Cookie即失效，浏览器也不会以任何形式保存该Cookie。</li><li>Path：该Cookie的使用路径。如果设置为／path/，则只有路径为／path/的页面可以访问该Cookie；如果设置为/，则本域名下的所有页面都可以访问该Cookie。</li><li>Size字段：此Cookie的大小。</li><li>HTTP字段：Cookie的httponly属性。若此属性为true，则只有在HTTP头中会带有此Cookie的信息，而不能通过document.cookie来访问此Cookie。</li><li>Secure：该Cookie是否仅被使用安全协议传输。安全协议有HTTPS和SSL等，在网络上传输数据之前先将数据加密。默认为false。</li></ul></li><li><p>会话Cookie和持久Cookie<br>表面意思为会话Cookie存在浏览器内存里，浏览器关闭则Cookie失效；持久Cookie保存在硬盘里，下次可再次使用。<br>实际为设置Cookie的Max Age或Expires字段。</p></li></ul><h4 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a>常见误区</h4><p>“浏览器关闭，会话就消失了”。是不准确的。</p><h3 id="代理的基本原理"><a href="#代理的基本原理" class="headerlink" title="代理的基本原理"></a>代理的基本原理</h3><h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h4><p>本机的网络请求通过代理服务器访问Web服务器。</p><h4 id="代理的作用"><a href="#代理的作用" class="headerlink" title="代理的作用"></a>代理的作用</h4><ul><li>突破自身IP访问限制。</li><li>访问一些单位或团体内部资源。</li><li>提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将·其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。</li><li>隐藏真实IP：免受攻击或防止IP被封锁。</li></ul><h4 id="代理分类"><a href="#代理分类" class="headerlink" title="代理分类"></a>代理分类</h4><ul><li>根据协议区分<ul><li>FTP代理服务器：主要用于访问FTP服务器，一般有上传、下载以及缓存功能，端口一般为21、2121等。</li><li>HTTP代理服务器：主要用于访问网页，一般有内容过滤和缓存功能，端口一般为80、8080、3128等。</li><li>SSl/TLS代理：主要用于访问加密网站，一般有SSL或TLS加密功能（最高支持128位加密强度），端口一般为443。</li><li>RTSP代理：主要用于访问Real流媒体服务器，一般有缓存功能，端口一般为554。</li><li>Telnet代理：主要用于telnet远程控制（黑客人侵计算机时常用于隐藏身份），端口一般为23。</li><li>POP3/SMTP代理：主要用于POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25。</li><li>SOCKS代理：只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080。SOCKS代理协议又分为SOCKS4和SOCKS5，前者只支持TCP，而后者支持TCP和UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCKS4能做到的SOCKS5都可以做到，但 SOCKS5能做到的SOCKS4不一定能做到。</li></ul></li><li>根据匿名程度区分<ul><li>高度匿名代理：会将数据包原封不动地转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的IP是代理服务器的IP。</li><li>普通匿名代理：会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实IP。代理服务器通常会加入的HTTP头有HTTP_VIA和 HTTP_X_FORWARDED_FOR。</li><li>透明代理：不但改动了数据包 还会告诉服务器客户端的真实IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网巾的硬件防火墙。</li><li>间谍代理：指组织或个人创建的用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。</li></ul></li></ul><h4 id="常见代理设置"><a href="#常见代理设置" class="headerlink" title="常见代理设置"></a>常见代理设置</h4><ul><li>网上的免费代理</li><li>付费代理服务</li><li>ADSL拨号：拨一次号换一次IP，稳定性高。</li></ul><h2 id="Ch-2-基本库的使用"><a href="#Ch-2-基本库的使用" class="headerlink" title="Ch 2 基本库的使用"></a>Ch 2 基本库的使用</h2><h3 id="使用urllib"><a href="#使用urllib" class="headerlink" title="使用urllib"></a>使用urllib</h3><p>urllib为Python内置的HTTP请求库，包含以下4个模块：</p><ul><li>request：它是最基本的HTTP请求模块，可以用来模拟发送请求。</li><li>error：异常处理模块。</li><li>parse：一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。</li><li>robot parser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，用得比较少。</li></ul><h4 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h4><ul><li>urlopen()<br>urlopen()返回一个HTTPResponse类型的对象，主要包含read()、readinto()、getheader(name)、getheaders()、fileno()等方法，以及msg、version、status、reason、debuglevel、closed等属性。<br>urlopen()函数的API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, timeout=<span class="number">1</span>, cafile=<span class="literal">None</span>, </span><br><span class="line">                            capath=<span class="literal">None</span>, cadefault=<span class="literal">False</span>, context=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">data：需要是字节流编码格式，即bytes类型，请求方法变为POST</span><br><span class="line">timeout：单位为秒</span><br><span class="line">cafile和capath：指定CA证书及其路径</span><br><span class="line">cadefault：已弃用</span><br><span class="line">context：用来指定SSL设置，必须是ssl.SSLContext类型</span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'word'</span>:<span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">response= urllib.request.urlopen(<span class="string">'http://httpbin.org/post'</span>, data=data)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure></li><li>Request类<br>API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(ur1, data=None, headers=&#123;&#125;,</span></span></span><br><span class="line"><span class="class"><span class="params">                                  origin_req_host=None, unverifiable=False, method=None)</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">headers</span>：为一个字典，用来构造请求头，也可以后面用<span class="title">add_header</span><span class="params">()</span>添加，常用来修改<span class="title">User</span>-<span class="title">Agent</span></span></span><br><span class="line"><span class="class"><span class="title">origin_req_host</span>：请求方的<span class="title">host</span>名称或<span class="title">IP</span>地址</span></span><br><span class="line"><span class="class"><span class="title">unverifiable</span>：请求权限问题</span></span><br><span class="line"><span class="class"><span class="title">method</span>：请求方法</span></span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (Compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Germey'</span></span><br><span class="line">&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict),encoding=<span class="string">'utf8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure></li><li>用Opener构建Handler<br>官方文档：<a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler</a><ul><li>验证<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">'username'</span></span><br><span class="line">password = <span class="string">'password'</span></span><br><span class="line">url = <span class="string">'http://localhost:5000/'</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>代理<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:9743'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>Cookies<ul><li>获取Cookies<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">"="</span>+item.value)</span><br></pre></td></tr></table></figure></li><li>Cookie保存至文件<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'cookies.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line"><span class="comment">#cookie = http.cookiejar.LWPCookieJar(filename)  另一种格式</span></span><br><span class="line"><span class="comment">#cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)  读取</span></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="处理异常"><a href="#处理异常" class="headerlink" title="处理异常"></a>处理异常</h4><ul><li>URLError<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>HTTPError<br>URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre></td></tr></table></figure></li></ul><h4 id="解析链接"><a href="#解析链接" class="headerlink" title="解析链接"></a>解析链接</h4><p>url.parse模块，定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。<br>支持如下协议的URL处理：file、ftp、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。<br>常用方法如下：</p><ul><li><p>urlparse()<br>实现URL的识别与分段。<br>API：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">''</span>, allow_fragments=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">scheme：url中没有协议时，作为默认的协议。</span><br><span class="line">allow_fragments：是否忽略fragment。如果设置为<span class="literal">False</span>，fragment部分就会被忽略，</span><br><span class="line">                 它会被依次解析为query、parameters或者path的一部分，而fragment部分为空。</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html#comment'</span>,</span><br><span class="line">                   scheme=<span class="string">'https'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)  <span class="comment">#返回的result为ParseResult类型，实际上是一个元组，支持result[0]和result.scheme</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">ParseResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, </span><br><span class="line">            path=<span class="string">'/index.html#comment'</span>, params=<span class="string">''</span>, query=<span class="string">''</span>, fragment=<span class="string">''</span>)</span><br></pre></td></tr></table></figure></li><li><p>urlunparse()<br>实现URL的构造。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"></span><br><span class="line"><span class="comment">#data可以用其他类型，但长度必须是6</span></span><br><span class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunparse(data))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">http://www.baidu.com/index.html;user?a=6#comment</span><br></pre></td></tr></table></figure></li><li><p>urlsplit()<br>类似urlparse()，但只返回5个结果，params合并到path里。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"></span><br><span class="line">result = urlsplit(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">SplitResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, </span><br><span class="line">            path=<span class="string">'/index.html;user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre></td></tr></table></figure></li><li><p>urlunsplit()<br>类似urlunparse()，但只传入5个参数。</p></li><li><p>urljoin()</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">urljoin(base_url, target_url)</span><br><span class="line"></span><br><span class="line">分析base_url的scheme、netloc和path三个内容并对target_url进行补充</span><br></pre></td></tr></table></figure></li><li><p>urlencode()<br>将字典序列化为GET请求的参数。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">'http://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">http://www.baidu.com?name=germey&amp;age=22</span><br></pre></td></tr></table></figure></li><li><p>parse_qs()<br>将URL反序列化为字典。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">'name=germey&amp;age=22'</span></span><br><span class="line">print(parse_qs(query))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&#123;<span class="string">'name'</span>: [<span class="string">'germey'</span>], <span class="string">'age'</span>: [<span class="string">'22'</span>]&#125;</span><br></pre></td></tr></table></figure></li><li><p>parse_qsl()<br>将URL转化为元组组成的列表。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[(<span class="string">'name'</span>, <span class="string">'germey'</span>), (<span class="string">'age'</span>, <span class="string">'22'</span>)]</span><br></pre></td></tr></table></figure></li><li><p>quote()<br>将内容（中文字符）转化为URL编码的格式。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">keyword = <span class="string">'壁纸'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8</span><br></pre></td></tr></table></figure></li><li><p>unquote()<br>进行URL解码</p></li></ul><h4 id="分析Robots协议"><a href="#分析Robots协议" class="headerlink" title="分析Robots协议"></a>分析Robots协议</h4><ul><li><p>Robots协议<br>Robots协议也称作爬虫协议、机器人协议，全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robot.txt的文本文件，一般放在网站的根目录下。</p></li><li><p>爬虫名称<br>常见的搜索爬虫的名称及对应的网站：<br>BaiduSpider 百度 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a><br>Googlebot 谷歌 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>360Spider 360 搜索 <a href="http://www.so.com" target="_blank" rel="noopener">www.so.com</a><br>YodaoBot 有道 <a href="http://www.youdao.com" target="_blank" rel="noopener">www.youdao.com</a><br>ia_archiver Alexa <a href="http://www.alexa.cn" target="_blank" rel="noopener">www.alexa.cn</a><br>Scooter altavista <a href="http://www.altavista.com" target="_blank" rel="noopener">www.altavista.com</a></p></li><li><p>robotparser<br>urllib.robotparser模块提供了一个RobotFileParser类，该类的一些方法如下：</p><ul><li>set_url()：用来设置robots.txt文件的链接。也可在创建RobotFileParser对象时传入链接。</li><li>read()：读取robots.txt文件并进行分析。</li><li>parse()：用来解析robots.txt文件，传人的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。</li><li>can_fetch()：该方法传人两个参数，第一个是 User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，结果为True 或False。</li><li>mtime()：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是有必要的，可能需要定期检查来抓取最新的robots.txt。</li><li>modified()：同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间。</li></ul><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line"><span class="comment">#上面两行可以用parse方法来执行读取和分析</span></span><br><span class="line"><span class="comment">#rp.parse(urlopen('http://www.jianshu.com/robots.txt').read().decode('utf-8').split('\n'))</span></span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="使用requests"><a href="#使用requests" class="headerlink" title="使用requests"></a>使用requests</h3><p>解决urllib中Cookies、登录验证、代理设置不方便的问题。<br>安装<code>pip install requests</code><br>requests的官方文档：<a href="http://docs.python-requests.org/" target="_blank" rel="noopener">http://docs.python-requests.org/</a></p><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>requests库包含get()、post()、put()、delete()、head()和options()等方法，分别对应各种方式请求网页。</p><ul><li><p>GET请求</p><ul><li>基本实例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line"><span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=data)</span><br><span class="line">print(type(r))</span><br><span class="line">print(r.status)</span><br><span class="line"><span class="comment">#网页的返回类型实际是JSON格式的str类型，调用json()可将其转化为字典</span></span><br><span class="line">print(type(r.json()))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">requests</span>.<span class="title">models</span>.<span class="title">Response</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">dict</span>'&gt;</span></span><br></pre></td></tr></table></figure></li><li>抓取网页<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) '</span></span><br><span class="line">                    <span class="string">'AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                     <span class="string">'Chrome/52.0.2743.116 Safari/537.36'</span>)</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>, headers=headers)</span><br><span class="line">pattern = re.compile(<span class="string">'explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;'</span>, re.S)</span><br><span class="line">titles = re.findall(pattern, r.text)</span><br><span class="line">print(titles)</span><br></pre></td></tr></table></figure></li><li>抓取二进制数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">"https://github.com/favicon.ico"</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'favicon.ico'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(r.content)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>POST请求</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="string">'22'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=data)</span><br></pre></td></tr></table></figure></li><li><p>响应</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://www.xxyr.cc'</span>)</span><br><span class="line">print(type(r.status_code))</span><br><span class="line">print(type(r.headers))</span><br><span class="line">print(type(r.cookies))</span><br><span class="line">print(type(r.url))</span><br><span class="line">print(type(r.history))</span><br><span class="line">print(type(r.text))  <span class="comment">#返回内容的字符串形式</span></span><br><span class="line">print(type(r.content)) <span class="comment">#返回内容的二进制形式</span></span><br><span class="line">print(requests.codes.ok)  <span class="comment">#内置的返回码</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">int</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">requests</span>.<span class="title">structures</span>.<span class="title">CaseInsensitiveDict</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">requests</span>.<span class="title">cookies</span>.<span class="title">RequestsCookieJar</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">list</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">bytes</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h4><ul><li><p>文件上传</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">files = &#123;<span class="string">'file'</span>: open(<span class="string">'favicon.ico'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>, files=files)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li><li><p>Cookies</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">print(r.cookies)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> r.cookies.items():</span><br><span class="line">    print(key + <span class="string">'='</span> + value)</span><br></pre></td></tr></table></figure><p>可将Cookie字段添加到headers里实现登录：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>,</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://www.zhihu.com'</span>, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>或将其作为cookie参数添加到get()方法里：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">cookies = <span class="string">'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'</span></span><br><span class="line">jar = requests.cookies.RequestsCookieJar()</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> cookie <span class="keyword">in</span> cookies.split(<span class="string">';'</span>):</span><br><span class="line">    key, value = cookie.split(<span class="string">'='</span>, <span class="number">1</span>)</span><br><span class="line">    jar.set(key, value)</span><br><span class="line">r = requests.get(<span class="string">'http://www.zhihu.com'</span>, cookies=jar, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li><li><p>会话维持<br>当访问登录网站后的页面，或同一站点的不同页面时，就需要进行会话维持。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>)</span><br><span class="line">r = s.get(<span class="string">'http://httpbin.org/cookies'</span>)</span><br><span class="line">print(r.text)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"cookies"</span>: &#123;</span><br><span class="line">    <span class="string">"number"</span>: <span class="string">"123456789"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>SSL证书验证</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)  <span class="comment">#默认为True，自动验证证书</span></span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure><p><code>verify=False</code>会忽略证书的验证，但会报一个警告，解决方法如下：</p><ul><li>设置忽略警告<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line"></span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li><li>捕获警告到日志<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">logging.captureWarnings(<span class="literal">True</span>)</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li><li>指定一个本地证书用作客户端证书<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>代理设置</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line"><span class="string">'http'</span>: <span class="string">'http://10.10.1.10:3128'</span>,</span><br><span class="line"><span class="string">'https'</span>: <span class="string">'http://10.10.1.10:1080'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><p>若要使用HTTP Basic Auth，可以使用如下代理形式：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'http://user:password@10.10.1.10:3128/'</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><p>requests还支持SOCKS代理：<br>安装：<code>pip3 install &#39;requests[socks]&#39;</code></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://user:password@host:port'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://user:password@host:port'</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure></li><li><p>超时设置<br>为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应就报错。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>, timeout=<span class="number">1</span>)</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure><p>timeout=1表示超时时间为1秒，默认为None，即永远等待。<br>实际上，请求分为两个阶段，即连接（connect）和读取（read）。上面设置的timeout将用作连接和读取这二者的timeout总和。<br>如果要分别指定，就可以传入一个元组：<br><code>r = requests.get(&#39;https://www.taobao.com&#39;, timeout=(5,11, 30))</code></p></li><li><p>身份认证</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment">#auth=('username', 'password')即auth=HTTPBasicAuth('username', 'password')</span></span><br><span class="line">r = requests.get(<span class="string">'http://localhost:5000'</span>, auth=(<span class="string">'username'</span>, <span class="string">'password'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure><p>此外requests还提供了其他认证方式，如OAuth认证等。</p></li><li><p>Prepared Request<br>用于将请求表示为数据结构，其中各个参数通过一个Request对象来表示。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 '</span>)</span><br><span class="line">                    (<span class="string">'(KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>)</span><br><span class="line">&#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(<span class="string">'POST'</span>, url, data=data, headers=headers)</span><br><span class="line"><span class="comment">#Session的prepare_request()方法将其转化为一个Prepared Request对象</span></span><br><span class="line">prepped = s.prepare_request(req)  </span><br><span class="line">r = s.send(prepped)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li></ul><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>详见： <a href="https://xxyr.cc/post/tech/regex-review/">正则表达式复习</a></p><h3 id="抓取猫眼电影排行"><a href="#抓取猫眼电影排行" class="headerlink" title="抓取猫眼电影排行"></a>抓取猫眼电影排行</h3><p>详见<a href="https://github.com/Python3WebSpider/MaoYan" target="_blank" rel="noopener">https://github.com/Python3WebSpider/MaoYan</a></p><h2 id="Ch-3-解析库的使用"><a href="#Ch-3-解析库的使用" class="headerlink" title="Ch 3 解析库的使用"></a>Ch 3 解析库的使用</h2><h3 id="使用XPath"><a href="#使用XPath" class="headerlink" title="使用XPath"></a>使用XPath</h3><p>lxml库安装：<code>pip install lxml</code><br>如果想查询更多XPath的用法，可以查看：<a href="http://www.w3school.com.cn/xpath/index.asp" target="_blank" rel="noopener">http://www.w3school.eom.cn/xpath/index.asp</a><br>如果想查询更多lxml库的用法，可以查看：<a href="http://lxml.de/" target="_blank" rel="noopener">http://lxml.de／</a></p><h4 id="XPath概览"><a href="#XPath概览" class="headerlink" title="XPath概览"></a>XPath概览</h4><ul><li>XPath，全称XML Path Language，即XML路径语言，是一门在XML文档中查找信息的语言，最初是用来搜寻XML文档的，但同样适用于HTML文档的搜索。</li><li>XPath于1999年ll月16日成为W3C标准，它被设计为供XSLT、XPointer以及其他XML解析软件使用，更多的文档可以访问其官方网站：<a href="https://www.w3.org/TR/xpath" target="_blank" rel="noopener">https://www.w3.org/TR/xpath</a>。</li></ul><h4 id="Xpath常用规则"><a href="#Xpath常用规则" class="headerlink" title="Xpath常用规则"></a>Xpath常用规则</h4><p><img src="https://s3.ax1x.com/2021/01/11/s89S0K.png" alt=""></p><h4 id="实例引入"><a href="#实例引入" class="headerlink" title="实例引入"></a>实例引入</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line"><span class="comment"># html = etree.parse('./test.html', etree.HTMLParser())  #从文件读取</span></span><br><span class="line">result = etree.tostring(html) <span class="comment">#修正和补全HTML代码，但返回bytes类型</span></span><br><span class="line">print(result.decode(<span class="string">'utf-8'</span>)) <span class="comment">#转成str类型</span></span><br></pre></td></tr></table></figure><h4 id="节点选择"><a href="#节点选择" class="headerlink" title="节点选择"></a>节点选择</h4><ul><li>所有节点<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//*'</span>)   <span class="comment">#选择所有节点</span></span><br><span class="line"><span class="comment">#result = html.xpath('//li') #选择所有li节点</span></span><br><span class="line">print(result)</span><br><span class="line">print(result[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回形式是一个列表，每个元素是Element类型</span></span><br><span class="line">Output：</span><br><span class="line">[&lt;Element li at <span class="number">0x2257c34aa48</span>&gt;, &lt;Element li at <span class="number">0x2257c34aa88</span>&gt;]</span><br><span class="line">&lt;Element li at <span class="number">0x2257c34aa48</span>&gt;</span><br></pre></td></tr></table></figure></li><li>子节点<br>/是直接节点，//是所有节点。<br>如选择li节点的所有直接a子节点，可以用<code>//li/a</code>，<br>选择ul节点下的所有a子节点，可以用<code>//ul//a</code></li><li>父节点<br>首先选中href属性为link4.html的a节点，然后再获取其父节点，然后再获取其class属性。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/../@class'</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/parent::*/@class'</span>)</span><br></pre></td></tr></table></figure></li><li>按序选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/a/text()'</span>)           <span class="comment">#序号从1开始</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[last()]/a/text()'</span>)      <span class="comment">#选取最后一个</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[position()&lt;3]/a/text()'</span>)<span class="comment">#选取第1、2个</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[last()-2]/a/text()'</span>)    <span class="comment">#选取倒数第三个</span></span><br></pre></td></tr></table></figure>具体参考：<a href="http://www.w3school.com.cn/xpath/xpath_functions.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_functions.asp</a></li><li>节点轴选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::*'</span>)          <span class="comment">#选取第一个li节点的所有祖先节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::div'</span>)        <span class="comment">#选取第一个li节点的祖先div节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/attribute::*'</span>)         <span class="comment">#获取第一个li节点的所有属性值</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/child::a'</span>)             <span class="comment">#child::直接子节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/descendant::span'</span>)     <span class="comment">#descendant::子孙节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following::*[2]'</span>)      <span class="comment">#following::当前结点之后的所有节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following-sibling::*'</span>) <span class="comment">#following-sibling::</span></span><br><span class="line">                                                    <span class="comment">#当前结点之后的所有同级节点</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="文本获取"><a href="#文本获取" class="headerlink" title="文本获取"></a>文本获取</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]/a/text()'</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]//text()'</span>)</span><br><span class="line"><span class="comment">#前者准确获取li&gt;a内的文本，</span></span><br><span class="line"><span class="comment">#而后者获取li内的所有文本，可能会获取到换行符之类的信息</span></span><br></pre></td></tr></table></figure><h4 id="属性操作"><a href="#属性操作" class="headerlink" title="属性操作"></a>属性操作</h4><ul><li>属性获取<br>获取所有li节点下所有a节点的href属性：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a/@href'</span>)</span><br></pre></td></tr></table></figure></li><li>属性匹配<br>通过属性筛选节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a[@href="link1.html"]'</span>)</span><br></pre></td></tr></table></figure></li><li>属性多值匹配<br>用contains()函数：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li")]/a/text()'</span>)</span><br></pre></td></tr></table></figure></li><li>多属性匹配<br>用运算符and来连接：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first" name="item"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li") and @name="item"]/a/text()'</span>)</span><br></pre></td></tr></table></figure>XPath中的运算符：<br><img src="https://s3.ax1x.com/2021/01/12/sJ0Uaj.png" alt="运算符及其介绍"></li></ul><h3 id="使用Beautiful-Soup"><a href="#使用Beautiful-Soup" class="headerlink" title="使用Beautiful Soup"></a>使用Beautiful Soup</h3><p>安装：<code>pip install beautifulsoup4</code></p><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>Beautiful Soup是Python的一个HTML或XML的解析库，可以用它来方便地从网页中提取数据.<br>官方解释如下：</p><ul><li>Beautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。</li><li>Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。不需要考虑编码方式，除非文档没有指定一个编码方式，这时仅需说明一下原始编码方式就可以了。</li><li>Beautiful Soup已成为和lxml、html6lib 一样出色的Python解释器，为用户灵活地提供不同的解析策略或强劲的速度。</li></ul><h4 id="解析器"><a href="#解析器" class="headerlink" title="解析器"></a>解析器</h4><p>Beautiful Soup在解析时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器。（推荐lxml）<br><img src="https://s3.ax1x.com/2021/01/12/sJ26Lq.png" alt="Beautiful Soup支持的解析器"></p><h4 id="基本用法-1"><a href="#基本用法-1" class="headerlink" title="基本用法"></a>基本用法</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)  <span class="comment">#会自动补全HTML代码</span></span><br><span class="line">print(soup.prettify())              <span class="comment">#将要解析的字符串以标准缩进格式输出</span></span><br><span class="line">print(soup.title.string)            <span class="comment">#获取title节点的文本</span></span><br></pre></td></tr></table></figure><h4 id="节点选择器"><a href="#节点选择器" class="headerlink" title="节点选择器"></a>节点选择器</h4><ul><li>选择元素<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.title)</span><br><span class="line">print(type(soup.title))</span><br><span class="line">print(soup.title.string)</span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.p)   <span class="comment">#选择第一个匹配的p节点</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;</span></span><br><span class="line"><span class="string">&lt;class '</span>bs4.element.Tag<span class="string">'&gt;</span></span><br><span class="line"><span class="string">The Dormouse'</span>s story</span><br><span class="line">&lt;head&gt;&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line">&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure></li><li>提取信息<ul><li>获取名称<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.title.name)</span><br><span class="line">title</span><br></pre></td></tr></table></figure></li><li>获取属性<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.p.attrs)</span><br><span class="line">print(soup.p.attrs[<span class="string">'name'</span>])</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'class'</span>: [<span class="string">'title'</span>], <span class="string">'name'</span>: <span class="string">'dromouse'</span>&#125;</span><br><span class="line">dromouse</span><br></pre></td></tr></table></figure>或：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.p[<span class="string">'class'</span>])</span><br><span class="line">print(soup.p[<span class="string">'name'</span>])</span><br><span class="line"></span><br><span class="line">[<span class="string">'title'</span>]   <span class="comment">#class属性值可能有多个，所以返回列表</span></span><br><span class="line">dromouse</span><br></pre></td></tr></table></figure></li><li>获取内容<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.title.string)  <span class="comment">#选择第一个匹配的</span></span><br><span class="line">The Dormouse<span class="string">'s story</span></span><br></pre></td></tr></table></figure></li></ul></li><li>嵌套选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.head.title.string)   <span class="comment">#选择head内的title节点</span></span><br><span class="line">The Dormouse<span class="string">'s story</span></span><br></pre></td></tr></table></figure></li><li>关联选择<ul><li>子节点和子孙节点<br>直接子节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">soup.p.contents   <span class="comment">#返回包含各直接子节点的列表</span></span><br></pre></td></tr></table></figure>或<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, child <span class="keyword">in</span> enumerate(soup.p.children):   <span class="comment">#返回生成器类型</span></span><br><span class="line">  print(i, child)</span><br></pre></td></tr></table></figure>子孙节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, child <span class="keyword">in</span> enumerate(soup.p.descendants):</span><br><span class="line">  print(i, child)</span><br></pre></td></tr></table></figure></li><li>父节点和祖先节点<br>直接父节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.a.parent)</span><br></pre></td></tr></table></figure>祖先节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, parent <span class="keyword">in</span> enumerate(soup.p.parents):</span><br><span class="line">  print(i, parent)</span><br></pre></td></tr></table></figure></li><li>兄弟节点<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Next Sibling'</span>, soup.a.next_sibling)</span><br><span class="line">print(<span class="string">'Prev Sibling'</span>, soup.a.previous_sibling)</span><br><span class="line">print(<span class="string">'Next Siblings'</span>, list(enumerate(soup.a.next_siblings)))</span><br><span class="line">print(<span class="string">'Prev Siblings'</span>, list(enumerate(soup.a.previous_siblings)))</span><br></pre></td></tr></table></figure></li><li>提取信息<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.a.next_sibling.string)</span><br><span class="line">print(list(soup.a.parents)[<span class="number">0</span>].attrs[<span class="string">'class'</span>])</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="方法选择器"><a href="#方法选择器" class="headerlink" title="方法选择器"></a>方法选择器</h4><ul><li>findall()<br>API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">find_all(name, attrs, recursive, text, **kwargs)</span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">print(soup.find_all(name=<span class="string">'ul'</span>))</span><br><span class="line">print(type(soup.find_all(name=<span class="string">'ul'</span>)[<span class="number">0</span>]))</span><br><span class="line"><span class="comment">#返回结果为列表类型，每个元素为bs4.element.Tag类型</span></span><br><span class="line"></span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">'id'</span>: <span class="string">'list-1'</span>&#125;))</span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">'name'</span>: <span class="string">'elements'</span>&#125;))</span><br><span class="line">或</span><br><span class="line">print(soup.find_all(id=<span class="string">'list-1'</span>))</span><br><span class="line">print(soup.find_all(class_=<span class="string">'element'</span>))    <span class="comment">#class_因为是关键字</span></span><br><span class="line"></span><br><span class="line">print(soup.find_all(text=re.compile(<span class="string">'link'</span>)))</span><br><span class="line"><span class="comment">#参数可以是字符串，也可以是正则表达式对象</span></span><br></pre></td></tr></table></figure></li><li>find()<br>返回第一个匹配的元素。</li><li>find_parents()和find_parent()：前者返回所有祖先节点，后者返回直接父节点。</li><li>find_next_siblings()和find_next_sibling()：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。</li><li>find_previous_siblings()和find_previous_sibling()：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点 。</li><li>find_all_next()和find_next()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。</li><li>find_all_previous()和find_previous()：前者返回节点前所有符合条件的节点，后者返回第一个符合条件的节点。</li></ul><h4 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><p>参考：<a href="http://www.w3school.com.cn/cssref/css_selectors.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/cssref/css_selectors.asp</a></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.select(<span class="string">'.panel .panel-heading'</span>))   <span class="comment">#返回列表类型</span></span><br><span class="line">print(soup.select(<span class="string">'ul li'</span>))</span><br><span class="line">print(soup.select(<span class="string">'#list-2 .element'</span>))</span><br><span class="line">print(type(soup.select(<span class="string">'ul'</span>)[<span class="number">0</span>]))             <span class="comment">#返回bs4.element.Tag类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#嵌套选择</span></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul.select(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取属性</span></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul[<span class="string">'id'</span>])</span><br><span class="line">    print(ul.attrs[<span class="string">'id'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取文本</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> soup.select(<span class="string">'li'</span>):</span><br><span class="line">    print(<span class="string">'Get Text:'</span>, li.get_text())</span><br><span class="line">    print(<span class="string">'String:'</span>, li.string)</span><br></pre></td></tr></table></figure><h3 id="使用pyquery"><a href="#使用pyquery" class="headerlink" title="使用pyquery"></a>使用pyquery</h3><p>安装<code>pip install pyquery</code><br>pyquery的官方文档：<a href="http://pyquery.readthedocs.io" target="_blank" rel="noopener">http://pyquery.readthedocs.io</a></p><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line"><span class="comment">#字符串初始化</span></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">print(doc(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#URL初始化</span></span><br><span class="line">doc = PyQuery(url=<span class="string">'http://xxyr.cc'</span>)</span><br><span class="line">print(doc(<span class="string">'title'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件初始化</span></span><br><span class="line">doc = PyQuery(filename=<span class="string">'demo.html'</span>)</span><br><span class="line">print(doc(<span class="string">'li'</span>))</span><br></pre></td></tr></table></figure><h4 id="基本CSS选择器"><a href="#基本CSS选择器" class="headerlink" title="基本CSS选择器"></a>基本CSS选择器</h4><p>关于CSS选择器的更多用法，可以参考：<a href="https://www.w3school.com.cn/css/css_selector_type.asp" target="_blank" rel="noopener">https://www.w3school.com.cn/css/css_selector_type.asp</a></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">print(doc(<span class="string">'#container .list li'</span>))         <span class="comment"># #id  .class</span></span><br><span class="line">print(type(doc(<span class="string">'#container .list li'</span>)))   </span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br></pre></td></tr></table></figure><h4 id="查找结点"><a href="#查找结点" class="headerlink" title="查找结点"></a>查找结点</h4><p>下面介绍一些常用的查询函数，这些函数和jQuery中函数的用法完全相同。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">items = doc(<span class="string">'.list'</span>)</span><br><span class="line">li = doc(<span class="string">'.list .item-0.active'</span>)</span><br><span class="line"></span><br><span class="line">lis1 = items.find(<span class="string">'li'</span>)          <span class="comment">#所有子孙节点</span></span><br><span class="line">lis2 = items.children(<span class="string">'.active'</span>) <span class="comment">#子节点</span></span><br><span class="line"></span><br><span class="line">container = items.parent()       <span class="comment">#获取直接父节点</span></span><br><span class="line">parents = items.parents()        <span class="comment">#获取所有祖先节点，可添加CSS选择器获取特定祖先节点</span></span><br><span class="line"></span><br><span class="line">bro = li.siblings()              <span class="comment">#获取所有兄弟节点，可添加CSS选择器获取特定兄弟节点</span></span><br></pre></td></tr></table></figure><h4 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">lis = doc(<span class="string">'li'</span>).items()</span><br><span class="line">print(type(lis))                <span class="comment">#&lt;class 'generator'&gt;</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> lis:</span><br><span class="line">    print(li, type(li))         <span class="comment">#&lt;class 'pyquery.pyquery.PyQuery'&gt;</span></span><br></pre></td></tr></table></figure><h4 id="获取信息"><a href="#获取信息" class="headerlink" title="获取信息"></a>获取信息</h4><ul><li>获取属性<br>提取到某个PyQuery类型的节点后，就可以调用attr()方法来获取属性：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">a = doc(<span class="string">'.item-0.active a'</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(type(a))</span><br><span class="line">print(a.attr(<span class="string">'href'</span>))   <span class="comment">#若a有多个，只返回第一个a的属性值</span></span><br><span class="line">print(a.attr.href)</span><br><span class="line"></span><br><span class="line">&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">link3</span>.<span class="title">html</span></span></span><br><span class="line"><span class="class"><span class="title">link3</span>.<span class="title">html</span></span></span><br></pre></td></tr></table></figure></li><li>获取文本<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'li'</span>)</span><br><span class="line">print(li.html())    <span class="comment">#返回第一个节点的内部HTML文本</span></span><br><span class="line">print(type(li.html()))</span><br><span class="line">print(li.text())    <span class="comment">#返回所有节点内部的纯文本，中间用一个空格分隔开</span></span><br><span class="line">print(type(li.text()))</span><br><span class="line"></span><br><span class="line">&lt;a href="link2.html"&gt;second item&lt;/a&gt;</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">second</span> <span class="title">item</span> <span class="title">third</span> <span class="title">item</span> <span class="title">fourth</span> <span class="title">item</span> <span class="title">fifth</span> <span class="title">item</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="节点操作"><a href="#节点操作" class="headerlink" title="节点操作"></a>节点操作</h4><p>pyquery提供了一系列方法来对节点进行动态修改，比如为某个节点添加一个 class，移除某个节点等，如append()、empty()和prepend()等方法，它们和jQuery的用法完全一致，详细的用法可以参考官方文档：<a href="http://pyquery.readthedocs.io/en/latest/api.html" target="_blank" rel="noopener">http://pyquery.readthedocs.io/en/latest/api.html</a></p><ul><li>addClass和removeClass<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'.item-0.active'</span>)</span><br><span class="line">li.removeClass(<span class="string">'active'</span>)</span><br><span class="line">li.addClass(<span class="string">'active'</span>)</span><br></pre></td></tr></table></figure></li><li>attr、text和html<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">li.attr(<span class="string">'name'</span>, <span class="string">'link'</span>)             <span class="comment">#修改或增加name属性值为link</span></span><br><span class="line">li.text(<span class="string">'changed item'</span>)             <span class="comment">#将li节点内部的文本替换为纯文本</span></span><br><span class="line">li.html(<span class="string">'&lt;span&gt;changed item&lt;/span&gt;'</span>)<span class="comment">#将li节点内部的文本替换为HTML文本</span></span><br></pre></td></tr></table></figure></li><li>remove()<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">wrap = doc(<span class="string">'.wrap'</span>)</span><br><span class="line">wrap.find(<span class="string">'p'</span>).remove()</span><br><span class="line">type(wrap.find(<span class="string">'p'</span>))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="伪类选择器"><a href="#伪类选择器" class="headerlink" title="伪类选择器"></a>伪类选择器</h4><p>css选择器之所以强大，还有一个很重要的原因，就是支持多种多样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等。示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'li:first-child'</span>)</span><br><span class="line">li = doc(<span class="string">'li:last-child'</span>)</span><br><span class="line">li = doc(<span class="string">'li:nth-child(2)'</span>)     <span class="comment">#第二个节点</span></span><br><span class="line">li = doc(<span class="string">'li:gt(2)'</span>)            <span class="comment">#第三个li之后的li节点</span></span><br><span class="line">li = doc(<span class="string">'li:nth-child(2n)'</span>)    <span class="comment">#偶数位置的节点</span></span><br><span class="line">li = doc(<span class="string">'li:contains(second)'</span>) <span class="comment">#包含second文本的li节点</span></span><br></pre></td></tr></table></figure><h2 id="Ch-4-数据存储"><a href="#Ch-4-数据存储" class="headerlink" title="Ch 4 数据存储"></a>Ch 4 数据存储</h2><h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><h4 id="TXT文本存储"><a href="#TXT文本存储" class="headerlink" title="TXT文本存储"></a>TXT文本存储</h4><h4 id="JSON文件存储"><a href="#JSON文件存储" class="headerlink" title="JSON文件存储"></a>JSON文件存储</h4><h4 id="CSV文件存储"><a href="#CSV文件存储" class="headerlink" title="CSV文件存储"></a>CSV文件存储</h4><h3 id="关系型数据库存储"><a href="#关系型数据库存储" class="headerlink" title="关系型数据库存储"></a>关系型数据库存储</h3><h4 id="MySQL存储"><a href="#MySQL存储" class="headerlink" title="MySQL存储"></a>MySQL存储</h4><h3 id="非关系型数据库存储"><a href="#非关系型数据库存储" class="headerlink" title="非关系型数据库存储"></a>非关系型数据库存储</h3><h4 id="MongoDB存储"><a href="#MongoDB存储" class="headerlink" title="MongoDB存储"></a>MongoDB存储</h4><h4 id="Redis存储"><a href="#Redis存储" class="headerlink" title="Redis存储"></a>Redis存储</h4><h2 id="Ch-5-Ajax数据爬取"><a href="#Ch-5-Ajax数据爬取" class="headerlink" title="Ch 5 Ajax数据爬取"></a>Ch 5 Ajax数据爬取</h2><h3 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h3><h3 id="Ajax分析方法"><a href="#Ajax分析方法" class="headerlink" title="Ajax分析方法"></a>Ajax分析方法</h3><h3 id="Ajax结果提取"><a href="#Ajax结果提取" class="headerlink" title="Ajax结果提取"></a>Ajax结果提取</h3><h3 id="分析Ajax爬取今日头条节拍美图"><a href="#分析Ajax爬取今日头条节拍美图" class="headerlink" title="分析Ajax爬取今日头条节拍美图"></a>分析Ajax爬取今日头条节拍美图</h3><h2 id="Ch-6-动态渲染页面爬取"><a href="#Ch-6-动态渲染页面爬取" class="headerlink" title="Ch 6 动态渲染页面爬取"></a>Ch 6 动态渲染页面爬取</h2><h3 id="Selenium的使用"><a href="#Selenium的使用" class="headerlink" title="Selenium的使用"></a>Selenium的使用</h3><h3 id="Splash的使用"><a href="#Splash的使用" class="headerlink" title="Splash的使用"></a>Splash的使用</h3><h3 id="Splash负载均衡配置"><a href="#Splash负载均衡配置" class="headerlink" title="Splash负载均衡配置"></a>Splash负载均衡配置</h3><h3 id="使用Selenium爬取淘宝商品"><a href="#使用Selenium爬取淘宝商品" class="headerlink" title="使用Selenium爬取淘宝商品"></a>使用Selenium爬取淘宝商品</h3><h2 id="Ch-7-验证码的识别"><a href="#Ch-7-验证码的识别" class="headerlink" title="Ch 7 验证码的识别"></a>Ch 7 验证码的识别</h2><h3 id="图形验证码识别"><a href="#图形验证码识别" class="headerlink" title="图形验证码识别"></a>图形验证码识别</h3><h3 id="滑动验证码识别"><a href="#滑动验证码识别" class="headerlink" title="滑动验证码识别"></a>滑动验证码识别</h3><h3 id="点触验证码识别"><a href="#点触验证码识别" class="headerlink" title="点触验证码识别"></a>点触验证码识别</h3><h3 id="宫格验证码识别"><a href="#宫格验证码识别" class="headerlink" title="宫格验证码识别"></a>宫格验证码识别</h3><h2 id="Ch-8-代理的使用"><a href="#Ch-8-代理的使用" class="headerlink" title="Ch 8 代理的使用"></a>Ch 8 代理的使用</h2><h3 id="代理的设置"><a href="#代理的设置" class="headerlink" title="代理的设置"></a>代理的设置</h3><h3 id="代理池的维护"><a href="#代理池的维护" class="headerlink" title="代理池的维护"></a>代理池的维护</h3><h3 id="付费代理的使用"><a href="#付费代理的使用" class="headerlink" title="付费代理的使用"></a>付费代理的使用</h3><h3 id="ADSL拨号代理"><a href="#ADSL拨号代理" class="headerlink" title="ADSL拨号代理"></a>ADSL拨号代理</h3><h3 id="使用代理爬取微信公众号文章"><a href="#使用代理爬取微信公众号文章" class="headerlink" title="使用代理爬取微信公众号文章"></a>使用代理爬取微信公众号文章</h3><h2 id="Ch-9-模拟登录"><a href="#Ch-9-模拟登录" class="headerlink" title="Ch 9 模拟登录"></a>Ch 9 模拟登录</h2><h3 id="模拟登录并爬取GitHub"><a href="#模拟登录并爬取GitHub" class="headerlink" title="模拟登录并爬取GitHub"></a>模拟登录并爬取GitHub</h3><h3 id="Cookie池的搭建"><a href="#Cookie池的搭建" class="headerlink" title="Cookie池的搭建"></a>Cookie池的搭建</h3><h2 id="Ch-10-APP的爬取"><a href="#Ch-10-APP的爬取" class="headerlink" title="Ch 10 APP的爬取"></a>Ch 10 APP的爬取</h2><h3 id="Charles的使用"><a href="#Charles的使用" class="headerlink" title="Charles的使用"></a>Charles的使用</h3><h3 id="mitmproxy的使用"><a href="#mitmproxy的使用" class="headerlink" title="mitmproxy的使用"></a>mitmproxy的使用</h3><h3 id="mitmdump爬取“得到”电子书信息"><a href="#mitmdump爬取“得到”电子书信息" class="headerlink" title="mitmdump爬取“得到”电子书信息"></a>mitmdump爬取“得到”电子书信息</h3><h3 id="APPium的基本使用"><a href="#APPium的基本使用" class="headerlink" title="APPium的基本使用"></a>APPium的基本使用</h3><h3 id="APPium爬取微信朋友圈"><a href="#APPium爬取微信朋友圈" class="headerlink" title="APPium爬取微信朋友圈"></a>APPium爬取微信朋友圈</h3><h3 id="APPium-mitmdump爬取京东商品"><a href="#APPium-mitmdump爬取京东商品" class="headerlink" title="APPium+mitmdump爬取京东商品"></a>APPium+mitmdump爬取京东商品</h3><h2 id="Ch-11-pyspider框架使用"><a href="#Ch-11-pyspider框架使用" class="headerlink" title="Ch 11 pyspider框架使用"></a>Ch 11 pyspider框架使用</h2><p><span style="border-bottom:2px solid red">pyspider框架应用场景较为单一，且可扩展程度不足，比较适合爬取固定分页展示内容的数据。</span></p><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><ul><li>pyspider是由国人binux编写的强大的网络爬虫系统，其GitHub地址为<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a>，官方文档地址为<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a>。</li><li>pyspider带有强大的 WebUI、脚本编辑器、任务监控器、项目管理器以及结果处理器，它支持多种数据库后端、多种消息队列、JavaScript渲染页面的爬取，使用起来非常方便。</li><li>pyspider开发快速便捷，适合中小型项目。</li></ul><h4 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h4><ul><li>提供方便易用的WebUI系统，可视化地编写和调试爬虫。</li><li>提供爬取进度监控、爬取结果查看、爬虫项目管理等功能。</li><li>支持多种后端数据库，如MySQL、MongoDB、Redis、SQLite、Elasticsearch、PostgreSQL。</li><li>支持多种消息队列，如RabbitMQ、Beanstalk、Redis、Kombu。</li><li>提供优先级控制、失败重试、定时抓取等功能。</li><li>对接了PhantomJS，可以抓取JavaScript渲染的页面。</li><li>支持单机和分布式部署，支持Docker部署。</li></ul><h4 id="与Scrapy比较"><a href="#与Scrapy比较" class="headerlink" title="与Scrapy比较"></a>与Scrapy比较</h4><ul><li>pyspider提供了WebUI，爬虫的编写、调试都是在WebUI中进行的。而Scrapy原生是不具备这个功能的，它采用的是代码和命令行操作，但可以通过对接Portia实现可视化配置。</li><li>pyspider调试非常方便，WebUI操作便捷直观。Scrapy则是使用parse命令进行调试，不够方便。</li><li>pyspider支持PhantomJS来进行JavaScript谊染页面的采集。Scrapy可以对接Scrapy-Splash组件，这需要额外配置。</li><li><span style="border-bottom:2px solid red">pyspider中内置了pyquery作为选择器</span>。Scrapy对接了XPath、css选择器和正则匹配。</li><li>pyspider的可扩展程度不足，可配制化程度不高。Scrapy可以通过对接Middleware、Pipeline、Extension等组件实现非常强大的功能，模块之间的耦合程度低，可扩展程度极高。</li></ul><h4 id="pyspider架构"><a href="#pyspider架构" class="headerlink" title="pyspider架构"></a>pyspider架构</h4><p>pyspider的架构主要分为Scheduler（调度器）、Fetcher（ 抓取器）、Processer（处理器）三个部分，整个爬取过程受到Monitor（监控器）的监控，抓取的结果被Result Worker（结果处理器）处理，如图：<br><img src="https://s3.ax1x.com/2021/01/17/sse47d.png" alt="pyspider 架构图"><br>Scheduler发起任务调度，Fetcher负责抓取网页内容，Processer负责解析网页内容，然后将新生成的Request发给Scheduler进行调度，将生成的提取结果输出保存。<br>执行过程如下：</p><ul><li>每个pyspider的项目对应一个Python脚本，该脚本中定义了一个Handler类，它有一个on_start()方法。爬取首先调用on_start()方法生成最初的抓取任务，然后发送给Scheduler进行调度。</li><li>Scheduler将抓取任务分发给Fetcher进行抓取，Fetcher执行并得到响应，随后将响应发送给Processer。</li><li>Processer处理响应并提取出新的URL生成新的抓取任务，然后通过消息队列的方式通知Schduler当前抓取任务执行情况，并将新生成的抓取任务发送给 Scheduler。如果生成了新的提取结果，则将其发送到结果队列等待Result Worker处理。</li><li>Scheduler接收到新的抓取任务，然后查询数据库，判断其如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其发送回Fetcher进行抓取。</li><li>不断重复以上工作，直到所有的任务都执行完毕，抓取结束。</li><li>抓取结束后，程序会回调on_finished()方法，这里可以定义后处理过程。</li></ul><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h4><ul><li>官方文档：<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a></li><li>PyPI：<a href="https://pypi.python.org/pypi/pyspider" target="_blank" rel="noopener">https://pypi.python.org/pypi/pyspider</a></li><li>GitHub：<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a></li><li>官方教程：<a href="http://docs.pyspider.org/en/latest/tutorial" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/tutorial</a></li><li>在线实例：<a href="http://demo.pyspider.org" target="_blank" rel="noopener">http://demo.pyspider.org</a></li></ul><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ul><li><code>pip3 install pyspider</code></li><li>PhantomJS安装：<a href="https://gitee.com/xxyrs/filehouse/raw/master/phantomjs-2.1.1-windows.zip" target="_blank" rel="noopener">phantomjs-2.1.1-windows.zip</a>，解压后将bin路径配置到用户Path中</li><li>常见错误及解决：<br><a href="https://www.cnblogs.com/Mayfly-nymph/p/10808088.html" target="_blank" rel="noopener">https://www.cnblogs.com/Mayfly-nymph/p/10808088.html</a><br><a href="https://github.com/binux/pyspider/issues/898" target="_blank" rel="noopener">https://github.com/binux/pyspider/issues/898</a><br><a href="https://www.cnblogs.com/shaosks/p/6856086.html" target="_blank" rel="noopener">https://www.cnblogs.com/shaosks/p/6856086.html</a></li><li>启动：<code>pyspider [all]</code></li></ul><h4 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspider.libs.base_handler <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config = &#123;</span><br><span class="line">        <span class="string">'itag'</span>: <span class="string">'v223'</span>,</span><br><span class="line">        <span class="string">"headers"</span>:&#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: UserAgent().random,</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @every(minutes=24 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.crawl(<span class="string">'http://travel.qunar.com/travelbook/list.htm'</span>, callback=self.index_page, fetch_type=<span class="string">'js'</span>, validate_cert=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @config(age=10 * 24 * 60 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.doc(<span class="string">'li &gt; .tit &gt; a'</span>).items():</span><br><span class="line">            <span class="comment">#fetch_type='js'，使用PhantomJS渲染</span></span><br><span class="line">            self.crawl(each.attr.href, callback=self.detail_page, validate_cert=<span class="literal">False</span>, fetch_type=<span class="string">'js'</span>)</span><br><span class="line">        <span class="comment">#获取下一页链接</span></span><br><span class="line">        next = response.doc(<span class="string">'.next'</span>).attr.href</span><br><span class="line">        self.crawl(next, callback=self.index_page, validate_cert=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @config(priority=2)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'url'</span>: response.url,</span><br><span class="line">            <span class="string">'title'</span>: response.doc(<span class="string">'#booktitle'</span>).text(),</span><br><span class="line">            <span class="string">'date'</span>: response.doc(<span class="string">'.when .data'</span>).text(),</span><br><span class="line">            <span class="string">'day'</span>: response.doc(<span class="string">'.howlong .data'</span>).text(),</span><br><span class="line">            <span class="string">'who'</span>: response.doc(<span class="string">'.who .data'</span>).text(),</span><br><span class="line">            <span class="string">'text'</span>: response.doc(<span class="string">'#b_panel_schedule'</span>).text(),</span><br><span class="line">            <span class="comment"># "btall": [(x.find('a').text(), x.find('a').eq(1).attr.href[0:59]) for x in response.doc('.dlist li').items()],</span></span><br><span class="line">            <span class="string">'image'</span>: response.doc(<span class="string">'.cover_img'</span>).attr.src</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h4 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h4><p><img src="https://s3.ax1x.com/2021/01/18/sy04PI.png" alt="新建项目"></p><h4 id="爬取首页"><a href="#爬取首页" class="headerlink" title="爬取首页"></a>爬取首页</h4><p><img src="https://s3.ax1x.com/2021/01/18/sywvjO.png" alt="爬取首页"></p><h4 id="爬取详情页"><a href="#爬取详情页" class="headerlink" title="爬取详情页"></a>爬取详情页</h4><p><img src="https://s3.ax1x.com/2021/01/18/sysBwD.png" alt="爬取详情页"><br>爬取的页面无法显示图片，出现此现象的原因是pyspider默认发送HTTP请求，请求的HTML文档本身就不包含img节点。但是在浏览器中我们看到了图片，这是因为这张图片是后期经过JavaScript出现的。<br>pyspider内部对接了PhantomJS，将index_page()中生成抓取详情页的请求方法添加一个参数 <code>fetch_type=&#39;js&#39;</code>即可</p><h4 id="启动爬虫"><a href="#启动爬虫" class="headerlink" title="启动爬虫"></a>启动爬虫</h4><ul><li>在最左侧可以定义项目的分组，以便管理。</li><li>rate/burst代表当前的爬取速率，rate代表每秒发出多少个请求，burst（并发数）相当于流量控制中的令牌桶算法的令牌数，rate和burst设置的越大，爬取速率越快。</li><li>process中的5m、1h、1d指的是最近5分、l小时、l天内的请求情况，all代表所有的请求情况。请求由不同颜色表示，蓝色的代表等待被执行的请求，绿色的代表成功的请求，黄色的代表请求失败后等待重试的请求，红色的代表失败次数过多被忽略的请求，这样可以直观知道爬取的进度和请求情况。</li><li>点击Active Tasks，可查看最近请求的详细状况。</li><li>点击Results，查看所有爬取结果。</li></ul><p><img src="https://s3.ax1x.com/2021/01/18/s61te0.png" alt=""></p><h3 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h3><p>参见官方文档：<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a></p><h4 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h4><p><a href="http://docs.pyspider.org/en/latest/Command-Line/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/Command-Line/</a></p><h4 id="crawl-方法"><a href="#crawl-方法" class="headerlink" title="crawl()方法"></a>crawl()方法</h4><p><a href="http://docs.pyspider.org/en/latest/apis/self.crawl/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/apis/self.crawl/</a></p><h4 id="任务区分"><a href="#任务区分" class="headerlink" title="任务区分"></a>任务区分</h4><p>在pyspider中判断两个任务是否是重复的，使用的是该任务对应的URL的MD5值作为任务的唯一ID，如果ID相同，那么两个任务就会判定为相同，其中一个就不会爬取。这时可以重写task_id()方法，改变这个ID的计算方式来实现不同任务的区分，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pyspider. libs. utils <span class="keyword">import</span> mdsstring</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_taskid</span><span class="params">(self, task)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> mdsstring(task[<span class="string">'url ’]+json.dumps(task[’ fetch'</span>].get (’ data <span class="string">' , " )) )</span></span><br></pre></td></tr></table></figure><h4 id="全局配置"><a href="#全局配置" class="headerlink" title="全局配置"></a>全局配置</h4><p>pyspider可以使用crawl_config来指定全局的配置，配置中的参数会和crawl()方法创建任务时的参数合井。<br>如要全局配置一个Headers，可以定义如下代码：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">  crawl_config = &#123;</span><br><span class="line">      <span class="string">'headers'</span>:&#123;</span><br><span class="line">          <span class="string">'User-Agent'</span>:<span class="string">'GoogleBot'</span>,</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="定时爬取"><a href="#定时爬取" class="headerlink" title="定时爬取"></a>定时爬取</h4><p>通过every属性来设置爬取的时间间隔：<br>在有效时间内爬取不会重复。所以要把有效时间设置得比重复时间更短，即age的时间小于minutes的时间这样才可以实现定时爬取。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#minutes为爬取的时间间隔，单位为分钟</span></span><br><span class="line"><span class="comment">#或写为seconds秒数</span></span><br><span class="line"><span class="meta">@every(minutes=24 * 60)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">  self.crawl(<span class="string">'http://www.example.org'</span>, callback=self.index_page)</span><br><span class="line"></span><br><span class="line"><span class="comment">#age为任务的有效时间，单位为秒</span></span><br><span class="line"><span class="meta">@config(age=10*24*60*60)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self)</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="项目状态"><a href="#项目状态" class="headerlink" title="项目状态"></a>项目状态</h4><ul><li>TODO：它是项目刚刚被创建还未实现时的状态。</li><li>STOP：如果想停止某项目的抓取，可以将项目的状态设置为STOP 。</li><li>CHECKING：正在运行的项目被修改后就会变成CHECKING状态，项目在中途出错需要调整的时候会遇到这种情况。</li><li>DEBUG/RUNNING：这两个状态对项目的运行没有影响，状态设置为任意一个，项目都可以运行，但是可以用二者来区分项目是否已经测试通过。</li><li>PAUSE：当爬取过程中出现连续多次错误时，项目会自动设置为PAUSE状态，并等待一定时间后继续爬取。</li></ul><h4 id="删除项目"><a href="#删除项目" class="headerlink" title="删除项目"></a>删除项目</h4><p>pyspider中没有直接删除项目的选项。如要删除任务，那么将项目的状态设置为STOP，将分组的名称设置为delete，等待24小时，则项目会自动删除。</p><h2 id="Ch-12-Scrapy框架使用"><a href="#Ch-12-Scrapy框架使用" class="headerlink" title="Ch 12 Scrapy框架使用"></a>Ch 12 Scrapy框架使用</h2><h2 id="Ch-13-分布式爬虫"><a href="#Ch-13-分布式爬虫" class="headerlink" title="Ch 13 分布式爬虫"></a>Ch 13 分布式爬虫</h2><h2 id="Ch-14-分布式爬虫的部署"><a href="#Ch-14-分布式爬虫的部署" class="headerlink" title="Ch 14 分布式爬虫的部署"></a>Ch 14 分布式爬虫的部署</h2></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>不负骤雨</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://xxyr.cc/post/Python/python-spider/" title="Python爬虫基础">https://xxyr.cc/post/Python/python-spider/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python/" rel="tag"># Python</a> <a href="/tags/Spider/" rel="tag"># Spider</a> <a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"># 爬虫</a></div><div class="post-nav"><div class="post-nav-item"><a href="/post/Networks/wireshark/" rel="prev" title="WireShark网络分析"><i class="fa fa-chevron-left"></i> WireShark网络分析</a></div><div class="post-nav-item"><a href="/post/Tips/common-commands/" rel="next" title="常用命令">常用命令 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-1-爬虫基础"><span class="nav-number">1.</span> <span class="nav-text">Ch 1 爬虫基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HTTP基本原理"><span class="nav-number">1.1.</span> <span class="nav-text">HTTP基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#URI和URL"><span class="nav-number">1.1.1.</span> <span class="nav-text">URI和URL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#超文本"><span class="nav-number">1.1.2.</span> <span class="nav-text">超文本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTP和HTTPS"><span class="nav-number">1.1.3.</span> <span class="nav-text">HTTP和HTTPS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTP请求过程"><span class="nav-number">1.1.4.</span> <span class="nav-text">HTTP请求过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#请求"><span class="nav-number">1.1.5.</span> <span class="nav-text">请求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#响应"><span class="nav-number">1.1.6.</span> <span class="nav-text">响应</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网页基础"><span class="nav-number">1.2.</span> <span class="nav-text">网页基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#网页的组成"><span class="nav-number">1.2.1.</span> <span class="nav-text">网页的组成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网页的结构"><span class="nav-number">1.2.2.</span> <span class="nav-text">网页的结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点树及节点间关系"><span class="nav-number">1.2.3.</span> <span class="nav-text">节点树及节点间关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#选择器"><span class="nav-number">1.2.4.</span> <span class="nav-text">选择器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫的基本原理"><span class="nav-number">1.3.</span> <span class="nav-text">爬虫的基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#爬虫概述"><span class="nav-number">1.3.1.</span> <span class="nav-text">爬虫概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#能抓怎样的数据"><span class="nav-number">1.3.2.</span> <span class="nav-text">能抓怎样的数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JavaScript渲染页面"><span class="nav-number">1.3.3.</span> <span class="nav-text">JavaScript渲染页面</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#会话和Cookies"><span class="nav-number">1.4.</span> <span class="nav-text">会话和Cookies</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#静态网页和动态网页"><span class="nav-number">1.4.1.</span> <span class="nav-text">静态网页和动态网页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#无状态HTTP"><span class="nav-number">1.4.2.</span> <span class="nav-text">无状态HTTP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见误区"><span class="nav-number">1.4.3.</span> <span class="nav-text">常见误区</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代理的基本原理"><span class="nav-number">1.5.</span> <span class="nav-text">代理的基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本原理"><span class="nav-number">1.5.1.</span> <span class="nav-text">基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代理的作用"><span class="nav-number">1.5.2.</span> <span class="nav-text">代理的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代理分类"><span class="nav-number">1.5.3.</span> <span class="nav-text">代理分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见代理设置"><span class="nav-number">1.5.4.</span> <span class="nav-text">常见代理设置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-2-基本库的使用"><span class="nav-number">2.</span> <span class="nav-text">Ch 2 基本库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用urllib"><span class="nav-number">2.1.</span> <span class="nav-text">使用urllib</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#发送请求"><span class="nav-number">2.1.1.</span> <span class="nav-text">发送请求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#处理异常"><span class="nav-number">2.1.2.</span> <span class="nav-text">处理异常</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解析链接"><span class="nav-number">2.1.3.</span> <span class="nav-text">解析链接</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分析Robots协议"><span class="nav-number">2.1.4.</span> <span class="nav-text">分析Robots协议</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用requests"><span class="nav-number">2.2.</span> <span class="nav-text">使用requests</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本用法"><span class="nav-number">2.2.1.</span> <span class="nav-text">基本用法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#高级用法"><span class="nav-number">2.2.2.</span> <span class="nav-text">高级用法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则表达式"><span class="nav-number">2.3.</span> <span class="nav-text">正则表达式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#抓取猫眼电影排行"><span class="nav-number">2.4.</span> <span class="nav-text">抓取猫眼电影排行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-3-解析库的使用"><span class="nav-number">3.</span> <span class="nav-text">Ch 3 解析库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用XPath"><span class="nav-number">3.1.</span> <span class="nav-text">使用XPath</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#XPath概览"><span class="nav-number">3.1.1.</span> <span class="nav-text">XPath概览</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Xpath常用规则"><span class="nav-number">3.1.2.</span> <span class="nav-text">Xpath常用规则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实例引入"><span class="nav-number">3.1.3.</span> <span class="nav-text">实例引入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点选择"><span class="nav-number">3.1.4.</span> <span class="nav-text">节点选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#文本获取"><span class="nav-number">3.1.5.</span> <span class="nav-text">文本获取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#属性操作"><span class="nav-number">3.1.6.</span> <span class="nav-text">属性操作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用Beautiful-Soup"><span class="nav-number">3.2.</span> <span class="nav-text">使用Beautiful Soup</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介"><span class="nav-number">3.2.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解析器"><span class="nav-number">3.2.2.</span> <span class="nav-text">解析器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基本用法-1"><span class="nav-number">3.2.3.</span> <span class="nav-text">基本用法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点选择器"><span class="nav-number">3.2.4.</span> <span class="nav-text">节点选择器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#方法选择器"><span class="nav-number">3.2.5.</span> <span class="nav-text">方法选择器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CSS选择器"><span class="nav-number">3.2.6.</span> <span class="nav-text">CSS选择器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用pyquery"><span class="nav-number">3.3.</span> <span class="nav-text">使用pyquery</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#初始化"><span class="nav-number">3.3.1.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基本CSS选择器"><span class="nav-number">3.3.2.</span> <span class="nav-text">基本CSS选择器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#查找结点"><span class="nav-number">3.3.3.</span> <span class="nav-text">查找结点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#遍历"><span class="nav-number">3.3.4.</span> <span class="nav-text">遍历</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#获取信息"><span class="nav-number">3.3.5.</span> <span class="nav-text">获取信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点操作"><span class="nav-number">3.3.6.</span> <span class="nav-text">节点操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#伪类选择器"><span class="nav-number">3.3.7.</span> <span class="nav-text">伪类选择器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-4-数据存储"><span class="nav-number">4.</span> <span class="nav-text">Ch 4 数据存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#文件存储"><span class="nav-number">4.1.</span> <span class="nav-text">文件存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TXT文本存储"><span class="nav-number">4.1.1.</span> <span class="nav-text">TXT文本存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JSON文件存储"><span class="nav-number">4.1.2.</span> <span class="nav-text">JSON文件存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CSV文件存储"><span class="nav-number">4.1.3.</span> <span class="nav-text">CSV文件存储</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关系型数据库存储"><span class="nav-number">4.2.</span> <span class="nav-text">关系型数据库存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MySQL存储"><span class="nav-number">4.2.1.</span> <span class="nav-text">MySQL存储</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非关系型数据库存储"><span class="nav-number">4.3.</span> <span class="nav-text">非关系型数据库存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MongoDB存储"><span class="nav-number">4.3.1.</span> <span class="nav-text">MongoDB存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Redis存储"><span class="nav-number">4.3.2.</span> <span class="nav-text">Redis存储</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-5-Ajax数据爬取"><span class="nav-number">5.</span> <span class="nav-text">Ch 5 Ajax数据爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是Ajax"><span class="nav-number">5.1.</span> <span class="nav-text">什么是Ajax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ajax分析方法"><span class="nav-number">5.2.</span> <span class="nav-text">Ajax分析方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ajax结果提取"><span class="nav-number">5.3.</span> <span class="nav-text">Ajax结果提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分析Ajax爬取今日头条节拍美图"><span class="nav-number">5.4.</span> <span class="nav-text">分析Ajax爬取今日头条节拍美图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-6-动态渲染页面爬取"><span class="nav-number">6.</span> <span class="nav-text">Ch 6 动态渲染页面爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Selenium的使用"><span class="nav-number">6.1.</span> <span class="nav-text">Selenium的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Splash的使用"><span class="nav-number">6.2.</span> <span class="nav-text">Splash的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Splash负载均衡配置"><span class="nav-number">6.3.</span> <span class="nav-text">Splash负载均衡配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用Selenium爬取淘宝商品"><span class="nav-number">6.4.</span> <span class="nav-text">使用Selenium爬取淘宝商品</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-7-验证码的识别"><span class="nav-number">7.</span> <span class="nav-text">Ch 7 验证码的识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#图形验证码识别"><span class="nav-number">7.1.</span> <span class="nav-text">图形验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#滑动验证码识别"><span class="nav-number">7.2.</span> <span class="nav-text">滑动验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#点触验证码识别"><span class="nav-number">7.3.</span> <span class="nav-text">点触验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#宫格验证码识别"><span class="nav-number">7.4.</span> <span class="nav-text">宫格验证码识别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-8-代理的使用"><span class="nav-number">8.</span> <span class="nav-text">Ch 8 代理的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代理的设置"><span class="nav-number">8.1.</span> <span class="nav-text">代理的设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代理池的维护"><span class="nav-number">8.2.</span> <span class="nav-text">代理池的维护</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#付费代理的使用"><span class="nav-number">8.3.</span> <span class="nav-text">付费代理的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ADSL拨号代理"><span class="nav-number">8.4.</span> <span class="nav-text">ADSL拨号代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用代理爬取微信公众号文章"><span class="nav-number">8.5.</span> <span class="nav-text">使用代理爬取微信公众号文章</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-9-模拟登录"><span class="nav-number">9.</span> <span class="nav-text">Ch 9 模拟登录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模拟登录并爬取GitHub"><span class="nav-number">9.1.</span> <span class="nav-text">模拟登录并爬取GitHub</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie池的搭建"><span class="nav-number">9.2.</span> <span class="nav-text">Cookie池的搭建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-10-APP的爬取"><span class="nav-number">10.</span> <span class="nav-text">Ch 10 APP的爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Charles的使用"><span class="nav-number">10.1.</span> <span class="nav-text">Charles的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mitmproxy的使用"><span class="nav-number">10.2.</span> <span class="nav-text">mitmproxy的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mitmdump爬取“得到”电子书信息"><span class="nav-number">10.3.</span> <span class="nav-text">mitmdump爬取“得到”电子书信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium的基本使用"><span class="nav-number">10.4.</span> <span class="nav-text">APPium的基本使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium爬取微信朋友圈"><span class="nav-number">10.5.</span> <span class="nav-text">APPium爬取微信朋友圈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium-mitmdump爬取京东商品"><span class="nav-number">10.6.</span> <span class="nav-text">APPium+mitmdump爬取京东商品</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-11-pyspider框架使用"><span class="nav-number">11.</span> <span class="nav-text">Ch 11 pyspider框架使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍"><span class="nav-number">11.1.</span> <span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本功能"><span class="nav-number">11.1.1.</span> <span class="nav-text">基本功能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#与Scrapy比较"><span class="nav-number">11.1.2.</span> <span class="nav-text">与Scrapy比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pyspider架构"><span class="nav-number">11.1.3.</span> <span class="nav-text">pyspider架构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基本使用"><span class="nav-number">11.2.</span> <span class="nav-text">基本使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#相关链接"><span class="nav-number">11.2.1.</span> <span class="nav-text">相关链接</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#安装"><span class="nav-number">11.2.2.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#源码"><span class="nav-number">11.2.3.</span> <span class="nav-text">源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#新建项目"><span class="nav-number">11.2.4.</span> <span class="nav-text">新建项目</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#爬取首页"><span class="nav-number">11.2.5.</span> <span class="nav-text">爬取首页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#爬取详情页"><span class="nav-number">11.2.6.</span> <span class="nav-text">爬取详情页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动爬虫"><span class="nav-number">11.2.7.</span> <span class="nav-text">启动爬虫</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#详解"><span class="nav-number">11.3.</span> <span class="nav-text">详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#命令行"><span class="nav-number">11.3.1.</span> <span class="nav-text">命令行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#crawl-方法"><span class="nav-number">11.3.2.</span> <span class="nav-text">crawl()方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#任务区分"><span class="nav-number">11.3.3.</span> <span class="nav-text">任务区分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#全局配置"><span class="nav-number">11.3.4.</span> <span class="nav-text">全局配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#定时爬取"><span class="nav-number">11.3.5.</span> <span class="nav-text">定时爬取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#项目状态"><span class="nav-number">11.3.6.</span> <span class="nav-text">项目状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#删除项目"><span class="nav-number">11.3.7.</span> <span class="nav-text">删除项目</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-12-Scrapy框架使用"><span class="nav-number">12.</span> <span class="nav-text">Ch 12 Scrapy框架使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-13-分布式爬虫"><span class="nav-number">13.</span> <span class="nav-text">Ch 13 分布式爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-14-分布式爬虫的部署"><span class="nav-number">14.</span> <span class="nav-text">Ch 14 分布式爬虫的部署</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="不负骤雨" src="https://i.loli.net/2019/12/15/ev4RZy7Iakn9WHl.jpg"><p class="site-author-name" itemprop="name">不负骤雨</p><div class="site-description" itemprop="description">reading coding keeping</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">文章</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zhishui1?tab=repositories" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhishui1?tab&#x3D;repositories" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/349434614" title="BiliBili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;349434614" rel="noopener" target="_blank"><i class="fa fa-fw fa-custom bili"></i></a> </span><span class="links-of-author-item"><a href="https://steamcommunity.com/id/zhishui_x" title="Steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;zhishui_x" rel="noopener" target="_blank"><i class="fa fa-fw fa-steam"></i></a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">不负骤雨</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">100k</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: true,
      notify: true,
      appId: 'hOTS2YD4rcQy3T7bS94tywjn-gzGzoHsz',
      appKey: '4fQPAOtQk7tqm6wJ8kJxODvT',
      placeholder: "Just go go",
      avatar: 'wavatar',
      meta: guest,
      pageSize: '10' || 10,
      visitor: true,
      lang: 'zh-cn' || 'zh-cn',
      path: location.pathname,
      recordIP: true,
      serverURLs: ''
    });
  }, window.Valine);
});</script></body></html>