<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://xxyr.cc").hostname,root:"/",scheme:"Gemini",version:"7.6.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"manual"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="《Python3网络爬虫开发实战》笔记"><meta property="og:type" content="article"><meta property="og:title" content="Python爬虫基础"><meta property="og:url" content="https://xxyr.cc/post/Python/python-spider/index.html"><meta property="og:site_name" content="不负骤雨"><meta property="og:description" content="《Python3网络爬虫开发实战》笔记"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skJuQI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skt3GQ.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/sktHsI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skNbp4.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skUWCD.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skaVxJ.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skavFK.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skyP9x.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skykjO.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skh0nx.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skhWjI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skhhut.png"><meta property="article:published_time" content="2020-08-31T03:43:55.000Z"><meta property="article:modified_time" content="2021-01-10T07:27:36.506Z"><meta property="article:author" content="不负骤雨"><meta property="article:tag" content="Python"><meta property="article:tag" content="Spider"><meta property="article:tag" content="爬虫"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://s3.ax1x.com/2021/01/05/skJuQI.png"><link rel="canonical" href="https://xxyr.cc/post/Python/python-spider/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>Python爬虫基础 | 不负骤雨</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" style="margin:10px"><div class="container use-motion"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">不负骤雨</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">围城</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">13</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">14</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://xxyr.cc/post/Python/python-spider/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://i.loli.net/2019/12/15/ev4RZy7Iakn9WHl.jpg"><meta itemprop="name" content="不负骤雨"><meta itemprop="description" content="reading coding keeping"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="不负骤雨"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Python爬虫基础</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-08-31 11:43:55" itemprop="dateCreated datePublished" datetime="2020-08-31T11:43:55+08:00">2020-08-31</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-01-10 15:27:36" itemprop="dateModified" datetime="2021-01-10T15:27:36+08:00">2021-01-10</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span> </a></span></span><span id="/post/Python/python-spider/" class="post-meta-item leancloud_visitors" data-flag-title="Python爬虫基础" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论次数：</span> <a title="valine" href="/post/Python/python-spider/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/post/Python/python-spider/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>《Python3网络爬虫开发实战》笔记<a id="more"></a></p><h2 id="Ch-1-爬虫基础"><a href="#Ch-1-爬虫基础" class="headerlink" title="Ch 1 爬虫基础"></a>Ch 1 爬虫基础</h2><h3 id="HTTP基本原理"><a href="#HTTP基本原理" class="headerlink" title="HTTP基本原理"></a>HTTP基本原理</h3><h4 id="URI和URL"><a href="#URI和URL" class="headerlink" title="URI和URL"></a>URI和URL</h4><p>URI的全称为 Uniform Resource Identifier，即统一资源标志符。<br>URL的全称为 Universal Resource Locator，即统一资源定位符。<br>URN的全称为 Universal Resource Name，即统一资源名称。<br>三者关系为：<br><img src="https://s3.ax1x.com/2021/01/05/skJuQI.png" alt=""></p><h4 id="超文本"><a href="#超文本" class="headerlink" title="超文本"></a>超文本</h4><p>HypeText</p><h4 id="HTTP和HTTPS"><a href="#HTTP和HTTPS" class="headerlink" title="HTTP和HTTPS"></a>HTTP和HTTPS</h4><p>HTTP的全称是Hyper Text Transfer Protocol，超文本传输协议。<br>HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的HTTP通道，简单讲是 HTTP 的安全版，即HTTP下加入 SSL层，简称为HTTPS。</p><h4 id="HTTP请求过程"><a href="#HTTP请求过程" class="headerlink" title="HTTP请求过程"></a>HTTP请求过程</h4><h4 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h4><ul><li><p>请求方法<br><img src="https://s3.ax1x.com/2021/01/05/skt3GQ.png" alt=""></p></li><li><p>请求网址<br>URL</p></li><li><p>请求头<br>用来说明服务器要使用的附加信息。<br><img src="https://s3.ax1x.com/2021/01/05/sktHsI.png" alt=""></p><p>因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。</p></li><li><p>请求体<br>一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。<br><img src="https://s3.ax1x.com/2021/01/05/skNbp4.png" alt=""></p><p>在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type， 不然可能会导致POST提交后无法正常响应。</p></li></ul><h4 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h4><ul><li><p>响应状态码<br>响应状态码表示服务器的响应状态。<br><img src="https://s3.ax1x.com/2021/01/05/skUWCD.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skaVxJ.png" alt=""></p></li><li><p>响应头<br>响应头包含了服务器对请求的应答信息。<br><img src="https://s3.ax1x.com/2021/01/05/skavFK.png" alt=""></p></li><li><p>响应体<br>响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。</p></li></ul><h3 id="网页基础"><a href="#网页基础" class="headerlink" title="网页基础"></a>网页基础</h3><h4 id="网页的组成"><a href="#网页的组成" class="headerlink" title="网页的组成"></a>网页的组成</h4><p>网页可以分为三大部分——HTML,CSS和JavaScript。如果把网页比作一个人的话， HTML相于骨架，JavaScript相当于肌肉，CSS相当于皮肤。</p><ul><li>HTML<br>HTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。</li><li>CSS<br>CSS，全称叫作Cascading Style Sheets，即层叠样式表。</li><li>JavaScript<br>JavaScript，简称JS，是一种脚本语言，实现了一种实时、动态、交互的页面功能。</li></ul><h4 id="网页的结构"><a href="#网页的结构" class="headerlink" title="网页的结构"></a>网页的结构</h4><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="节点树及节点间关系"><a href="#节点树及节点间关系" class="headerlink" title="节点树及节点间关系"></a>节点树及节点间关系</h4><p><img src="https://s3.ax1x.com/2021/01/05/skyP9x.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skykjO.png" alt=""></p><h4 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h4><p>常用三种方法：根据id（#）、根据class（.）以及标签名（h1）进行筛选。<br>嵌套选择：</p><ul><li>各选择器之间加空格代表嵌套关系，如div #container为先选择一个div节点，在选择其内部id为container的节点。</li><li>不加空格代表并列关系，如div#container为选择id为container的div节点。</li></ul><p>css选择器还有一些其他语法规则，具体如表2-4所示。<br><img src="https://s3.ax1x.com/2021/01/05/skh0nx.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skhWjI.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skhhut.png" alt=""></p><h3 id="爬虫的基本原理"><a href="#爬虫的基本原理" class="headerlink" title="爬虫的基本原理"></a>爬虫的基本原理</h3><h4 id="爬虫概述"><a href="#爬虫概述" class="headerlink" title="爬虫概述"></a>爬虫概述</h4><ul><li>获取网页</li><li>提取信息</li><li>保存数据</li><li>自动化程序</li></ul><h4 id="能抓怎样的数据"><a href="#能抓怎样的数据" class="headerlink" title="能抓怎样的数据"></a>能抓怎样的数据</h4><p>HTML代码、JSON文件、二进制数据等</p><h4 id="JavaScript渲染页面"><a href="#JavaScript渲染页面" class="headerlink" title="JavaScript渲染页面"></a>JavaScript渲染页面</h4><p>通过分析其后台Ajax接口，或使用Selenium、Splash库来模拟JavaScript渲染。</p><h3 id="会话和Cookies"><a href="#会话和Cookies" class="headerlink" title="会话和Cookies"></a>会话和Cookies</h3><h4 id="静态网页和动态网页"><a href="#静态网页和动态网页" class="headerlink" title="静态网页和动态网页"></a>静态网页和动态网页</h4><h4 id="无状态HTTP"><a href="#无状态HTTP" class="headerlink" title="无状态HTTP"></a>无状态HTTP</h4><p>HTTP连接本身是无状态的。</p><ul><li><p>会话<br>Web中，会话对象用来存储特定用户会话所需的属性及配置信息。</p></li><li><p>Cookies<br>Cookies指某些网站为了辨别用户身份、进行会话跟踪而存储在用户本地终端上的数据。</p></li><li><p>会话维持<br>当客户端第一次请求服务器时，服务器会返回一个请求头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把Cookie保 存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies放到请求头一起提交给服务器，“Cookies携带了会话ID信息，服务器检查该Cookies即可找到对应的会话是什么，然后再判断会话来以此来辨认用户状态。</p></li><li><p>属性结构</p><ul><li>Name：该Cookie的名称。一旦创建，该名称便不可更改。</li><li>Value：该Cookie的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。</li><li>Domain：可以访问该 Cookie 的域名 。 例如，如果设置为 . zhihu.com ，则所有以 zh ihu .com 结尾的域名都可以访问该 Cookie。</li><li>Max Age：该Cookie失效的时间，单位为秒，也常和Expires一起使用，通过它可以计算出其有效时间。Max Age如果为正数，则该Cookie在Max Age 秒之后失效。如果为负数，则关闭浏览器时Cookie即失效，浏览器也不会以任何形式保存该Cookie。</li><li>Path：该Cookie的使用路径。如果设置为／path/，则只有路径为／path/的页面可以访问该Cookie；如果设置为/，则本域名下的所有页面都可以访问该Cookie。</li><li>Size字段：此Cookie的大小。</li><li>HTTP字段：Cookie的httponly属性。若此属性为true，则只有在HTTP头中会带有此Cookie的信息，而不能通过document.cookie来访问此Cookie。</li><li>Secure：该Cookie是否仅被使用安全协议传输。安全协议有HTTPS和SSL等，在网络上传输数据之前先将数据加密。默认为false。</li></ul></li><li><p>会话Cookie和持久Cookie<br>表面意思为会话Cookie存在浏览器内存里，浏览器关闭则Cookie失效；持久Cookie保存在硬盘里，下次可再次使用。<br>实际为设置Cookie的Max Age或Expires字段。</p></li></ul><h4 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a>常见误区</h4><p>“浏览器关闭，会话就消失了”。是不准确的。</p><h3 id="代理的基本原理"><a href="#代理的基本原理" class="headerlink" title="代理的基本原理"></a>代理的基本原理</h3><h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h4><p>本机的网络请求通过代理服务器访问Web服务器。</p><h4 id="代理的作用"><a href="#代理的作用" class="headerlink" title="代理的作用"></a>代理的作用</h4><ul><li>突破自身IP访问限制。</li><li>访问一些单位或团体内部资源。</li><li>提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将·其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。</li><li>隐藏真实IP：免受攻击或防止IP被封锁。</li></ul><h4 id="代理分类"><a href="#代理分类" class="headerlink" title="代理分类"></a>代理分类</h4><ul><li>根据协议区分<ul><li>FTP代理服务器：主要用于访问FTP服务器，一般有上传、下载以及缓存功能，端口一般为21、2121等。</li><li>HTTP代理服务器：主要用于访问网页，一般有内容过滤和缓存功能，端口一般为80、8080、3128等。</li><li>SSl/TLS代理：主要用于访问加密网站，一般有SSL或TLS加密功能（最高支持128位加密强度），端口一般为443。</li><li>RTSP代理：主要用于访问Real流媒体服务器，一般有缓存功能，端口一般为554。</li><li>Telnet代理：主要用于telnet远程控制（黑客人侵计算机时常用于隐藏身份），端口一般为23。</li><li>POP3/SMTP代理：主要用于POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25。</li><li>SOCKS代理：只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080。SOCKS代理协议又分为SOCKS4和SOCKS5，前者只支持TCP，而后者支持TCP和UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCKS4能做到的SOCKS5都可以做到，但 SOCKS5能做到的SOCKS4不一定能做到。</li></ul></li><li>根据匿名程度区分<ul><li>高度匿名代理：会将数据包原封不动地转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的IP是代理服务器的IP。</li><li>普通匿名代理：会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实IP。代理服务器通常会加入的HTTP头有HTTP_VIA和 HTTP_X_FORWARDED_FOR。</li><li>透明代理：不但改动了数据包 还会告诉服务器客户端的真实IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网巾的硬件防火墙。</li><li>间谍代理：指组织或个人创建的用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。</li></ul></li></ul><h4 id="常见代理设置"><a href="#常见代理设置" class="headerlink" title="常见代理设置"></a>常见代理设置</h4><ul><li>网上的免费代理</li><li>付费代理服务</li><li>ADSL拨号：拨一次号换一次IP，稳定性高。</li></ul><h2 id="Ch-2-基本库的使用"><a href="#Ch-2-基本库的使用" class="headerlink" title="Ch 2 基本库的使用"></a>Ch 2 基本库的使用</h2><h3 id="使用urllib"><a href="#使用urllib" class="headerlink" title="使用urllib"></a>使用urllib</h3><p>urllib为Python内置的HTTP请求库，包含以下4个模块：</p><ul><li>request：它是最基本的HTTP请求模块，可以用来模拟发送请求。</li><li>error：异常处理模块。</li><li>parse：一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。</li><li>robot parser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，用得比较少。</li></ul><h4 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h4><ul><li>urlopen()<br>urlopen()返回一个HTTPResponse类型的对象，主要包含read()、readinto()、getheader(name)、getheaders()、fileno()等方法，以及msg、version、status、reason、debuglevel、closed等属性。<br>urlopen()函数的API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, timeout=<span class="number">1</span>, cafile=<span class="literal">None</span>, </span><br><span class="line">                            capath=<span class="literal">None</span>, cadefault=<span class="literal">False</span>, context=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">data：需要是字节流编码格式，即bytes类型，请求方法变为POST</span><br><span class="line">timeout：单位为秒</span><br><span class="line">cafile和capath：指定CA证书及其路径</span><br><span class="line">cadefault：已弃用</span><br><span class="line">context：用来指定SSL设置，必须是ssl.SSLContext类型</span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'word'</span>:<span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">response= urllib.request.urlopen(<span class="string">'http://httpbin.org/post'</span>, data=data)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure></li><li>Request类<br>API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(ur1, data=None, headers=&#123;&#125;,</span></span></span><br><span class="line"><span class="class"><span class="params">                                  origin_req_host=None, unverifiable=False, method=None)</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">headers</span>：为一个字典，用来构造请求头，也可以后面用<span class="title">add_header</span><span class="params">()</span>添加，常用来修改<span class="title">User</span>-<span class="title">Agent</span></span></span><br><span class="line"><span class="class"><span class="title">origin_req_host</span>：请求方的<span class="title">host</span>名称或<span class="title">IP</span>地址</span></span><br><span class="line"><span class="class"><span class="title">unverifiable</span>：请求权限问题</span></span><br><span class="line"><span class="class"><span class="title">method</span>：请求方法</span></span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (Compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Germey'</span></span><br><span class="line">&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict),encoding=<span class="string">'utf8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure></li><li>用Opener构建Handler<br>官方文档：<a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler</a><ul><li>验证<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">'username'</span></span><br><span class="line">password = <span class="string">'password'</span></span><br><span class="line">url = <span class="string">'http://localhost:5000/'</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>代理<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:9743'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>Cookies<ul><li>获取Cookies<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">"="</span>+item.value)</span><br></pre></td></tr></table></figure></li><li>Cookie保存至文件<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'cookies.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line"><span class="comment">#cookie = http.cookiejar.LWPCookieJar(filename)  另一种格式</span></span><br><span class="line"><span class="comment">#cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)  读取</span></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="处理异常"><a href="#处理异常" class="headerlink" title="处理异常"></a>处理异常</h4><ul><li>URLError<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>HTTPError<br>URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre></td></tr></table></figure></li></ul><h4 id="解析链接"><a href="#解析链接" class="headerlink" title="解析链接"></a>解析链接</h4><p>url.parse模块，定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。<br>支持如下协议的URL处理：file、ftp、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。<br>常用方法如下：</p><ul><li><p>urlparse()<br>实现URL的识别与分段。<br>API：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">''</span>, allow_fragments=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">scheme：url中没有协议时，作为默认的协议。</span><br><span class="line">allow_fragments：是否忽略fragment。如果设置为<span class="literal">False</span>，fragment部分就会被忽略，</span><br><span class="line">                 它会被依次解析为query、parameters或者path的一部分，而fragment部分为空。</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html#comment'</span>,</span><br><span class="line">                   scheme=<span class="string">'https'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)  <span class="comment">#返回的result为ParseResult类型，实际上是一个元组，支持result[0]和result.scheme</span></span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">ParseResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, </span><br><span class="line">            path=<span class="string">'/index.html#comment'</span>, params=<span class="string">''</span>, query=<span class="string">''</span>, fragment=<span class="string">''</span>)</span><br></pre></td></tr></table></figure></li><li><p>urlunparse()<br>实现URL的构造。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"></span><br><span class="line"><span class="comment">#data可以用其他类型，但长度必须是6</span></span><br><span class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunparse(data))</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">http://www.baidu.com/index.html;user?a=6#comment</span><br></pre></td></tr></table></figure></li><li><p>urlsplit()<br>类似urlparse()，但只返回5个结果，params合并到path里。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"></span><br><span class="line">result = urlsplit(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">SplitResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, </span><br><span class="line">            path=<span class="string">'/index.html;user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre></td></tr></table></figure></li><li><p>urlunsplit()<br>类似urlunparse()，但只传入5个参数。</p></li><li><p>urljoin()</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">urljoin(base_url, target_url)</span><br><span class="line"></span><br><span class="line">分析base_url的scheme、netloc和path三个内容并对target_url进行补充</span><br></pre></td></tr></table></figure></li><li><p>urlencode()<br>将字典序列化为GET请求的参数。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">'http://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">http://www.baidu.com?name=germey&amp;age=22</span><br></pre></td></tr></table></figure></li><li><p>parse_qs()<br>将URL反序列化为字典。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">'name=germey&amp;age=22'</span></span><br><span class="line">print(parse_qs(query))</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">&#123;<span class="string">'name'</span>: [<span class="string">'germey'</span>], <span class="string">'age'</span>: [<span class="string">'22'</span>]&#125;</span><br></pre></td></tr></table></figure></li><li><p>parse_qsl()<br>将URL转化为元组组成的列表。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[(<span class="string">'name'</span>, <span class="string">'germey'</span>), (<span class="string">'age'</span>, <span class="string">'22'</span>)]</span><br></pre></td></tr></table></figure></li><li><p>quote()<br>将内容（中文字符）转化为URL编码的格式。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">keyword = <span class="string">'壁纸'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8</span><br></pre></td></tr></table></figure></li><li><p>unquote()<br>进行URL解码</p></li></ul><h4 id="分析Robots协议"><a href="#分析Robots协议" class="headerlink" title="分析Robots协议"></a>分析Robots协议</h4><ul><li><p>Robots协议<br>Robots协议也称作爬虫协议、机器人协议，全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robot.txt的文本文件，一般放在网站的根目录下。</p></li><li><p>爬虫名称<br>常见的搜索爬虫的名称及对应的网站：<br>BaiduSpider 百度 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a><br>Googlebot 谷歌 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>360Spider 360 搜索 <a href="http://www.so.com" target="_blank" rel="noopener">www.so.com</a><br>YodaoBot 有道 <a href="http://www.youdao.com" target="_blank" rel="noopener">www.youdao.com</a><br>ia_archiver Alexa <a href="http://www.alexa.cn" target="_blank" rel="noopener">www.alexa.cn</a><br>Scooter altavista <a href="http://www.altavista.com" target="_blank" rel="noopener">www.altavista.com</a></p></li><li><p>robotparser<br>urllib.robotparser模块提供了一个RobotFileParser类，该类的一些方法如下：</p><ul><li>set_url()：用来设置robots.txt文件的链接。也可在创建RobotFileParser对象时传入链接。</li><li>read()：读取robots.txt文件并进行分析。</li><li>parse()：用来解析robots.txt文件，传人的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。</li><li>can_fetch()：该方法传人两个参数，第一个是 User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，结果为True 或False。</li><li>mtime()：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是有必要的，可能需要定期检查来抓取最新的robots.txt。</li><li>modified()：同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间。</li></ul><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line"><span class="comment">#上面两行可以用parse方法来执行读取和分析</span></span><br><span class="line"><span class="comment">#rp.parse(urlopen('http://www.jianshu.com/robots.txt').read().decode('utf-8').split('\n'))</span></span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="使用requests"><a href="#使用requests" class="headerlink" title="使用requests"></a>使用requests</h3><p>解决urllib中Cookies、登录验证、代理设置不方便的问题。<br>安装<code>pip install requests</code><br>requests的官方文档：<a href="http://docs.python-requests.org/" target="_blank" rel="noopener">http://docs.python-requests.org/</a></p><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>requests库包含get()、post()、put()、delete()、head()和options()等方法，分别对应各种方式请求网页。</p><ul><li><p>GET请求</p><ul><li>基本实例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line"><span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=data)</span><br><span class="line">print(type(r))</span><br><span class="line">print(r.status)</span><br><span class="line"><span class="comment">#网页的返回类型实际是JSON格式的str类型，调用json()可将其转化为字典</span></span><br><span class="line">print(type(r.json()))</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">requests</span>.<span class="title">models</span>.<span class="title">Response</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">dict</span>'&gt;</span></span><br></pre></td></tr></table></figure></li><li>抓取网页<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) '</span></span><br><span class="line">                    <span class="string">'AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                     <span class="string">'Chrome/52.0.2743.116 Safari/537.36'</span>)</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>, headers=headers)</span><br><span class="line">pattern = re.compile(<span class="string">'explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;'</span>, re.S)</span><br><span class="line">titles = re.findall(pattern, r.text)</span><br><span class="line">print(titles)</span><br></pre></td></tr></table></figure></li><li>抓取二进制数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">"https://github.com/favicon.ico"</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'favicon.ico'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(r.content)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>POST请求</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="string">'22'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=data)</span><br></pre></td></tr></table></figure></li><li><p>响应</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://www.xxyr.cc'</span>)</span><br><span class="line">print(type(r.status_code))</span><br><span class="line">print(type(r.headers))</span><br><span class="line">print(type(r.cookies))</span><br><span class="line">print(type(r.url))</span><br><span class="line">print(type(r.history))</span><br><span class="line">print(type(r.text))  <span class="comment">#返回内容的字符串形式</span></span><br><span class="line">print(type(r.content)) <span class="comment">#返回内容的二进制形式</span></span><br><span class="line">print(requests.codes.ok)  <span class="comment">#内置的返回码</span></span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">int</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">requests</span>.<span class="title">structures</span>.<span class="title">CaseInsensitiveDict</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">requests</span>.<span class="title">cookies</span>.<span class="title">RequestsCookieJar</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">list</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">bytes</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h4><ul><li><p>文件上传</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">files = &#123;<span class="string">'file'</span>: open(<span class="string">'favicon.ico'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>, files=files)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li><li><p>Cookies</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">print(r.cookies)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> r.cookies.items():</span><br><span class="line">    print(key + <span class="string">'='</span> + value)</span><br></pre></td></tr></table></figure><p>可将Cookie字段添加到headers里实现登录：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>,</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://www.zhihu.com'</span>, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>或将其作为cookie参数添加到get()方法里：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">cookies = <span class="string">'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'</span></span><br><span class="line">jar = requests.cookies.RequestsCookieJar()</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> cookie <span class="keyword">in</span> cookies.split(<span class="string">';'</span>):</span><br><span class="line">    key, value = cookie.split(<span class="string">'='</span>, <span class="number">1</span>)</span><br><span class="line">    jar.set(key, value)</span><br><span class="line">r = requests.get(<span class="string">'http://www.zhihu.com'</span>, cookies=jar, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li><li><p>会话维持<br>当访问登录网站后的页面，或同一站点的不同页面时，就需要进行会话维持。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>)</span><br><span class="line">r = s.get(<span class="string">'http://httpbin.org/cookies'</span>)</span><br><span class="line">print(r.text)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"cookies"</span>: &#123;</span><br><span class="line">    <span class="string">"number"</span>: <span class="string">"123456789"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>SSL证书验证</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)  <span class="comment">#默认为True，自动验证证书</span></span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure><p><code>verify=False</code>会忽略证书的验证，但会报一个警告，解决方法如下：</p><ul><li>设置忽略警告<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line"></span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li><li>捕获警告到日志<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">logging.captureWarnings(<span class="literal">True</span>)</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li><li>指定一个本地证书用作客户端证书<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>代理设置</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line"><span class="string">'http'</span>: <span class="string">'http://10.10.1.10:3128'</span>,</span><br><span class="line"><span class="string">'https'</span>: <span class="string">'http://10.10.1.10:1080'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><p>若要使用HTTP Basic Auth，可以使用如下代理形式：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'http://user:password@10.10.1.10:3128/'</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><p>requests还支持SOCKS代理：<br>安装：<code>pip3 install &#39;requests[socks]&#39;</code></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://user:password@host:port'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://user:password@host:port'</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure></li><li><p>超时设置<br>为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应就报错。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>, timeout=<span class="number">1</span>)</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure><p>timeout=1表示超时时间为1秒，默认为None，即永远等待。<br>实际上，请求分为两个阶段，即连接（connect）和读取（read）。上面设置的timeout将用作连接和读取这二者的timeout总和。<br>如果要分别指定，就可以传入一个元组：<br><code>r = requests.get(&#39;https://www.taobao.com&#39;, timeout=(5,11, 30))</code></p></li><li><p>身份认证</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment">#auth=('username', 'password')即auth=HTTPBasicAuth('username', 'password')</span></span><br><span class="line">r = requests.get(<span class="string">'http://localhost:5000'</span>, auth=(<span class="string">'username'</span>, <span class="string">'password'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure><p>此外requests还提供了其他认证方式，如OAuth认证等。</p></li><li><p>Prepared Request<br>用于将请求表示为数据结构，其中各个参数通过一个Request对象来表示。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 '</span>)</span><br><span class="line">                    (<span class="string">'(KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>)</span><br><span class="line">&#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(<span class="string">'POST'</span>, url, data=data, headers=headers)</span><br><span class="line"><span class="comment">#Session的prepare_request()方法将其转化为一个Prepared Request对象</span></span><br><span class="line">prepped = s.prepare_request(req)  </span><br><span class="line">r = s.send(prepped)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li></ul><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>详见： <a href="https://xxyr.cc/post/tech/regex-review/">正则表达式复习</a></p><h3 id="抓取猫眼电影排行"><a href="#抓取猫眼电影排行" class="headerlink" title="抓取猫眼电影排行"></a>抓取猫眼电影排行</h3><p>详见<a href="https://github.com/Python3WebSpider/MaoYan" target="_blank" rel="noopener">https://github.com/Python3WebSpider/MaoYan</a></p><h2 id="Ch-3-解析库的使用"><a href="#Ch-3-解析库的使用" class="headerlink" title="Ch 3 解析库的使用"></a>Ch 3 解析库的使用</h2><h3 id="使用XPath"><a href="#使用XPath" class="headerlink" title="使用XPath"></a>使用XPath</h3><h3 id="使用Beautiful-Soup"><a href="#使用Beautiful-Soup" class="headerlink" title="使用Beautiful Soup"></a>使用Beautiful Soup</h3><h3 id="使用pyquery"><a href="#使用pyquery" class="headerlink" title="使用pyquery"></a>使用pyquery</h3><h2 id="Ch-4-数据存储"><a href="#Ch-4-数据存储" class="headerlink" title="Ch 4 数据存储"></a>Ch 4 数据存储</h2><h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><h4 id="TXT文本存储"><a href="#TXT文本存储" class="headerlink" title="TXT文本存储"></a>TXT文本存储</h4><h4 id="JSON文件存储"><a href="#JSON文件存储" class="headerlink" title="JSON文件存储"></a>JSON文件存储</h4><h4 id="CSV文件存储"><a href="#CSV文件存储" class="headerlink" title="CSV文件存储"></a>CSV文件存储</h4><h3 id="关系型数据库存储"><a href="#关系型数据库存储" class="headerlink" title="关系型数据库存储"></a>关系型数据库存储</h3><h4 id="MySQL存储"><a href="#MySQL存储" class="headerlink" title="MySQL存储"></a>MySQL存储</h4><h3 id="非关系型数据库存储"><a href="#非关系型数据库存储" class="headerlink" title="非关系型数据库存储"></a>非关系型数据库存储</h3><h4 id="MongoDB存储"><a href="#MongoDB存储" class="headerlink" title="MongoDB存储"></a>MongoDB存储</h4><h4 id="Redis存储"><a href="#Redis存储" class="headerlink" title="Redis存储"></a>Redis存储</h4><h2 id="Ch-5-Ajax数据爬取"><a href="#Ch-5-Ajax数据爬取" class="headerlink" title="Ch 5 Ajax数据爬取"></a>Ch 5 Ajax数据爬取</h2><h3 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h3><h3 id="Ajax分析方法"><a href="#Ajax分析方法" class="headerlink" title="Ajax分析方法"></a>Ajax分析方法</h3><h3 id="Ajax结果提取"><a href="#Ajax结果提取" class="headerlink" title="Ajax结果提取"></a>Ajax结果提取</h3><h3 id="分析Ajax爬取今日头条节拍美图"><a href="#分析Ajax爬取今日头条节拍美图" class="headerlink" title="分析Ajax爬取今日头条节拍美图"></a>分析Ajax爬取今日头条节拍美图</h3><h2 id="Ch-6-动态渲染页面爬取"><a href="#Ch-6-动态渲染页面爬取" class="headerlink" title="Ch 6 动态渲染页面爬取"></a>Ch 6 动态渲染页面爬取</h2><h3 id="Selenium的使用"><a href="#Selenium的使用" class="headerlink" title="Selenium的使用"></a>Selenium的使用</h3><h3 id="Splash的使用"><a href="#Splash的使用" class="headerlink" title="Splash的使用"></a>Splash的使用</h3><h3 id="Splash负载均衡配置"><a href="#Splash负载均衡配置" class="headerlink" title="Splash负载均衡配置"></a>Splash负载均衡配置</h3><h3 id="使用Selenium爬取淘宝商品"><a href="#使用Selenium爬取淘宝商品" class="headerlink" title="使用Selenium爬取淘宝商品"></a>使用Selenium爬取淘宝商品</h3><h2 id="Ch-7-验证码的识别"><a href="#Ch-7-验证码的识别" class="headerlink" title="Ch 7 验证码的识别"></a>Ch 7 验证码的识别</h2><h3 id="图形验证码识别"><a href="#图形验证码识别" class="headerlink" title="图形验证码识别"></a>图形验证码识别</h3><h3 id="滑动验证码识别"><a href="#滑动验证码识别" class="headerlink" title="滑动验证码识别"></a>滑动验证码识别</h3><h3 id="点触验证码识别"><a href="#点触验证码识别" class="headerlink" title="点触验证码识别"></a>点触验证码识别</h3><h3 id="宫格验证码识别"><a href="#宫格验证码识别" class="headerlink" title="宫格验证码识别"></a>宫格验证码识别</h3><h2 id="Ch-8-代理的使用"><a href="#Ch-8-代理的使用" class="headerlink" title="Ch 8 代理的使用"></a>Ch 8 代理的使用</h2><h3 id="代理的设置"><a href="#代理的设置" class="headerlink" title="代理的设置"></a>代理的设置</h3><h3 id="代理池的维护"><a href="#代理池的维护" class="headerlink" title="代理池的维护"></a>代理池的维护</h3><h3 id="付费代理的使用"><a href="#付费代理的使用" class="headerlink" title="付费代理的使用"></a>付费代理的使用</h3><h3 id="ADSL拨号代理"><a href="#ADSL拨号代理" class="headerlink" title="ADSL拨号代理"></a>ADSL拨号代理</h3><h3 id="使用代理爬取微信公众号文章"><a href="#使用代理爬取微信公众号文章" class="headerlink" title="使用代理爬取微信公众号文章"></a>使用代理爬取微信公众号文章</h3><h2 id="Ch-9-模拟登录"><a href="#Ch-9-模拟登录" class="headerlink" title="Ch 9 模拟登录"></a>Ch 9 模拟登录</h2><h3 id="模拟登录并爬取GitHub"><a href="#模拟登录并爬取GitHub" class="headerlink" title="模拟登录并爬取GitHub"></a>模拟登录并爬取GitHub</h3><h3 id="Cookie池的搭建"><a href="#Cookie池的搭建" class="headerlink" title="Cookie池的搭建"></a>Cookie池的搭建</h3><h2 id="Ch-10-APP的爬取"><a href="#Ch-10-APP的爬取" class="headerlink" title="Ch 10 APP的爬取"></a>Ch 10 APP的爬取</h2><h3 id="Charles的使用"><a href="#Charles的使用" class="headerlink" title="Charles的使用"></a>Charles的使用</h3><h3 id="mitmproxy的使用"><a href="#mitmproxy的使用" class="headerlink" title="mitmproxy的使用"></a>mitmproxy的使用</h3><h3 id="mitmdump爬取“得到”电子书信息"><a href="#mitmdump爬取“得到”电子书信息" class="headerlink" title="mitmdump爬取“得到”电子书信息"></a>mitmdump爬取“得到”电子书信息</h3><h3 id="APPium的基本使用"><a href="#APPium的基本使用" class="headerlink" title="APPium的基本使用"></a>APPium的基本使用</h3><h3 id="APPium爬取微信朋友圈"><a href="#APPium爬取微信朋友圈" class="headerlink" title="APPium爬取微信朋友圈"></a>APPium爬取微信朋友圈</h3><h3 id="APPium-mitmdump爬取京东商品"><a href="#APPium-mitmdump爬取京东商品" class="headerlink" title="APPium+mitmdump爬取京东商品"></a>APPium+mitmdump爬取京东商品</h3><h2 id="Ch-11-pyspider框架使用"><a href="#Ch-11-pyspider框架使用" class="headerlink" title="Ch 11 pyspider框架使用"></a>Ch 11 pyspider框架使用</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h3 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h3><h2 id="Ch-12-Scrapy框架使用"><a href="#Ch-12-Scrapy框架使用" class="headerlink" title="Ch 12 Scrapy框架使用"></a>Ch 12 Scrapy框架使用</h2><h2 id="Ch-13-分布式爬虫"><a href="#Ch-13-分布式爬虫" class="headerlink" title="Ch 13 分布式爬虫"></a>Ch 13 分布式爬虫</h2><h2 id="Ch-14-分布式爬虫的部署"><a href="#Ch-14-分布式爬虫的部署" class="headerlink" title="Ch 14 分布式爬虫的部署"></a>Ch 14 分布式爬虫的部署</h2></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>不负骤雨</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://xxyr.cc/post/Python/python-spider/" title="Python爬虫基础">https://xxyr.cc/post/Python/python-spider/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python/" rel="tag"># Python</a> <a href="/tags/Spider/" rel="tag"># Spider</a> <a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"># 爬虫</a></div><div class="post-nav"><div class="post-nav-item"><a href="/post/Networks/wireshark/" rel="prev" title="WireShark网络分析"><i class="fa fa-chevron-left"></i> WireShark网络分析</a></div><div class="post-nav-item"><a href="/post/Tips/common-commands/" rel="next" title="常用命令">常用命令 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-1-爬虫基础"><span class="nav-number">1.</span> <span class="nav-text">Ch 1 爬虫基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HTTP基本原理"><span class="nav-number">1.1.</span> <span class="nav-text">HTTP基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#URI和URL"><span class="nav-number">1.1.1.</span> <span class="nav-text">URI和URL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#超文本"><span class="nav-number">1.1.2.</span> <span class="nav-text">超文本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTP和HTTPS"><span class="nav-number">1.1.3.</span> <span class="nav-text">HTTP和HTTPS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTP请求过程"><span class="nav-number">1.1.4.</span> <span class="nav-text">HTTP请求过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#请求"><span class="nav-number">1.1.5.</span> <span class="nav-text">请求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#响应"><span class="nav-number">1.1.6.</span> <span class="nav-text">响应</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网页基础"><span class="nav-number">1.2.</span> <span class="nav-text">网页基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#网页的组成"><span class="nav-number">1.2.1.</span> <span class="nav-text">网页的组成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网页的结构"><span class="nav-number">1.2.2.</span> <span class="nav-text">网页的结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点树及节点间关系"><span class="nav-number">1.2.3.</span> <span class="nav-text">节点树及节点间关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#选择器"><span class="nav-number">1.2.4.</span> <span class="nav-text">选择器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫的基本原理"><span class="nav-number">1.3.</span> <span class="nav-text">爬虫的基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#爬虫概述"><span class="nav-number">1.3.1.</span> <span class="nav-text">爬虫概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#能抓怎样的数据"><span class="nav-number">1.3.2.</span> <span class="nav-text">能抓怎样的数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JavaScript渲染页面"><span class="nav-number">1.3.3.</span> <span class="nav-text">JavaScript渲染页面</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#会话和Cookies"><span class="nav-number">1.4.</span> <span class="nav-text">会话和Cookies</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#静态网页和动态网页"><span class="nav-number">1.4.1.</span> <span class="nav-text">静态网页和动态网页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#无状态HTTP"><span class="nav-number">1.4.2.</span> <span class="nav-text">无状态HTTP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见误区"><span class="nav-number">1.4.3.</span> <span class="nav-text">常见误区</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代理的基本原理"><span class="nav-number">1.5.</span> <span class="nav-text">代理的基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本原理"><span class="nav-number">1.5.1.</span> <span class="nav-text">基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代理的作用"><span class="nav-number">1.5.2.</span> <span class="nav-text">代理的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代理分类"><span class="nav-number">1.5.3.</span> <span class="nav-text">代理分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见代理设置"><span class="nav-number">1.5.4.</span> <span class="nav-text">常见代理设置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-2-基本库的使用"><span class="nav-number">2.</span> <span class="nav-text">Ch 2 基本库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用urllib"><span class="nav-number">2.1.</span> <span class="nav-text">使用urllib</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#发送请求"><span class="nav-number">2.1.1.</span> <span class="nav-text">发送请求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#处理异常"><span class="nav-number">2.1.2.</span> <span class="nav-text">处理异常</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解析链接"><span class="nav-number">2.1.3.</span> <span class="nav-text">解析链接</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分析Robots协议"><span class="nav-number">2.1.4.</span> <span class="nav-text">分析Robots协议</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用requests"><span class="nav-number">2.2.</span> <span class="nav-text">使用requests</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本用法"><span class="nav-number">2.2.1.</span> <span class="nav-text">基本用法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#高级用法"><span class="nav-number">2.2.2.</span> <span class="nav-text">高级用法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则表达式"><span class="nav-number">2.3.</span> <span class="nav-text">正则表达式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#抓取猫眼电影排行"><span class="nav-number">2.4.</span> <span class="nav-text">抓取猫眼电影排行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-3-解析库的使用"><span class="nav-number">3.</span> <span class="nav-text">Ch 3 解析库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用XPath"><span class="nav-number">3.1.</span> <span class="nav-text">使用XPath</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用Beautiful-Soup"><span class="nav-number">3.2.</span> <span class="nav-text">使用Beautiful Soup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用pyquery"><span class="nav-number">3.3.</span> <span class="nav-text">使用pyquery</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-4-数据存储"><span class="nav-number">4.</span> <span class="nav-text">Ch 4 数据存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#文件存储"><span class="nav-number">4.1.</span> <span class="nav-text">文件存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TXT文本存储"><span class="nav-number">4.1.1.</span> <span class="nav-text">TXT文本存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JSON文件存储"><span class="nav-number">4.1.2.</span> <span class="nav-text">JSON文件存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CSV文件存储"><span class="nav-number">4.1.3.</span> <span class="nav-text">CSV文件存储</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关系型数据库存储"><span class="nav-number">4.2.</span> <span class="nav-text">关系型数据库存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MySQL存储"><span class="nav-number">4.2.1.</span> <span class="nav-text">MySQL存储</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非关系型数据库存储"><span class="nav-number">4.3.</span> <span class="nav-text">非关系型数据库存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MongoDB存储"><span class="nav-number">4.3.1.</span> <span class="nav-text">MongoDB存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Redis存储"><span class="nav-number">4.3.2.</span> <span class="nav-text">Redis存储</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-5-Ajax数据爬取"><span class="nav-number">5.</span> <span class="nav-text">Ch 5 Ajax数据爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是Ajax"><span class="nav-number">5.1.</span> <span class="nav-text">什么是Ajax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ajax分析方法"><span class="nav-number">5.2.</span> <span class="nav-text">Ajax分析方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ajax结果提取"><span class="nav-number">5.3.</span> <span class="nav-text">Ajax结果提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分析Ajax爬取今日头条节拍美图"><span class="nav-number">5.4.</span> <span class="nav-text">分析Ajax爬取今日头条节拍美图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-6-动态渲染页面爬取"><span class="nav-number">6.</span> <span class="nav-text">Ch 6 动态渲染页面爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Selenium的使用"><span class="nav-number">6.1.</span> <span class="nav-text">Selenium的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Splash的使用"><span class="nav-number">6.2.</span> <span class="nav-text">Splash的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Splash负载均衡配置"><span class="nav-number">6.3.</span> <span class="nav-text">Splash负载均衡配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用Selenium爬取淘宝商品"><span class="nav-number">6.4.</span> <span class="nav-text">使用Selenium爬取淘宝商品</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-7-验证码的识别"><span class="nav-number">7.</span> <span class="nav-text">Ch 7 验证码的识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#图形验证码识别"><span class="nav-number">7.1.</span> <span class="nav-text">图形验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#滑动验证码识别"><span class="nav-number">7.2.</span> <span class="nav-text">滑动验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#点触验证码识别"><span class="nav-number">7.3.</span> <span class="nav-text">点触验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#宫格验证码识别"><span class="nav-number">7.4.</span> <span class="nav-text">宫格验证码识别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-8-代理的使用"><span class="nav-number">8.</span> <span class="nav-text">Ch 8 代理的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代理的设置"><span class="nav-number">8.1.</span> <span class="nav-text">代理的设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代理池的维护"><span class="nav-number">8.2.</span> <span class="nav-text">代理池的维护</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#付费代理的使用"><span class="nav-number">8.3.</span> <span class="nav-text">付费代理的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ADSL拨号代理"><span class="nav-number">8.4.</span> <span class="nav-text">ADSL拨号代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用代理爬取微信公众号文章"><span class="nav-number">8.5.</span> <span class="nav-text">使用代理爬取微信公众号文章</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-9-模拟登录"><span class="nav-number">9.</span> <span class="nav-text">Ch 9 模拟登录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模拟登录并爬取GitHub"><span class="nav-number">9.1.</span> <span class="nav-text">模拟登录并爬取GitHub</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie池的搭建"><span class="nav-number">9.2.</span> <span class="nav-text">Cookie池的搭建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-10-APP的爬取"><span class="nav-number">10.</span> <span class="nav-text">Ch 10 APP的爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Charles的使用"><span class="nav-number">10.1.</span> <span class="nav-text">Charles的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mitmproxy的使用"><span class="nav-number">10.2.</span> <span class="nav-text">mitmproxy的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mitmdump爬取“得到”电子书信息"><span class="nav-number">10.3.</span> <span class="nav-text">mitmdump爬取“得到”电子书信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium的基本使用"><span class="nav-number">10.4.</span> <span class="nav-text">APPium的基本使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium爬取微信朋友圈"><span class="nav-number">10.5.</span> <span class="nav-text">APPium爬取微信朋友圈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium-mitmdump爬取京东商品"><span class="nav-number">10.6.</span> <span class="nav-text">APPium+mitmdump爬取京东商品</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-11-pyspider框架使用"><span class="nav-number">11.</span> <span class="nav-text">Ch 11 pyspider框架使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍"><span class="nav-number">11.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基本使用"><span class="nav-number">11.2.</span> <span class="nav-text">基本使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#详解"><span class="nav-number">11.3.</span> <span class="nav-text">详解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-12-Scrapy框架使用"><span class="nav-number">12.</span> <span class="nav-text">Ch 12 Scrapy框架使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-13-分布式爬虫"><span class="nav-number">13.</span> <span class="nav-text">Ch 13 分布式爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-14-分布式爬虫的部署"><span class="nav-number">14.</span> <span class="nav-text">Ch 14 分布式爬虫的部署</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="不负骤雨" src="https://i.loli.net/2019/12/15/ev4RZy7Iakn9WHl.jpg"><p class="site-author-name" itemprop="name">不负骤雨</p><div class="site-description" itemprop="description">reading coding keeping</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">文章</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zhishui1?tab=repositories" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhishui1?tab&#x3D;repositories" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/349434614" title="BiliBili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;349434614" rel="noopener" target="_blank"><i class="fa fa-fw fa-custom bili"></i></a> </span><span class="links-of-author-item"><a href="https://steamcommunity.com/id/zhishui_x" title="Steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;zhishui_x" rel="noopener" target="_blank"><i class="fa fa-fw fa-steam"></i></a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">不负骤雨</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">76k</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: true,
      notify: true,
      appId: 'hOTS2YD4rcQy3T7bS94tywjn-gzGzoHsz',
      appKey: '4fQPAOtQk7tqm6wJ8kJxODvT',
      placeholder: "Just go go",
      avatar: 'wavatar',
      meta: guest,
      pageSize: '10' || 10,
      visitor: true,
      lang: 'zh-cn' || 'zh-cn',
      path: location.pathname,
      recordIP: true,
      serverURLs: ''
    });
  }, window.Valine);
});</script></body></html>