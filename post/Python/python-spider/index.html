<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://xxyr.cc").hostname,root:"/",scheme:"Gemini",version:"7.6.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"manual"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="《Python3网络爬虫开发实战》笔记"><meta property="og:type" content="article"><meta property="og:title" content="Python爬虫基础"><meta property="og:url" content="https://xxyr.cc/post/Python/python-spider/index.html"><meta property="og:site_name" content="不负骤雨"><meta property="og:description" content="《Python3网络爬虫开发实战》笔记"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skJuQI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skt3GQ.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/sktHsI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skNbp4.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skUWCD.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skaVxJ.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skavFK.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skyP9x.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skykjO.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skh0nx.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skhWjI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/05/skhhut.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/11/s89S0K.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/12/sJ0Uaj.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/12/sJ26Lq.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/22/soZbse.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/22/soZXdA.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/23/s7gFxA.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/17/sse47d.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/18/sy04PI.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/18/sywvjO.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/18/sysBwD.png"><meta property="og:image" content="https://s3.ax1x.com/2021/01/18/s61te0.png"><meta property="article:published_time" content="2020-08-31T03:43:55.000Z"><meta property="article:modified_time" content="2021-01-28T02:06:48.037Z"><meta property="article:author" content="不负骤雨"><meta property="article:tag" content="Python"><meta property="article:tag" content="Spider"><meta property="article:tag" content="爬虫"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://s3.ax1x.com/2021/01/05/skJuQI.png"><link rel="canonical" href="https://xxyr.cc/post/Python/python-spider/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>Python爬虫基础 | 不负骤雨</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" style="margin:10px"><div class="container use-motion"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">不负骤雨</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">围城</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">13</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">14</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://xxyr.cc/post/Python/python-spider/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://i.loli.net/2019/12/15/ev4RZy7Iakn9WHl.jpg"><meta itemprop="name" content="不负骤雨"><meta itemprop="description" content="reading coding keeping"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="不负骤雨"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Python爬虫基础</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-08-31 11:43:55" itemprop="dateCreated datePublished" datetime="2020-08-31T11:43:55+08:00">2020-08-31</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-01-28 10:06:48" itemprop="dateModified" datetime="2021-01-28T10:06:48+08:00">2021-01-28</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span> </a></span></span><span id="/post/Python/python-spider/" class="post-meta-item leancloud_visitors" data-flag-title="Python爬虫基础" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">评论次数：</span> <a title="valine" href="/post/Python/python-spider/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/post/Python/python-spider/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>《Python3网络爬虫开发实战》笔记<a id="more"></a></p><h2 id="Ch-1-爬虫基础"><a href="#Ch-1-爬虫基础" class="headerlink" title="Ch 1 爬虫基础"></a>Ch 1 爬虫基础</h2><h3 id="HTTP基本原理"><a href="#HTTP基本原理" class="headerlink" title="HTTP基本原理"></a>HTTP基本原理</h3><h4 id="URI和URL"><a href="#URI和URL" class="headerlink" title="URI和URL"></a>URI和URL</h4><p>URI的全称为 Uniform Resource Identifier，即统一资源标志符。<br>URL的全称为 Universal Resource Locator，即统一资源定位符。<br>URN的全称为 Universal Resource Name，即统一资源名称。<br>三者关系为：<br><img src="https://s3.ax1x.com/2021/01/05/skJuQI.png" alt=""></p><h4 id="超文本"><a href="#超文本" class="headerlink" title="超文本"></a>超文本</h4><p>HypeText</p><h4 id="HTTP和HTTPS"><a href="#HTTP和HTTPS" class="headerlink" title="HTTP和HTTPS"></a>HTTP和HTTPS</h4><p>HTTP的全称是Hyper Text Transfer Protocol，超文本传输协议。<br>HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的HTTP通道，简单讲是 HTTP 的安全版，即HTTP下加入 SSL层，简称为HTTPS。</p><h4 id="HTTP请求过程"><a href="#HTTP请求过程" class="headerlink" title="HTTP请求过程"></a>HTTP请求过程</h4><h4 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h4><ul><li><p>请求方法<br><img src="https://s3.ax1x.com/2021/01/05/skt3GQ.png" alt=""></p></li><li><p>请求网址<br>URL</p></li><li><p>请求头<br>用来说明服务器要使用的附加信息。<br><img src="https://s3.ax1x.com/2021/01/05/sktHsI.png" alt=""></p><p>因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。</p></li><li><p>请求体<br>一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。<br><img src="https://s3.ax1x.com/2021/01/05/skNbp4.png" alt=""></p><p>在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type， 不然可能会导致POST提交后无法正常响应。</p></li></ul><h4 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h4><ul><li><p>响应状态码<br>响应状态码表示服务器的响应状态。<br><img src="https://s3.ax1x.com/2021/01/05/skUWCD.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skaVxJ.png" alt=""></p></li><li><p>响应头<br>响应头包含了服务器对请求的应答信息。<br><img src="https://s3.ax1x.com/2021/01/05/skavFK.png" alt=""></p></li><li><p>响应体<br>响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。</p></li></ul><h3 id="网页基础"><a href="#网页基础" class="headerlink" title="网页基础"></a>网页基础</h3><h4 id="网页的组成"><a href="#网页的组成" class="headerlink" title="网页的组成"></a>网页的组成</h4><p>网页可以分为三大部分——HTML,CSS和JavaScript。如果把网页比作一个人的话， HTML相于骨架，JavaScript相当于肌肉，CSS相当于皮肤。</p><ul><li>HTML<br>HTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。</li><li>CSS<br>CSS，全称叫作Cascading Style Sheets，即层叠样式表。</li><li>JavaScript<br>JavaScript，简称JS，是一种脚本语言，实现了一种实时、动态、交互的页面功能。</li></ul><h4 id="网页的结构"><a href="#网页的结构" class="headerlink" title="网页的结构"></a>网页的结构</h4><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="节点树及节点间关系"><a href="#节点树及节点间关系" class="headerlink" title="节点树及节点间关系"></a>节点树及节点间关系</h4><p><img src="https://s3.ax1x.com/2021/01/05/skyP9x.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skykjO.png" alt=""></p><h4 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h4><p>常用三种方法：根据id（#）、根据class（.）以及标签名（h1）进行筛选。<br>嵌套选择：</p><ul><li>各选择器之间加空格代表嵌套关系，如div #container为先选择一个div节点，在选择其内部id为container的节点。</li><li>不加空格代表并列关系，如div#container为选择id为container的div节点。</li></ul><p>css选择器还有一些其他语法规则，具体如表2-4所示。<br><img src="https://s3.ax1x.com/2021/01/05/skh0nx.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skhWjI.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skhhut.png" alt=""></p><h3 id="爬虫的基本原理"><a href="#爬虫的基本原理" class="headerlink" title="爬虫的基本原理"></a>爬虫的基本原理</h3><h4 id="爬虫概述"><a href="#爬虫概述" class="headerlink" title="爬虫概述"></a>爬虫概述</h4><ul><li>获取网页</li><li>提取信息</li><li>保存数据</li><li>自动化程序</li></ul><h4 id="能抓怎样的数据"><a href="#能抓怎样的数据" class="headerlink" title="能抓怎样的数据"></a>能抓怎样的数据</h4><p>HTML代码、JSON文件、二进制数据等</p><h4 id="JavaScript渲染页面"><a href="#JavaScript渲染页面" class="headerlink" title="JavaScript渲染页面"></a>JavaScript渲染页面</h4><p>通过分析其后台Ajax接口，或使用Selenium、Splash库来模拟JavaScript渲染。</p><h3 id="会话和Cookies"><a href="#会话和Cookies" class="headerlink" title="会话和Cookies"></a>会话和Cookies</h3><h4 id="静态网页和动态网页"><a href="#静态网页和动态网页" class="headerlink" title="静态网页和动态网页"></a>静态网页和动态网页</h4><h4 id="无状态HTTP"><a href="#无状态HTTP" class="headerlink" title="无状态HTTP"></a>无状态HTTP</h4><p>HTTP连接本身是无状态的。</p><ul><li><p>会话<br>Web中，会话对象用来存储特定用户会话所需的属性及配置信息。</p></li><li><p>Cookies<br>Cookies指某些网站为了辨别用户身份、进行会话跟踪而存储在用户本地终端上的数据。</p></li><li><p>会话维持<br>当客户端第一次请求服务器时，服务器会返回一个请求头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把Cookie保 存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies放到请求头一起提交给服务器，“Cookies携带了会话ID信息，服务器检查该Cookies即可找到对应的会话是什么，然后再判断会话来以此来辨认用户状态。</p></li><li><p>属性结构</p><ul><li>Name：该Cookie的名称。一旦创建，该名称便不可更改。</li><li>Value：该Cookie的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。</li><li>Domain：可以访问该 Cookie 的域名 。 例如，如果设置为 . zhihu.com ，则所有以 zh ihu .com 结尾的域名都可以访问该 Cookie。</li><li>Max Age：该Cookie失效的时间，单位为秒，也常和Expires一起使用，通过它可以计算出其有效时间。Max Age如果为正数，则该Cookie在Max Age 秒之后失效。如果为负数，则关闭浏览器时Cookie即失效，浏览器也不会以任何形式保存该Cookie。</li><li>Path：该Cookie的使用路径。如果设置为／path/，则只有路径为／path/的页面可以访问该Cookie；如果设置为/，则本域名下的所有页面都可以访问该Cookie。</li><li>Size字段：此Cookie的大小。</li><li>HTTP字段：Cookie的httponly属性。若此属性为true，则只有在HTTP头中会带有此Cookie的信息，而不能通过document.cookie来访问此Cookie。</li><li>Secure：该Cookie是否仅被使用安全协议传输。安全协议有HTTPS和SSL等，在网络上传输数据之前先将数据加密。默认为false。</li></ul></li><li><p>会话Cookie和持久Cookie<br>表面意思为会话Cookie存在浏览器内存里，浏览器关闭则Cookie失效；持久Cookie保存在硬盘里，下次可再次使用。<br>实际为设置Cookie的Max Age或Expires字段。</p></li></ul><h4 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a>常见误区</h4><p>“浏览器关闭，会话就消失了”。是不准确的。</p><h3 id="代理的基本原理"><a href="#代理的基本原理" class="headerlink" title="代理的基本原理"></a>代理的基本原理</h3><h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h4><p>本机的网络请求通过代理服务器访问Web服务器。</p><h4 id="代理的作用"><a href="#代理的作用" class="headerlink" title="代理的作用"></a>代理的作用</h4><ul><li>突破自身IP访问限制。</li><li>访问一些单位或团体内部资源。</li><li>提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将·其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。</li><li>隐藏真实IP：免受攻击或防止IP被封锁。</li></ul><h4 id="代理分类"><a href="#代理分类" class="headerlink" title="代理分类"></a>代理分类</h4><ul><li>根据协议区分<ul><li>FTP代理服务器：主要用于访问FTP服务器，一般有上传、下载以及缓存功能，端口一般为21、2121等。</li><li>HTTP代理服务器：主要用于访问网页，一般有内容过滤和缓存功能，端口一般为80、8080、3128等。</li><li>SSl/TLS代理：主要用于访问加密网站，一般有SSL或TLS加密功能（最高支持128位加密强度），端口一般为443。</li><li>RTSP代理：主要用于访问Real流媒体服务器，一般有缓存功能，端口一般为554。</li><li>Telnet代理：主要用于telnet远程控制（黑客人侵计算机时常用于隐藏身份），端口一般为23。</li><li>POP3/SMTP代理：主要用于POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25。</li><li>SOCKS代理：只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080。SOCKS代理协议又分为SOCKS4和SOCKS5，前者只支持TCP，而后者支持TCP和UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCKS4能做到的SOCKS5都可以做到，但 SOCKS5能做到的SOCKS4不一定能做到。</li></ul></li><li>根据匿名程度区分<ul><li>高度匿名代理：会将数据包原封不动地转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的IP是代理服务器的IP。</li><li>普通匿名代理：会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实IP。代理服务器通常会加入的HTTP头有HTTP_VIA和 HTTP_X_FORWARDED_FOR。</li><li>透明代理：不但改动了数据包 还会告诉服务器客户端的真实IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网巾的硬件防火墙。</li><li>间谍代理：指组织或个人创建的用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。</li></ul></li></ul><h4 id="常见代理设置"><a href="#常见代理设置" class="headerlink" title="常见代理设置"></a>常见代理设置</h4><ul><li>网上的免费代理</li><li>付费代理服务</li><li>ADSL拨号：拨一次号换一次IP，稳定性高。</li></ul><h2 id="Ch-2-基本库的使用"><a href="#Ch-2-基本库的使用" class="headerlink" title="Ch 2 基本库的使用"></a>Ch 2 基本库的使用</h2><h3 id="使用urllib"><a href="#使用urllib" class="headerlink" title="使用urllib"></a>使用urllib</h3><p>urllib为Python内置的HTTP请求库，包含以下4个模块：</p><ul><li>request：它是最基本的HTTP请求模块，可以用来模拟发送请求。</li><li>error：异常处理模块。</li><li>parse：一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。</li><li>robot parser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，用得比较少。</li></ul><h4 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h4><ul><li>urlopen()<br>urlopen()返回一个HTTPResponse类型的对象，主要包含read()、readinto()、getheader(name)、getheaders()、fileno()等方法，以及msg、version、status、reason、debuglevel、closed等属性。<br>urlopen()函数的API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, timeout=<span class="number">1</span>, cafile=<span class="literal">None</span>, </span><br><span class="line">                            capath=<span class="literal">None</span>, cadefault=<span class="literal">False</span>, context=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">data：需要是字节流编码格式，即bytes类型，请求方法变为POST</span><br><span class="line">timeout：单位为秒</span><br><span class="line">cafile和capath：指定CA证书及其路径</span><br><span class="line">cadefault：已弃用</span><br><span class="line">context：用来指定SSL设置，必须是ssl.SSLContext类型</span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'word'</span>:<span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">response= urllib.request.urlopen(<span class="string">'http://httpbin.org/post'</span>, data=data)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure></li><li>Request类<br>API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(ur1, data=None, headers=&#123;&#125;,</span></span></span><br><span class="line"><span class="class"><span class="params">                                  origin_req_host=None, unverifiable=False, method=None)</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">headers</span>：为一个字典，用来构造请求头，也可以后面用<span class="title">add_header</span><span class="params">()</span>添加，常用来修改<span class="title">User</span>-<span class="title">Agent</span></span></span><br><span class="line"><span class="class"><span class="title">origin_req_host</span>：请求方的<span class="title">host</span>名称或<span class="title">IP</span>地址</span></span><br><span class="line"><span class="class"><span class="title">unverifiable</span>：请求权限问题</span></span><br><span class="line"><span class="class"><span class="title">method</span>：请求方法</span></span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (Compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Germey'</span></span><br><span class="line">&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict),encoding=<span class="string">'utf8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure></li><li>用Opener构建Handler<br>官方文档：<a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler</a><ul><li>验证<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">'username'</span></span><br><span class="line">password = <span class="string">'password'</span></span><br><span class="line">url = <span class="string">'http://localhost:5000/'</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>代理<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:9743'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>Cookies<ul><li>获取Cookies<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">"="</span>+item.value)</span><br></pre></td></tr></table></figure></li><li>Cookie保存至文件<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'cookies.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line"><span class="comment">#cookie = http.cookiejar.LWPCookieJar(filename)  另一种格式</span></span><br><span class="line"><span class="comment">#cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)  读取</span></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="处理异常"><a href="#处理异常" class="headerlink" title="处理异常"></a>处理异常</h4><ul><li>URLError<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>HTTPError<br>URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre></td></tr></table></figure></li></ul><h4 id="解析链接"><a href="#解析链接" class="headerlink" title="解析链接"></a>解析链接</h4><p>url.parse模块，定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。<br>支持如下协议的URL处理：file、ftp、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。<br>常用方法如下：</p><ul><li><p>urlparse()<br>实现URL的识别与分段。<br>API：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">''</span>, allow_fragments=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">scheme：url中没有协议时，作为默认的协议。</span><br><span class="line">allow_fragments：是否忽略fragment。如果设置为<span class="literal">False</span>，fragment部分就会被忽略，</span><br><span class="line">                 它会被依次解析为query、parameters或者path的一部分，而fragment部分为空。</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html#comment'</span>,</span><br><span class="line">                   scheme=<span class="string">'https'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)  <span class="comment">#返回的result为ParseResult类型，实际上是一个元组，支持result[0]和result.scheme</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">ParseResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, </span><br><span class="line">            path=<span class="string">'/index.html#comment'</span>, params=<span class="string">''</span>, query=<span class="string">''</span>, fragment=<span class="string">''</span>)</span><br></pre></td></tr></table></figure></li><li><p>urlunparse()<br>实现URL的构造。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"></span><br><span class="line"><span class="comment">#data可以用其他类型，但长度必须是6</span></span><br><span class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunparse(data))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">http://www.baidu.com/index.html;user?a=6#comment</span><br></pre></td></tr></table></figure></li><li><p>urlsplit()<br>类似urlparse()，但只返回5个结果，params合并到path里。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"></span><br><span class="line">result = urlsplit(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">SplitResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, </span><br><span class="line">            path=<span class="string">'/index.html;user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre></td></tr></table></figure></li><li><p>urlunsplit()<br>类似urlunparse()，但只传入5个参数。</p></li><li><p>urljoin()</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">urljoin(base_url, target_url)</span><br><span class="line"></span><br><span class="line">分析base_url的scheme、netloc和path三个内容并对target_url进行补充</span><br></pre></td></tr></table></figure></li><li><p>urlencode()<br>将字典序列化为GET请求的参数。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">'http://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">http://www.baidu.com?name=germey&amp;age=22</span><br></pre></td></tr></table></figure></li><li><p>parse_qs()<br>将URL反序列化为字典。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">'name=germey&amp;age=22'</span></span><br><span class="line">print(parse_qs(query))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&#123;<span class="string">'name'</span>: [<span class="string">'germey'</span>], <span class="string">'age'</span>: [<span class="string">'22'</span>]&#125;</span><br></pre></td></tr></table></figure></li><li><p>parse_qsl()<br>将URL转化为元组组成的列表。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[(<span class="string">'name'</span>, <span class="string">'germey'</span>), (<span class="string">'age'</span>, <span class="string">'22'</span>)]</span><br></pre></td></tr></table></figure></li><li><p>quote()<br>将内容（中文字符）转化为URL编码的格式。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">keyword = <span class="string">'壁纸'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8</span><br></pre></td></tr></table></figure></li><li><p>unquote()<br>进行URL解码</p></li></ul><h4 id="分析Robots协议"><a href="#分析Robots协议" class="headerlink" title="分析Robots协议"></a>分析Robots协议</h4><ul><li><p>Robots协议<br>Robots协议也称作爬虫协议、机器人协议，全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robot.txt的文本文件，一般放在网站的根目录下。</p></li><li><p>爬虫名称<br>常见的搜索爬虫的名称及对应的网站：<br>BaiduSpider 百度 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a><br>Googlebot 谷歌 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>360Spider 360 搜索 <a href="http://www.so.com" target="_blank" rel="noopener">www.so.com</a><br>YodaoBot 有道 <a href="http://www.youdao.com" target="_blank" rel="noopener">www.youdao.com</a><br>ia_archiver Alexa <a href="http://www.alexa.cn" target="_blank" rel="noopener">www.alexa.cn</a><br>Scooter altavista <a href="http://www.altavista.com" target="_blank" rel="noopener">www.altavista.com</a></p></li><li><p>robotparser<br>urllib.robotparser模块提供了一个RobotFileParser类，该类的一些方法如下：</p><ul><li>set_url()：用来设置robots.txt文件的链接。也可在创建RobotFileParser对象时传入链接。</li><li>read()：读取robots.txt文件并进行分析。</li><li>parse()：用来解析robots.txt文件，传人的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。</li><li>can_fetch()：该方法传人两个参数，第一个是 User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，结果为True 或False。</li><li>mtime()：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是有必要的，可能需要定期检查来抓取最新的robots.txt。</li><li>modified()：同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间。</li></ul><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line"><span class="comment">#上面两行可以用parse方法来执行读取和分析</span></span><br><span class="line"><span class="comment">#rp.parse(urlopen('http://www.jianshu.com/robots.txt').read().decode('utf-8').split('\n'))</span></span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="使用requests"><a href="#使用requests" class="headerlink" title="使用requests"></a>使用requests</h3><p>解决urllib中Cookies、登录验证、代理设置不方便的问题。<br>安装<code>pip install requests</code><br>requests的官方文档：<a href="http://docs.python-requests.org/" target="_blank" rel="noopener">http://docs.python-requests.org/</a></p><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>requests库包含get()、post()、put()、delete()、head()和options()等方法，分别对应各种方式请求网页。</p><ul><li><p>GET请求</p><ul><li>基本实例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line"><span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=data)</span><br><span class="line">print(type(r))</span><br><span class="line">print(r.status)</span><br><span class="line"><span class="comment">#网页的返回类型实际是JSON格式的str类型，调用json()可将其转化为字典</span></span><br><span class="line">print(type(r.json()))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">requests</span>.<span class="title">models</span>.<span class="title">Response</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">dict</span>'&gt;</span></span><br></pre></td></tr></table></figure></li><li>抓取网页<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) '</span></span><br><span class="line">                    <span class="string">'AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                     <span class="string">'Chrome/52.0.2743.116 Safari/537.36'</span>)</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>, headers=headers)</span><br><span class="line">pattern = re.compile(<span class="string">'explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;'</span>, re.S)</span><br><span class="line">titles = re.findall(pattern, r.text)</span><br><span class="line">print(titles)</span><br></pre></td></tr></table></figure></li><li>抓取二进制数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">"https://github.com/favicon.ico"</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'favicon.ico'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(r.content)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>POST请求</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="string">'22'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=data)</span><br></pre></td></tr></table></figure></li><li><p>响应</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://www.xxyr.cc'</span>)</span><br><span class="line">print(type(r.status_code))</span><br><span class="line">print(type(r.headers))</span><br><span class="line">print(type(r.cookies))</span><br><span class="line">print(type(r.url))</span><br><span class="line">print(type(r.history))</span><br><span class="line">print(type(r.text))  <span class="comment">#返回内容的字符串形式</span></span><br><span class="line">print(type(r.content)) <span class="comment">#返回内容的二进制形式</span></span><br><span class="line">print(requests.codes.ok)  <span class="comment">#内置的返回码</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">int</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">requests</span>.<span class="title">structures</span>.<span class="title">CaseInsensitiveDict</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">requests</span>.<span class="title">cookies</span>.<span class="title">RequestsCookieJar</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">list</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">bytes</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h4><ul><li><p>文件上传</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">files = &#123;<span class="string">'file'</span>: open(<span class="string">'favicon.ico'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>, files=files)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li><li><p>Cookies</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">print(r.cookies)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> r.cookies.items():</span><br><span class="line">    print(key + <span class="string">'='</span> + value)</span><br></pre></td></tr></table></figure><p>可将Cookie字段添加到headers里实现登录：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>,</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://www.zhihu.com'</span>, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>或将其作为cookie参数添加到get()方法里：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">cookies = <span class="string">'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'</span></span><br><span class="line">jar = requests.cookies.RequestsCookieJar()</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> cookie <span class="keyword">in</span> cookies.split(<span class="string">';'</span>):</span><br><span class="line">    key, value = cookie.split(<span class="string">'='</span>, <span class="number">1</span>)</span><br><span class="line">    jar.set(key, value)</span><br><span class="line">r = requests.get(<span class="string">'http://www.zhihu.com'</span>, cookies=jar, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li><li><p>会话维持<br>当访问登录网站后的页面，或同一站点的不同页面时，就需要进行会话维持。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>)</span><br><span class="line">r = s.get(<span class="string">'http://httpbin.org/cookies'</span>)</span><br><span class="line">print(r.text)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"cookies"</span>: &#123;</span><br><span class="line">    <span class="string">"number"</span>: <span class="string">"123456789"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>SSL证书验证</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)  <span class="comment">#默认为True，自动验证证书</span></span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure><p><code>verify=False</code>会忽略证书的验证，但会报一个警告，解决方法如下：</p><ul><li>设置忽略警告<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line"></span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li><li>捕获警告到日志<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">logging.captureWarnings(<span class="literal">True</span>)</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li><li>指定一个本地证书用作客户端证书<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>代理设置</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line"><span class="string">'http'</span>: <span class="string">'http://10.10.1.10:3128'</span>,</span><br><span class="line"><span class="string">'https'</span>: <span class="string">'http://10.10.1.10:1080'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><p>若要使用HTTP Basic Auth，可以使用如下代理形式：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'http://user:password@10.10.1.10:3128/'</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><p>requests还支持SOCKS代理：<br>安装：<code>pip3 install &#39;requests[socks]&#39;</code></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://user:password@host:port'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://user:password@host:port'</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure></li><li><p>超时设置<br>为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应就报错。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>, timeout=<span class="number">1</span>)</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure><p>timeout=1表示超时时间为1秒，默认为None，即永远等待。<br>实际上，请求分为两个阶段，即连接（connect）和读取（read）。上面设置的timeout将用作连接和读取这二者的timeout总和。<br>如果要分别指定，就可以传入一个元组：<br><code>r = requests.get(&#39;https://www.taobao.com&#39;, timeout=(5,11, 30))</code></p></li><li><p>身份认证</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment">#auth=('username', 'password')即auth=HTTPBasicAuth('username', 'password')</span></span><br><span class="line">r = requests.get(<span class="string">'http://localhost:5000'</span>, auth=(<span class="string">'username'</span>, <span class="string">'password'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure><p>此外requests还提供了其他认证方式，如OAuth认证等。</p></li><li><p>Prepared Request<br>用于将请求表示为数据结构，其中各个参数通过一个Request对象来表示。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 '</span>)</span><br><span class="line">                    (<span class="string">'(KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>)</span><br><span class="line">&#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(<span class="string">'POST'</span>, url, data=data, headers=headers)</span><br><span class="line"><span class="comment">#Session的prepare_request()方法将其转化为一个Prepared Request对象</span></span><br><span class="line">prepped = s.prepare_request(req)  </span><br><span class="line">r = s.send(prepped)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li></ul><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>详见： <a href="https://xxyr.cc/post/tech/regex-review/">正则表达式复习</a></p><h3 id="抓取猫眼电影排行"><a href="#抓取猫眼电影排行" class="headerlink" title="抓取猫眼电影排行"></a>抓取猫眼电影排行</h3><p>详见<a href="https://github.com/Python3WebSpider/MaoYan" target="_blank" rel="noopener">https://github.com/Python3WebSpider/MaoYan</a></p><h2 id="Ch-3-解析库的使用"><a href="#Ch-3-解析库的使用" class="headerlink" title="Ch 3 解析库的使用"></a>Ch 3 解析库的使用</h2><h3 id="使用XPath"><a href="#使用XPath" class="headerlink" title="使用XPath"></a>使用XPath</h3><p>lxml库安装：<code>pip install lxml</code><br>如果想查询更多XPath的用法，可以查看：<a href="http://www.w3school.com.cn/xpath/index.asp" target="_blank" rel="noopener">http://www.w3school.eom.cn/xpath/index.asp</a><br>如果想查询更多lxml库的用法，可以查看：<a href="http://lxml.de/" target="_blank" rel="noopener">http://lxml.de／</a></p><h4 id="XPath概览"><a href="#XPath概览" class="headerlink" title="XPath概览"></a>XPath概览</h4><ul><li>XPath，全称XML Path Language，即XML路径语言，是一门在XML文档中查找信息的语言，最初是用来搜寻XML文档的，但同样适用于HTML文档的搜索。</li><li>XPath于1999年ll月16日成为W3C标准，它被设计为供XSLT、XPointer以及其他XML解析软件使用，更多的文档可以访问其官方网站：<a href="https://www.w3.org/TR/xpath" target="_blank" rel="noopener">https://www.w3.org/TR/xpath</a>。</li></ul><h4 id="Xpath常用规则"><a href="#Xpath常用规则" class="headerlink" title="Xpath常用规则"></a>Xpath常用规则</h4><p><img src="https://s3.ax1x.com/2021/01/11/s89S0K.png" alt=""></p><h4 id="实例引入"><a href="#实例引入" class="headerlink" title="实例引入"></a>实例引入</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line"><span class="comment"># html = etree.parse('./test.html', etree.HTMLParser())  #从文件读取</span></span><br><span class="line">result = etree.tostring(html) <span class="comment">#修正和补全HTML代码，但返回bytes类型</span></span><br><span class="line">print(result.decode(<span class="string">'utf-8'</span>)) <span class="comment">#转成str类型</span></span><br></pre></td></tr></table></figure><h4 id="节点选择"><a href="#节点选择" class="headerlink" title="节点选择"></a>节点选择</h4><ul><li>所有节点<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//*'</span>)   <span class="comment">#选择所有节点</span></span><br><span class="line"><span class="comment">#result = html.xpath('//li') #选择所有li节点</span></span><br><span class="line">print(result)</span><br><span class="line">print(result[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回形式是一个列表，每个元素是Element类型</span></span><br><span class="line">Output：</span><br><span class="line">[&lt;Element li at <span class="number">0x2257c34aa48</span>&gt;, &lt;Element li at <span class="number">0x2257c34aa88</span>&gt;]</span><br><span class="line">&lt;Element li at <span class="number">0x2257c34aa48</span>&gt;</span><br></pre></td></tr></table></figure></li><li>子节点<br>/是直接节点，//是所有节点。<br>如选择li节点的所有直接a子节点，可以用<code>//li/a</code>，<br>选择ul节点下的所有a子节点，可以用<code>//ul//a</code></li><li>父节点<br>首先选中href属性为link4.html的a节点，然后再获取其父节点，然后再获取其class属性。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/../@class'</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/parent::*/@class'</span>)</span><br></pre></td></tr></table></figure></li><li>按序选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/a/text()'</span>)           <span class="comment">#序号从1开始</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[last()]/a/text()'</span>)      <span class="comment">#选取最后一个</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[position()&lt;3]/a/text()'</span>)<span class="comment">#选取第1、2个</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[last()-2]/a/text()'</span>)    <span class="comment">#选取倒数第三个</span></span><br></pre></td></tr></table></figure>具体参考：<a href="http://www.w3school.com.cn/xpath/xpath_functions.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_functions.asp</a></li><li>节点轴选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::*'</span>)          <span class="comment">#选取第一个li节点的所有祖先节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::div'</span>)        <span class="comment">#选取第一个li节点的祖先div节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/attribute::*'</span>)         <span class="comment">#获取第一个li节点的所有属性值</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/child::a'</span>)             <span class="comment">#child::直接子节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/descendant::span'</span>)     <span class="comment">#descendant::子孙节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following::*[2]'</span>)      <span class="comment">#following::当前结点之后的所有节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following-sibling::*'</span>) <span class="comment">#following-sibling::</span></span><br><span class="line">                                                    <span class="comment">#当前结点之后的所有同级节点</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="文本获取"><a href="#文本获取" class="headerlink" title="文本获取"></a>文本获取</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]/a/text()'</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]//text()'</span>)</span><br><span class="line"><span class="comment">#前者准确获取li&gt;a内的文本，</span></span><br><span class="line"><span class="comment">#而后者获取li内的所有文本，可能会获取到换行符之类的信息</span></span><br></pre></td></tr></table></figure><h4 id="属性操作"><a href="#属性操作" class="headerlink" title="属性操作"></a>属性操作</h4><ul><li>属性获取<br>获取所有li节点下所有a节点的href属性：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a/@href'</span>)</span><br></pre></td></tr></table></figure></li><li>属性匹配<br>通过属性筛选节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a[@href="link1.html"]'</span>)</span><br></pre></td></tr></table></figure></li><li>属性多值匹配<br>用contains()函数：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li")]/a/text()'</span>)</span><br></pre></td></tr></table></figure></li><li>多属性匹配<br>用运算符and来连接：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first" name="item"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li") and @name="item"]/a/text()'</span>)</span><br></pre></td></tr></table></figure>XPath中的运算符：<br><img src="https://s3.ax1x.com/2021/01/12/sJ0Uaj.png" alt="运算符及其介绍"></li></ul><h3 id="使用Beautiful-Soup"><a href="#使用Beautiful-Soup" class="headerlink" title="使用Beautiful Soup"></a>使用Beautiful Soup</h3><p>安装：<code>pip install beautifulsoup4</code></p><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>Beautiful Soup是Python的一个HTML或XML的解析库，可以用它来方便地从网页中提取数据.<br>官方解释如下：</p><ul><li>Beautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。</li><li>Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。不需要考虑编码方式，除非文档没有指定一个编码方式，这时仅需说明一下原始编码方式就可以了。</li><li>Beautiful Soup已成为和lxml、html6lib 一样出色的Python解释器，为用户灵活地提供不同的解析策略或强劲的速度。</li></ul><h4 id="解析器"><a href="#解析器" class="headerlink" title="解析器"></a>解析器</h4><p>Beautiful Soup在解析时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器。（推荐lxml）<br><img src="https://s3.ax1x.com/2021/01/12/sJ26Lq.png" alt="Beautiful Soup支持的解析器"></p><h4 id="基本用法-1"><a href="#基本用法-1" class="headerlink" title="基本用法"></a>基本用法</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)  <span class="comment">#会自动补全HTML代码</span></span><br><span class="line">print(soup.prettify())              <span class="comment">#将要解析的字符串以标准缩进格式输出</span></span><br><span class="line">print(soup.title.string)            <span class="comment">#获取title节点的文本</span></span><br></pre></td></tr></table></figure><h4 id="节点选择器"><a href="#节点选择器" class="headerlink" title="节点选择器"></a>节点选择器</h4><ul><li>选择元素<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.title)</span><br><span class="line">print(type(soup.title))</span><br><span class="line">print(soup.title.string)</span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.p)   <span class="comment">#选择第一个匹配的p节点</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;</span></span><br><span class="line"><span class="string">&lt;class '</span>bs4.element.Tag<span class="string">'&gt;</span></span><br><span class="line"><span class="string">The Dormouse'</span>s story</span><br><span class="line">&lt;head&gt;&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line">&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure></li><li>提取信息<ul><li>获取名称<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.title.name)</span><br><span class="line">title</span><br></pre></td></tr></table></figure></li><li>获取属性<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.p.attrs)</span><br><span class="line">print(soup.p.attrs[<span class="string">'name'</span>])</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'class'</span>: [<span class="string">'title'</span>], <span class="string">'name'</span>: <span class="string">'dromouse'</span>&#125;</span><br><span class="line">dromouse</span><br></pre></td></tr></table></figure>或：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.p[<span class="string">'class'</span>])</span><br><span class="line">print(soup.p[<span class="string">'name'</span>])</span><br><span class="line"></span><br><span class="line">[<span class="string">'title'</span>]   <span class="comment">#class属性值可能有多个，所以返回列表</span></span><br><span class="line">dromouse</span><br></pre></td></tr></table></figure></li><li>获取内容<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.title.string)  <span class="comment">#选择第一个匹配的</span></span><br><span class="line">The Dormouse<span class="string">'s story</span></span><br></pre></td></tr></table></figure></li></ul></li><li>嵌套选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.head.title.string)   <span class="comment">#选择head内的title节点</span></span><br><span class="line">The Dormouse<span class="string">'s story</span></span><br></pre></td></tr></table></figure></li><li>关联选择<ul><li>子节点和子孙节点<br>直接子节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">soup.p.contents   <span class="comment">#返回包含各直接子节点的列表</span></span><br></pre></td></tr></table></figure>或<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, child <span class="keyword">in</span> enumerate(soup.p.children):   <span class="comment">#返回生成器类型</span></span><br><span class="line">  print(i, child)</span><br></pre></td></tr></table></figure>子孙节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, child <span class="keyword">in</span> enumerate(soup.p.descendants):</span><br><span class="line">  print(i, child)</span><br></pre></td></tr></table></figure></li><li>父节点和祖先节点<br>直接父节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.a.parent)</span><br></pre></td></tr></table></figure>祖先节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, parent <span class="keyword">in</span> enumerate(soup.p.parents):</span><br><span class="line">  print(i, parent)</span><br></pre></td></tr></table></figure></li><li>兄弟节点<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Next Sibling'</span>, soup.a.next_sibling)</span><br><span class="line">print(<span class="string">'Prev Sibling'</span>, soup.a.previous_sibling)</span><br><span class="line">print(<span class="string">'Next Siblings'</span>, list(enumerate(soup.a.next_siblings)))</span><br><span class="line">print(<span class="string">'Prev Siblings'</span>, list(enumerate(soup.a.previous_siblings)))</span><br></pre></td></tr></table></figure></li><li>提取信息<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.a.next_sibling.string)</span><br><span class="line">print(list(soup.a.parents)[<span class="number">0</span>].attrs[<span class="string">'class'</span>])</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="方法选择器"><a href="#方法选择器" class="headerlink" title="方法选择器"></a>方法选择器</h4><ul><li>findall()<br>API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">find_all(name, attrs, recursive, text, **kwargs)</span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">print(soup.find_all(name=<span class="string">'ul'</span>))</span><br><span class="line">print(type(soup.find_all(name=<span class="string">'ul'</span>)[<span class="number">0</span>]))</span><br><span class="line"><span class="comment">#返回结果为列表类型，每个元素为bs4.element.Tag类型</span></span><br><span class="line"></span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">'id'</span>: <span class="string">'list-1'</span>&#125;))</span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">'name'</span>: <span class="string">'elements'</span>&#125;))</span><br><span class="line">或</span><br><span class="line">print(soup.find_all(id=<span class="string">'list-1'</span>))</span><br><span class="line">print(soup.find_all(class_=<span class="string">'element'</span>))    <span class="comment">#class_因为是关键字</span></span><br><span class="line"></span><br><span class="line">print(soup.find_all(text=re.compile(<span class="string">'link'</span>)))</span><br><span class="line"><span class="comment">#参数可以是字符串，也可以是正则表达式对象</span></span><br></pre></td></tr></table></figure></li><li>find()<br>返回第一个匹配的元素。</li><li>find_parents()和find_parent()：前者返回所有祖先节点，后者返回直接父节点。</li><li>find_next_siblings()和find_next_sibling()：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。</li><li>find_previous_siblings()和find_previous_sibling()：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点 。</li><li>find_all_next()和find_next()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。</li><li>find_all_previous()和find_previous()：前者返回节点前所有符合条件的节点，后者返回第一个符合条件的节点。</li></ul><h4 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><p>参考：<a href="http://www.w3school.com.cn/cssref/css_selectors.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/cssref/css_selectors.asp</a></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.select(<span class="string">'.panel .panel-heading'</span>))   <span class="comment">#返回列表类型</span></span><br><span class="line">print(soup.select(<span class="string">'ul li'</span>))</span><br><span class="line">print(soup.select(<span class="string">'#list-2 .element'</span>))</span><br><span class="line">print(type(soup.select(<span class="string">'ul'</span>)[<span class="number">0</span>]))             <span class="comment">#返回bs4.element.Tag类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#嵌套选择</span></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul.select(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取属性</span></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul[<span class="string">'id'</span>])</span><br><span class="line">    print(ul.attrs[<span class="string">'id'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取文本</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> soup.select(<span class="string">'li'</span>):</span><br><span class="line">    print(<span class="string">'Get Text:'</span>, li.get_text())</span><br><span class="line">    print(<span class="string">'String:'</span>, li.string)</span><br></pre></td></tr></table></figure><h3 id="使用pyquery"><a href="#使用pyquery" class="headerlink" title="使用pyquery"></a>使用pyquery</h3><p>安装<code>pip install pyquery</code><br>pyquery的官方文档：<a href="http://pyquery.readthedocs.io" target="_blank" rel="noopener">http://pyquery.readthedocs.io</a></p><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">html = requests.get(url, headers=headers).text()</span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">print(doc(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#URL初始化</span></span><br><span class="line">doc = PyQuery(url=<span class="string">'http://xxyr.cc'</span>)</span><br><span class="line">print(doc(<span class="string">'title'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件初始化</span></span><br><span class="line">doc = PyQuery(filename=<span class="string">'demo.html'</span>)</span><br><span class="line">print(doc(<span class="string">'li'</span>))</span><br></pre></td></tr></table></figure><h4 id="基本CSS选择器"><a href="#基本CSS选择器" class="headerlink" title="基本CSS选择器"></a>基本CSS选择器</h4><p>关于CSS选择器的更多用法，可以参考：<a href="https://www.w3school.com.cn/css/css_selector_type.asp" target="_blank" rel="noopener">https://www.w3school.com.cn/css/css_selector_type.asp</a></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">print(doc(<span class="string">'#container .list li'</span>))         <span class="comment"># #id  .class</span></span><br><span class="line">print(type(doc(<span class="string">'#container .list li'</span>)))   </span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br></pre></td></tr></table></figure><h4 id="查找结点"><a href="#查找结点" class="headerlink" title="查找结点"></a>查找结点</h4><p>下面介绍一些常用的查询函数，这些函数和jQuery中函数的用法完全相同。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">items = doc(<span class="string">'.list'</span>)</span><br><span class="line">li = doc(<span class="string">'.list .item-0.active'</span>)</span><br><span class="line"></span><br><span class="line">lis1 = items.find(<span class="string">'li'</span>)          <span class="comment">#所有子孙节点</span></span><br><span class="line">lis2 = items.children(<span class="string">'.active'</span>) <span class="comment">#子节点</span></span><br><span class="line"></span><br><span class="line">container = items.parent()       <span class="comment">#获取直接父节点</span></span><br><span class="line">parents = items.parents()        <span class="comment">#获取所有祖先节点，可添加CSS选择器获取特定祖先节点</span></span><br><span class="line"></span><br><span class="line">bro = li.siblings()              <span class="comment">#获取所有兄弟节点，可添加CSS选择器获取特定兄弟节点</span></span><br></pre></td></tr></table></figure><h4 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">lis = doc(<span class="string">'li'</span>).items()</span><br><span class="line">print(type(lis))                <span class="comment">#&lt;class 'generator'&gt;</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> lis:</span><br><span class="line">    print(li, type(li))         <span class="comment">#&lt;class 'pyquery.pyquery.PyQuery'&gt;</span></span><br></pre></td></tr></table></figure><h4 id="获取信息"><a href="#获取信息" class="headerlink" title="获取信息"></a>获取信息</h4><ul><li>获取属性<br>提取到某个PyQuery类型的节点后，就可以调用attr()方法来获取属性：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">a = doc(<span class="string">'.item-0.active a'</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(type(a))</span><br><span class="line">print(a.attr(<span class="string">'href'</span>))   <span class="comment">#若a有多个，只返回第一个a的属性值</span></span><br><span class="line">print(a.attr.href)</span><br><span class="line"></span><br><span class="line">&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">link3</span>.<span class="title">html</span></span></span><br><span class="line"><span class="class"><span class="title">link3</span>.<span class="title">html</span></span></span><br></pre></td></tr></table></figure></li><li>获取文本<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'li'</span>)</span><br><span class="line">print(li.html())    <span class="comment">#返回第一个节点的内部HTML文本</span></span><br><span class="line">print(type(li.html()))</span><br><span class="line">print(li.text())    <span class="comment">#返回所有节点内部的纯文本，中间用一个空格分隔开</span></span><br><span class="line">print(type(li.text()))</span><br><span class="line"></span><br><span class="line">&lt;a href="link2.html"&gt;second item&lt;/a&gt;</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">second</span> <span class="title">item</span> <span class="title">third</span> <span class="title">item</span> <span class="title">fourth</span> <span class="title">item</span> <span class="title">fifth</span> <span class="title">item</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="节点操作"><a href="#节点操作" class="headerlink" title="节点操作"></a>节点操作</h4><p>pyquery提供了一系列方法来对节点进行动态修改，比如为某个节点添加一个 class，移除某个节点等，如append()、empty()和prepend()等方法，它们和jQuery的用法完全一致，详细的用法可以参考官方文档：<a href="http://pyquery.readthedocs.io/en/latest/api.html" target="_blank" rel="noopener">http://pyquery.readthedocs.io/en/latest/api.html</a></p><ul><li>addClass和removeClass<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'.item-0.active'</span>)</span><br><span class="line">li.removeClass(<span class="string">'active'</span>)</span><br><span class="line">li.addClass(<span class="string">'active'</span>)</span><br></pre></td></tr></table></figure></li><li>attr、text和html<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">li.attr(<span class="string">'name'</span>, <span class="string">'link'</span>)             <span class="comment">#修改或增加name属性值为link</span></span><br><span class="line">li.text(<span class="string">'changed item'</span>)             <span class="comment">#将li节点内部的文本替换为纯文本</span></span><br><span class="line">li.html(<span class="string">'&lt;span&gt;changed item&lt;/span&gt;'</span>)<span class="comment">#将li节点内部的文本替换为HTML文本</span></span><br></pre></td></tr></table></figure></li><li>remove()<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">wrap = doc(<span class="string">'.wrap'</span>)</span><br><span class="line">wrap.find(<span class="string">'p'</span>).remove()</span><br><span class="line">type(wrap.find(<span class="string">'p'</span>))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="伪类选择器"><a href="#伪类选择器" class="headerlink" title="伪类选择器"></a>伪类选择器</h4><p>css选择器之所以强大，还有一个很重要的原因，就是支持多种多样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等。示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'li:first-child'</span>)</span><br><span class="line">li = doc(<span class="string">'li:last-child'</span>)</span><br><span class="line">li = doc(<span class="string">'li:nth-child(2)'</span>)     <span class="comment">#第二个节点</span></span><br><span class="line">li = doc(<span class="string">'li:gt(2)'</span>)            <span class="comment">#第三个li之后的li节点</span></span><br><span class="line">li = doc(<span class="string">'li:nth-child(2n)'</span>)    <span class="comment">#偶数位置的节点</span></span><br><span class="line">li = doc(<span class="string">'li:contains(second)'</span>) <span class="comment">#包含second文本的li节点</span></span><br></pre></td></tr></table></figure><h2 id="Ch-4-数据存储"><a href="#Ch-4-数据存储" class="headerlink" title="Ch 4 数据存储"></a>Ch 4 数据存储</h2><h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><h4 id="TXT文本存储"><a href="#TXT文本存储" class="headerlink" title="TXT文本存储"></a>TXT文本存储</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'test.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">  file.write(<span class="string">'\n'</span>.join([question, author, answer]))</span><br><span class="line">  file.write(<span class="string">'\n'</span>+<span class="string">'='</span>*<span class="number">50</span>+<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><p>打开方式：</p><ul><li>r：以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。</li><li>rb：以二进制只读方式打开一个文件。文件指针将会放在文件的开头。</li><li>r+：以读写方式打开一个文件。文件指针将会放在文件的开头。</li><li>rb+：以二进制读写方式打开一个文件。文件指针将会放在文件的开头。</li><li>w：以写入方式打开一个文件。如果该文件已存在，则将其瞿盖。如果该文件不存在，则创建新文件。</li><li>wb：以二进制写入方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li><li>w+：以读写方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li><li>wb+：以二进制读写格式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li><li>a：以追加方式打开一个文件。如果该文件已存在，文件指针将会放在文件结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，则创建新文件来写入。</li><li>ab：以二进制追加方式打开一个文件。如果该文件已存在，则文件指针将会放在文件结尾。如果该文件不存在，则创建新文件来写入。</li><li>a+：以读写方式打开一个文件。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果眩文件不存在，则创建新文件来读写。</li><li>ab+：以二进制追加方式打开一个文件。如果该文件已存在，则文件指针将会放在文件结尾。如果该文件不存在，则创建新文件用于读写。</li></ul><h4 id="JSON文件存储"><a href="#JSON文件存储" class="headerlink" title="JSON文件存储"></a>JSON文件存储</h4><p>JSON，全称为JavaScript Object Notation，也就是JavaScript对象标记，通过对象和数组的组合来表示数据，构造简洁但是结构化程度非常高，是一种轻量级的数据交换格式。<br>对象和数组：</p><ul><li>对象：在JavaScript中使用花括号｛｝包裹起来的内容，数据结构为｛ keyl : valuel, key2 : value2, … ｝的键值对结构。 在面向对象的语言中，key为对象的属性，value为对应的值。键名可以使用整数和字符串来表示。值的类型可以是任意类型。</li><li>数组：数组在JavaScript中是方括号［］包裹起来的内容，数据结构为［ “java”， “javascript”］的索引结构。在JavaScript中，数组是一种比较特殊的数据类型，它也可以像对象那样使用键值对，但还是索引用得多。同样，值的类型可以是任意类型。</li></ul><p>示例：loads()和dumps()</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'package.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    str = file.read()</span><br><span class="line">    data = json.loads(str)</span><br><span class="line">    print(type(data))</span><br><span class="line">    <span class="comment">#获取key为scripts里的key为build的value，若没有则返回test</span></span><br><span class="line">    print(data[<span class="string">'scripts'</span>].get(<span class="string">'build'</span>, <span class="string">'test'</span>))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">dict</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">hexo</span> <span class="title">generate</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">with</span> <span class="title">open</span><span class="params">(<span class="string">'package.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span> <span class="title">as</span> <span class="title">file</span>:</span></span><br><span class="line">    <span class="comment">#将json对象转化为字符串，且缩进为2，指定编码为了显示中文</span></span><br><span class="line">    file.write(json.dumps(data, indent=<span class="number">2</span>, ensure_ascii=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure><p>若json文件包含多条记录，可以：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#逐行读取</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'moviebt.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file.readlines():</span><br><span class="line">        data = json.loads(line)                 <span class="comment">#data为字典</span></span><br><span class="line">        print(data.get(<span class="string">'result'</span>).get(<span class="string">'title'</span>))</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">    data = [json.loads(line) <span class="keyword">for</span> line <span class="keyword">in</span> file]  <span class="comment">#data为列表</span></span><br><span class="line">    print(data[<span class="number">0</span>].get(<span class="string">'result'</span>).get(<span class="string">'title'</span>))</span><br></pre></td></tr></table></figure><p>参见：<br><a href="https://stackoverflow.com/questions/48140858/json-decoder-jsondecodeerror-extra-data-line-2-column-1-char-190" target="_blank" rel="noopener">json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 190) [duplicate]</a><br><a href="https://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data" target="_blank" rel="noopener">Python json.loads shows ValueError: Extra data</a></p><h4 id="CSV文件存储"><a href="#CSV文件存储" class="headerlink" title="CSV文件存储"></a>CSV文件存储</h4><p>CSV，全称为Comma-Separated Values，中文可以叫作逗号分隔值或字符分隔值，其文件以纯文本形式存储表格数据。</p><ul><li>写入<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open (<span class="string">'data.csv'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">  <span class="comment">#delimiter指定分隔符 lineterminator指定行终止符，默认\n</span></span><br><span class="line">  writer = csv.writer(csvfile, delimiter=<span class="string">' '</span>, lineterminator=<span class="string">'\n\n'</span>)</span><br><span class="line">  writer.writerow([<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'age'</span>])</span><br><span class="line">  writer.writerow([<span class="string">'10001'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>])</span><br><span class="line">  writer.writerow([<span class="string">'10002'</span>, <span class="string">'Bob'</span>, <span class="number">22</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">#写入多行</span></span><br><span class="line">  writer.writerows([<span class="string">'10001'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>], [<span class="string">'10002'</span>, <span class="string">'Bob'</span>, <span class="number">22</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">#字典写入</span></span><br><span class="line">  fieldnames = [<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'age'</span>)</span><br><span class="line">  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)</span><br><span class="line">  writer. writeheader()</span><br><span class="line">  writer.writerow(&#123;<span class="string">'id'</span>: <span class="string">'10001'</span>, <span class="string">'name'</span> :<span class="string">'Mike'</span>, <span class="string">'age'</span> : <span class="number">20</span>&#125;)</span><br><span class="line">  writer.writerow(&#123;<span class="string">'id'</span>: <span class="string">'10002'</span>, <span class="string">'name'</span> : <span class="string">'Bob'</span>, <span class="string">'age'</span> : <span class="number">22</span>&#125;)</span><br><span class="line">  writer.writerow(&#123;<span class="string">'id'</span>: <span class="string">'10003'</span>, <span class="string">'name'</span> :<span class="string">'Jordan'</span>, <span class="string">'age'</span> : <span class="number">21</span>&#125;)</span><br></pre></td></tr></table></figure></li><li>读取<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>,<span class="string">'r'</span>, encoding=<span class="string">'utf 8'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">  reader = csv.reader(csvfile)</span><br><span class="line">  <span class="comment"># fileData = list(reader)   #fileData[0][0]</span></span><br><span class="line">  <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">    print(<span class="string">'row'</span>+str(row.line_num)+str(row))</span><br><span class="line"></span><br><span class="line"><span class="comment">#或</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read csv(<span class="string">'data.csv'</span>)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure></li></ul><h3 id="关系型数据库存储"><a href="#关系型数据库存储" class="headerlink" title="关系型数据库存储"></a>关系型数据库存储</h3><p>关系型数据库是基于关系模型的数据库，而关系模型是通过二维表来保存的，所以它的存储方式就是行列组成的表，每一列是一个字段，每一行是一条记录。<br>表可以看作某个实体的集合，而实体之间存在联系，这就需要表与表之间的关联关系来体现，如主键外键的关联关系。多个表组成一个数据库，也就是关系型数据库。</p><h4 id="MySQL存储"><a href="#MySQL存储" class="headerlink" title="MySQL存储"></a>MySQL存储</h4><p>相关链接:</p><ul><li>GitHub: <a href="https://github.com/PyMySQL/PyMySQL" target="_blank" rel="noopener">https://github.com/PyMySQL/PyMySQL</a></li><li>官方文档：<a href="http://pymysql.readthedocs.io/" target="_blank" rel="noopener">http://pymysql.readthedocs.io/</a></li><li>PyPI: <a href="https://pypi.python.org/pypi/PyMySQL" target="_blank" rel="noopener">https://pypi.python.org/pypi/PyMySQL</a></li></ul><p>安装：<code>pip install pymysql</code><br>相关操作:</p><ul><li>连接、创建数据库<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>)</span><br><span class="line">cursor = db.cursor()        <span class="comment">#获得操作游标</span></span><br><span class="line">cursor.execute(<span class="string">'SELECT VERSION()'</span>)</span><br><span class="line">data = cursor.fetchone()    <span class="comment">#获得第一条数据</span></span><br><span class="line">print(<span class="string">'Database version:'</span>, data)</span><br><span class="line">cursor.execute(<span class="string">"CREATE DATABASE spiders DEFAULT CHARACTER SET utf8"</span>)</span><br><span class="line">db.close()</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">Database version: (<span class="string">'5.7.29-log'</span>,)</span><br></pre></td></tr></table></figure></li><li>创建表<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS students (id VARCHAR(255) NOT NULL, name VARCHAR(255) NOT NULL, age INT NOT NULL, PRIMARY KEY (id))'</span></span><br><span class="line">cursor.execute(sql)</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure></li><li>插入数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">id = <span class="string">'20120001'</span></span><br><span class="line">user = <span class="string">'Bob'</span></span><br><span class="line">age = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'INSERT INTO students(id, name, age) values(%s, %s, %s)'</span>  <span class="comment">#采用格式化符%而非字符串拼接</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql, (id, user, age))</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>改进,根据字典动态构造：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20120001'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span></span><br><span class="line">&#125;</span><br><span class="line">table = <span class="string">'students'</span></span><br><span class="line">keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">sql = <span class="string">'INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES (&#123;values&#125;)'</span>.format(table=table, keys=keys, values=values)</span><br><span class="line"><span class="comment">#INSERT INTO students(id , name, age) VALUES (%s, %s, %s)</span></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">if</span> cursor.execute(sql, tuple(data.values())):</span><br><span class="line">        print(<span class="string">'Successful'</span>)</span><br><span class="line">        db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Failed'</span>)</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>这里涉及事务的问题，事务的特性如下：<ul><li>原子性（atomicity)：事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做。</li><li>一致性（consistency)：事务必须使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。</li><li>隔离性（isolation)：一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。</li><li>持久性（durability)：持续性也称永久性（permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。</li></ul></li><li>更新数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">sql = <span class="string">'UPDATE students SET age = %s WHERE name = %s'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql, (<span class="number">25</span>, <span class="string">'Bob'</span>))</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br></pre></td></tr></table></figure>改进,字典传值+去重:<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20120001'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">table = <span class="string">'students'</span></span><br><span class="line">keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES (&#123;values&#125;) ON DUPLICATE KEY UPDATE'</span>.format(table=table, keys=keys,</span><br><span class="line">                                                                                     values=values)</span><br><span class="line"><span class="comment">#INSERT INTO students(id, name, age) VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE</span></span><br><span class="line">update = <span class="string">','</span>.join([<span class="string">" &#123;key&#125; = %s"</span>.format(key=key) <span class="keyword">for</span> key <span class="keyword">in</span> data])</span><br><span class="line">sql += update</span><br><span class="line"><span class="comment">#INSERT INTO students(id, name, age) VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE id = %s, name = %s, age = %s</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">if</span> cursor.execute(sql, tuple(data.values()) * <span class="number">2</span>):</span><br><span class="line">        print(<span class="string">'Successful'</span>)</span><br><span class="line">        db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Failed'</span>)</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure></li><li>删除数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">table = <span class="string">'students'</span></span><br><span class="line">condition = <span class="string">'age &gt; 20'</span></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'DELETE FROM  &#123;table&#125; WHERE &#123;condition&#125;'</span>.format(table=table, condition=condition)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br><span class="line"></span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure></li><li>查询数据<br>逐行读取(注意游标类似全局变量)<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'SELECT * FROM students WHERE age &gt;= 20'</span></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    print(<span class="string">'Count:'</span>, cursor.rowcount)</span><br><span class="line">    row = cursor.fetchone()</span><br><span class="line">    <span class="comment"># results = cursor.fetchall()</span></span><br><span class="line">    <span class="keyword">while</span> row:</span><br><span class="line">        print(<span class="string">'Row:'</span>, row)</span><br><span class="line">        row = cursor.fetchone()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Error'</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="非关系型数据库存储"><a href="#非关系型数据库存储" class="headerlink" title="非关系型数据库存储"></a>非关系型数据库存储</h3><p>NoSQL,全称Not Only SQL，意为不仅仅是SQL，泛指非关系型数据库。NoSQL是基于键值对的，且不需要经过SQL层的解析，数据之间没有耦合性，性能非常高。<br>非关系型数据库又可细分如下。</p><ul><li>键值存储数据库：代表有Redis、Voldemort和Oracle BDB等。</li><li>列存储数据库：代表有Cassandra、HBase和Riak等。</li><li>文档型数据库：代表有CouchDB和MongoDB等。</li><li>图形数据库：代表有Neo4J、lnfoGrid和Infinite Graph等。</li></ul><p>对于爬虫的数据存储来说一条数据可能存在某些字段提取失败而缺失的情况，而且数据可能随时调整。另外，数据之间还存在嵌套关系。如果使用关系型数据库存储，一是需要提前建表，二是如果存在数据嵌套关系的话，需要进行序列化操作才可以存储，非常不方便。如果用了非关系型数据库，就可以避免一些麻烦，更简单高效。</p><h4 id="MongoDB存储"><a href="#MongoDB存储" class="headerlink" title="MongoDB存储"></a>MongoDB存储</h4><p>MongoDB 是由C++语言编写的非关系型数据库，是一个基于分布式文件存储的开源数据库系统，其内容存储形式类似JSON对象，字段值可以包含其他文档、数组及文档数组，非常灵活。<br>相关链接:</p><ul><li>GitHub: <a href="https://github.com/mongodb/mongo-python-driver" target="_blank" rel="noopener">https://github.com/mongodb/mongo-python-driver</a></li><li>官方文档：<a href="https://api.mongodb.com/python/current/" target="_blank" rel="noopener">https://api.mongodb.com/python/current/</a></li><li>PyPI: <a href="https://pypi.python.org/pypi/pymongo" target="_blank" rel="noopener">https://pypi.python.org/pypi/pymongo</a><br>安装:<code>pip install pymongo</code><br>相关操作:</li><li>初始化<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"></span><br><span class="line"><span class="comment">#连接MongoDB</span></span><br><span class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</span><br><span class="line"><span class="comment"># client = MongoClient('mongodb://localhost:27017/')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#指定数据库</span></span><br><span class="line">db = client.test</span><br><span class="line"><span class="comment"># db = client['test']</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#指定集合</span></span><br><span class="line">collection = db.students</span><br><span class="line"><span class="comment"># collection = db['students']</span></span><br></pre></td></tr></table></figure></li><li>插入数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">student1 = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170101'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Jordan'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">student2 = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170202'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Mike'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#insert返回一个ObjectId类型的_id值</span></span><br><span class="line">result = collection.insert(student1)</span><br><span class="line">result = collection.insert([student1, student2])</span><br><span class="line"></span><br><span class="line"><span class="comment">#官方推荐使用更加严格的方法</span></span><br><span class="line"><span class="comment">#返回的是InsertOneResult对象</span></span><br><span class="line">result = collection.insert_one(student1)</span><br><span class="line">print(result.inserted_id)</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回的是InsertManyResult对象</span></span><br><span class="line">result = collection.insert_many([student1, student2])</span><br><span class="line">print(result.inserted_ids)</span><br></pre></td></tr></table></figure></li><li>查询<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">查询单条数据</span><br><span class="line">result = collection.find_one(&#123;<span class="string">'name'</span>: <span class="string">'Mike'</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据ObjectId查询</span></span><br><span class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</span><br><span class="line"></span><br><span class="line">result = collection.find_one(&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'593278c115c2602667ec6bae'</span>)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多条数据查询</span></span><br><span class="line">results = collection.find(&#123;<span class="string">'age'</span>: <span class="number">20</span>&#125;)  <span class="comment">#返回Cursor类型，相当于生成器</span></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 条件查询</span></span><br><span class="line">results = collection.find(&#123;<span class="string">'age'</span>: &#123;<span class="string">'$gt'</span>: <span class="number">20</span>&#125;&#125;)</span><br><span class="line"><span class="comment"># 正则匹配查询</span></span><br><span class="line">results = collection.find(&#123;<span class="string">'age'</span>: &#123;<span class="string">'$Regex'</span>: <span class="string">'^M.*'</span>&#125;&#125;)</span><br></pre></td></tr></table></figure>比较符号表如下:<br><img src="https://s3.ax1x.com/2021/01/22/soZbse.png" alt=""><br>功能符号表如下:<br><img src="https://s3.ax1x.com/2021/01/22/soZXdA.png" alt=""></li><li>计数<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">count = collection.find(<span class="string">'age'</span>: <span class="number">20</span>&#125;).count()</span><br></pre></td></tr></table></figure></li><li>排序<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING)</span><br></pre></td></tr></table></figure></li><li>偏移<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 偏移2，忽略前两个元素，得到第三个及以后的元素</span></span><br><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING).skip(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定个数</span></span><br><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING).skip(<span class="number">2</span>).limit(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据量大时采用以下办法</span></span><br><span class="line">collection.find(&#123;<span class="string">'_id'</span>: &#123;<span class="string">'$gt'</span>: ObjectId(<span class="string">'593278c815c2602678bb2b8d'</span>)&#125;&#125;)</span><br></pre></td></tr></table></figure></li><li>更新<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">condition = &#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;</span><br><span class="line">student = collection.find_one(condition)</span><br><span class="line">student[<span class="string">'age'</span>] = <span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回结果是字典形式，ok代表执行成功，nModified代表影响的数据条数</span></span><br><span class="line">result = collection.update(condition, student)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只更新该字段，不影响其他已存在字段</span></span><br><span class="line">result = collection.update_one(condition, &#123;<span class="string">'$set'</span>: student&#125;)</span><br><span class="line">print(result.matched_count, result.modified_count)</span><br></pre></td></tr></table></figure></li><li>删除<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">result = collection.remove(&#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;)</span><br><span class="line"></span><br><span class="line">result = collection.delete_one(&#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;)</span><br><span class="line">result = collection.delete_many(&#123;<span class="string">'age'</span>: &#123;<span class="string">'$lt'</span>: <span class="number">25</span>&#125;&#125;)</span><br><span class="line">print(result.deleted_count)</span><br></pre></td></tr></table></figure></li><li>其他操作<ul><li>另外，PyMongo还提供了一些组合方法，如find_one_and_delete()、find one and_replace()和find_one_and_update()，它们是查找后删除、替换和更新操作，其用法与上述方法基本一致。</li><li>还可以对索引进行操作，相关方法有create_index()、create_indexes()和drop_index()等。</li><li>关于PyMongo的详细用法，可以参见官方文档：<br><a href="http://api.mongodb.com/python/current/api/pymongo/collection.html" target="_blank" rel="noopener">http://api.mongodb.com/python/current/api/pymongo/collection.html</a></li><li>另外，还有对数据库和集合本身等的一些操作，可以参见官方文档：<br><a href="http://api.mongodb.com/python/current/api/pymongo" target="_blank" rel="noopener">http://api.mongodb.com/python/current/api/pymongo</a></li></ul></li></ul><h4 id="Redis存储"><a href="#Redis存储" class="headerlink" title="Redis存储"></a>Redis存储</h4><p>GitHub: <a href="https://github.com/andymccurdy/redis-py" target="_blank" rel="noopener">https://github.com/andymccurdy/redis-py</a><br>官方文档：<a href="https://redis-py.readthedocs.io/" target="_blank" rel="noopener">https://redis-py.readthedocs.io/</a></p><ul><li>初始化<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> StrictRedis</span><br><span class="line"></span><br><span class="line">redis = StrictRedis(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, password=<span class="string">'foobared'</span>)</span><br><span class="line">redis.set(<span class="string">'name'</span>, <span class="string">'Bob'</span>)</span><br><span class="line">print(redis.get(<span class="string">'name'</span>))</span><br></pre></td></tr></table></figure>或<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> StrictRedis, ConnectionPool</span><br><span class="line"></span><br><span class="line">pool = ConnectionPool(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, password=<span class="string">'foobared'</span>)</span><br><span class="line">redis = StrictRedis(connection_pool=pool)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过URL构建pool</span></span><br><span class="line">url = <span class="string">'redis://:foobared@localhost:6379/0'</span></span><br><span class="line">pool = ConnectionPool.from_url(url)</span><br><span class="line">redis = StrictRedis(connection_pool=pool)</span><br></pre></td></tr></table></figure></li><li>键操作</li><li>字符串操作</li><li>列表操作</li><li>散列操作</li><li>集合操作</li><li>有序集合操作</li><li>RedisDump<ul><li>redis-dump</li><li>redis-load</li></ul></li></ul><h2 id="Ch-5-Ajax数据爬取"><a href="#Ch-5-Ajax数据爬取" class="headerlink" title="Ch 5 Ajax数据爬取"></a>Ch 5 Ajax数据爬取</h2><h3 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h3><h3 id="Ajax分析方法"><a href="#Ajax分析方法" class="headerlink" title="Ajax分析方法"></a>Ajax分析方法</h3><h3 id="Ajax结果提取"><a href="#Ajax结果提取" class="headerlink" title="Ajax结果提取"></a>Ajax结果提取</h3><h3 id="分析Ajax爬取今日头条节拍美图"><a href="#分析Ajax爬取今日头条节拍美图" class="headerlink" title="分析Ajax爬取今日头条节拍美图"></a>分析Ajax爬取今日头条节拍美图</h3><h2 id="Ch-6-动态渲染页面爬取"><a href="#Ch-6-动态渲染页面爬取" class="headerlink" title="Ch 6 动态渲染页面爬取"></a>Ch 6 动态渲染页面爬取</h2><h3 id="Selenium的使用"><a href="#Selenium的使用" class="headerlink" title="Selenium的使用"></a>Selenium的使用</h3><h3 id="Splash的使用"><a href="#Splash的使用" class="headerlink" title="Splash的使用"></a>Splash的使用</h3><h3 id="Splash负载均衡配置"><a href="#Splash负载均衡配置" class="headerlink" title="Splash负载均衡配置"></a>Splash负载均衡配置</h3><h3 id="使用Selenium爬取淘宝商品"><a href="#使用Selenium爬取淘宝商品" class="headerlink" title="使用Selenium爬取淘宝商品"></a>使用Selenium爬取淘宝商品</h3><h2 id="Ch-7-验证码的识别"><a href="#Ch-7-验证码的识别" class="headerlink" title="Ch 7 验证码的识别"></a>Ch 7 验证码的识别</h2><h3 id="图形验证码识别"><a href="#图形验证码识别" class="headerlink" title="图形验证码识别"></a>图形验证码识别</h3><h3 id="滑动验证码识别"><a href="#滑动验证码识别" class="headerlink" title="滑动验证码识别"></a>滑动验证码识别</h3><h3 id="点触验证码识别"><a href="#点触验证码识别" class="headerlink" title="点触验证码识别"></a>点触验证码识别</h3><h3 id="宫格验证码识别"><a href="#宫格验证码识别" class="headerlink" title="宫格验证码识别"></a>宫格验证码识别</h3><h2 id="Ch-8-代理的使用"><a href="#Ch-8-代理的使用" class="headerlink" title="Ch 8 代理的使用"></a>Ch 8 代理的使用</h2><h3 id="代理的设置"><a href="#代理的设置" class="headerlink" title="代理的设置"></a>代理的设置</h3><h4 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy = <span class="string">'127.0.0.1:9743'</span></span><br><span class="line"><span class="comment">#需认证的代理</span></span><br><span class="line"><span class="comment"># proxy = 'username:password@127.0.0.1:9743'</span></span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy</span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure><p>SOCKS5代理：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># pip3 install PySocks</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> socks</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">socks.set_default_proxy(socks.SOCKS5, <span class="string">'127.0.0.1'</span>, <span class="number">9742</span>)</span><br><span class="line">socket.socket = socks.socksocket</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure><h4 id="requests"><a href="#requests" class="headerlink" title="requests"></a>requests</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxy = <span class="string">'127.0.0.1:9743'</span></span><br><span class="line"><span class="comment">#需认证的代理</span></span><br><span class="line"><span class="comment"># proxy = 'username:password@127.0.0.1:9743'</span></span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">'http://httpbin.org/get'</span>, proxies=proxies)</span><br><span class="line">    print(response.text)</span><br><span class="line"><span class="keyword">except</span> requests.exceptions.ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'Error'</span>, e.args)</span><br></pre></td></tr></table></figure><p>SOCKS5代理：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pip install 'requests[socks]'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxy = <span class="string">'127.0.0.1:9742'</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://'</span> + proxy,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://'</span> + proxy</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">'http://httpbin.org/get'</span>, proxies=proxies)</span><br><span class="line">    print(response.text)</span><br><span class="line"><span class="keyword">except</span> requests.exceptions.ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'Error'</span>, e.args)</span><br></pre></td></tr></table></figure><p>或：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> socks</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line">socks.set_default_proxy(socks.SOCKS5, <span class="string">'127.0.0.1'</span>, <span class="number">9742</span>)</span><br><span class="line">socket.socket = socks.socksocket</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">    print(response.text)</span><br><span class="line"><span class="keyword">except</span> requests.exceptions.ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'Error'</span>, e.args)</span><br></pre></td></tr></table></figure><h4 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h4><ul><li>Chrome<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">proxy = <span class="string">'127.0.0.1:9743'</span></span><br><span class="line"></span><br><span class="line">chrome_options = webdriver.ChromeOptions()</span><br><span class="line">chrome_options.add_argument(<span class="string">'--proxy-server=http://'</span> + proxy)</span><br><span class="line">chrome = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line">chrome.get(<span class="string">'http://httpbin.org/get'</span>)</span><br></pre></td></tr></table></figure>认证代理：<br>需要在本地创建一个manifest.json配置文件和background脚本来设置认证代理。运行代码之后本地会生成一个proxy_auth__plugin.zip文件来保存当前配置。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line">ip = <span class="string">'127.0.0.1'</span></span><br><span class="line">port = <span class="number">9743</span></span><br><span class="line">username = <span class="string">'foo'</span></span><br><span class="line">password = <span class="string">'bar'</span></span><br><span class="line"></span><br><span class="line">manifest_json = <span class="string">"""</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    "version": "1.0.0",</span></span><br><span class="line"><span class="string">    "manifest_version": 2,</span></span><br><span class="line"><span class="string">    "name": "Chrome Proxy",</span></span><br><span class="line"><span class="string">    "permissions": [</span></span><br><span class="line"><span class="string">        "proxy",</span></span><br><span class="line"><span class="string">        "tabs",</span></span><br><span class="line"><span class="string">        "unlimitedStorage",</span></span><br><span class="line"><span class="string">        "storage",</span></span><br><span class="line"><span class="string">        "&lt;all_urls&gt;",</span></span><br><span class="line"><span class="string">        "webRequest",</span></span><br><span class="line"><span class="string">        "webRequestBlocking"</span></span><br><span class="line"><span class="string">    ],</span></span><br><span class="line"><span class="string">    "background": &#123;</span></span><br><span class="line"><span class="string">        "scripts": ["background.js"]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">background_js = <span class="string">"""</span></span><br><span class="line"><span class="string">var config = &#123;</span></span><br><span class="line"><span class="string">        mode: "fixed_servers",</span></span><br><span class="line"><span class="string">        rules: &#123;</span></span><br><span class="line"><span class="string">          singleProxy: &#123;</span></span><br><span class="line"><span class="string">            scheme: "http",</span></span><br><span class="line"><span class="string">            host: "%(ip)s",</span></span><br><span class="line"><span class="string">            port: %(port)s</span></span><br><span class="line"><span class="string">          &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">chrome.proxy.settings.set(&#123;value: config, scope: "regular"&#125;, function() &#123;&#125;);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">function callbackFn(details) &#123;</span></span><br><span class="line"><span class="string">    return &#123;</span></span><br><span class="line"><span class="string">        authCredentials: &#123;</span></span><br><span class="line"><span class="string">            username: "%(username)s",</span></span><br><span class="line"><span class="string">            password: "%(password)s"</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">chrome.webRequest.onAuthRequired.addListener(</span></span><br><span class="line"><span class="string">            callbackFn,</span></span><br><span class="line"><span class="string">            &#123;urls: ["&lt;all_urls&gt;"]&#125;,</span></span><br><span class="line"><span class="string">            ['blocking']</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span> % &#123;<span class="string">'ip'</span>: ip, <span class="string">'port'</span>: port, <span class="string">'username'</span>: username, <span class="string">'password'</span>: password&#125;</span><br><span class="line"></span><br><span class="line">plugin_file = <span class="string">'proxy_auth_plugin.zip'</span></span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(plugin_file, <span class="string">'w'</span>) <span class="keyword">as</span> zp:</span><br><span class="line">    zp.writestr(<span class="string">"manifest.json"</span>, manifest_json)</span><br><span class="line">    zp.writestr(<span class="string">"background.js"</span>, background_js)</span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">"--start-maximized"</span>)</span><br><span class="line">chrome_options.add_extension(plugin_file)</span><br><span class="line">browser = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line">browser.get(<span class="string">'http://httpbin.org/get'</span>)</span><br></pre></td></tr></table></figure></li><li>PhatomJS<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">service_args = [</span><br><span class="line">    <span class="string">'--proxy=127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'--proxy-type=http'</span></span><br><span class="line">]</span><br><span class="line"><span class="comment"># 认证代理</span></span><br><span class="line"><span class="comment"># service_args = [</span></span><br><span class="line"><span class="comment">#     '--proxy=127.0.0.1:9743',</span></span><br><span class="line"><span class="comment">#     '--proxy-type=http',</span></span><br><span class="line"><span class="comment">#     '--proxy-auth=username:password'</span></span><br><span class="line"><span class="comment"># ]</span></span><br><span class="line"></span><br><span class="line">browser = webdriver.PhantomJS(service_args=service_args)</span><br><span class="line">browser.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">print(browser.page_source)</span><br></pre></td></tr></table></figure></li></ul><h3 id="代理池的维护"><a href="#代理池的维护" class="headerlink" title="代理池的维护"></a>代理池的维护</h3><h4 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h4><p><img src="https://s3.ax1x.com/2021/01/23/s7gFxA.png" alt="代理池架构"><br>代理池分为4个模块：存储模块、获取模块、检测模块和接口模块。</p><ul><li>存储模块<br>存储模块使用Redis的有序集合，用来做代理的去重和状态标识，同时它也是中心模块和基础模块，将其他模块串联起来。</li><li>获取模块<br>获取模块定时从代理网站获取代理，将获取的代理传递给存储模块，并保存到数据库。</li><li>检测模块<br>检测模块定时通过存储模块获取所有代理，并对代理进行检测，根据不同的检测结果对代理设置不同的标识。</li><li>接口模块<br>接口模块通过WebAPI提供服务接口，接口通过连接数据库并通过Web形式返回可用的代理。</li></ul><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>参考项目地址：<a href="https://github.com/Python3WebSpider/ProxyPool" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ProxyPool</a></p><ul><li><p>存储模块<br>对于代理池来说，有序集合中每个元素的分数可以作为判断一个代理是否可用的标志，100为最高分，代表最可用，0为最低分，代表最不可用。如果要获取可用代理，可以从代理池中随机获取分数最高的代理。<br>设置分数规则如下：</p><ul><li>分数100为可用，检测器会定时循环检测每个代理可用情况，一旦检测到有可用的代理就立即置为100，检测到不可用就将分数减1，分数减至0后代理移除。</li><li>新获取的代理的分数为10，如果测试可行，分数立即置为100，不可行则分数减1，分数减至0后代理移除。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">from</span> proxypool.exceptions <span class="keyword">import</span> PoolEmptyException</span><br><span class="line"><span class="keyword">from</span> proxypool.schemas.proxy <span class="keyword">import</span> Proxy</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> REDIS_HOST, REDIS_PORT, REDIS_PASSWORD, REDIS_DB, REDIS_KEY</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> PROXY_SCORE_MAX, PROXY_SCORE_MIN, PROXY_SCORE_INIT</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> choice</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">from</span> proxypool.utils.proxy <span class="keyword">import</span> is_valid_proxy, convert_proxy_or_proxies</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">REDIS_CLIENT_VERSION = redis.__version__</span><br><span class="line">IS_REDIS_VERSION_2 = REDIS_CLIENT_VERSION.startswith(<span class="string">'2.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisClient</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    redis connection client of proxypool</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, db=REDIS_DB, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        init redis client</span></span><br><span class="line"><span class="string">        :param host: redis host</span></span><br><span class="line"><span class="string">        :param port: redis port</span></span><br><span class="line"><span class="string">        :param password: redis password</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.db = redis.StrictRedis(host=host, port=port, password=password, db=db, decode_responses=<span class="literal">True</span>, **kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 函数参数中的冒号是参数的类型建议符，告诉程序员希望传入的实参的类型。</span></span><br><span class="line">    <span class="comment"># 函数后面跟着的箭头是函数返回值的类型建议符，用来说明该函数返回的值是什么类型。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, proxy: Proxy, score=PROXY_SCORE_INIT)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        add proxy and set it to init score</span></span><br><span class="line"><span class="string">        :param proxy: proxy, ip:port, like 8.8.8.8:88</span></span><br><span class="line"><span class="string">        :param score: int score</span></span><br><span class="line"><span class="string">        :return: result</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_valid_proxy(<span class="string">f'<span class="subst">&#123;proxy.host&#125;</span>:<span class="subst">&#123;proxy.port&#125;</span>'</span>):</span><br><span class="line">            logger.info(<span class="string">f'invalid proxy <span class="subst">&#123;proxy&#125;</span>, throw it'</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.exists(proxy):</span><br><span class="line">            <span class="keyword">if</span> IS_REDIS_VERSION_2:</span><br><span class="line">                <span class="keyword">return</span> self.db.zadd(REDIS_KEY, score, proxy.string())</span><br><span class="line">            <span class="keyword">return</span> self.db.zadd(REDIS_KEY, &#123;proxy.string(): score&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">random</span><span class="params">(self)</span> -&gt; Proxy:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        get random proxy</span></span><br><span class="line"><span class="string">        firstly try to get proxy with max score</span></span><br><span class="line"><span class="string">        if not exists, try to get proxy by rank</span></span><br><span class="line"><span class="string">        if not exists, raise error</span></span><br><span class="line"><span class="string">        :return: proxy, like 8.8.8.8:8</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># try to get proxy with max score</span></span><br><span class="line">        proxies = self.db.zrangebyscore(REDIS_KEY, PROXY_SCORE_MAX, PROXY_SCORE_MAX)</span><br><span class="line">        <span class="keyword">if</span> len(proxies):</span><br><span class="line">            <span class="keyword">return</span> convert_proxy_or_proxies(choice(proxies))</span><br><span class="line">        <span class="comment"># else get proxy by rank</span></span><br><span class="line">        proxies = self.db.zrevrange(REDIS_KEY, PROXY_SCORE_MIN, PROXY_SCORE_MAX)</span><br><span class="line">        <span class="keyword">if</span> len(proxies):</span><br><span class="line">            <span class="keyword">return</span> convert_proxy_or_proxies(choice(proxies))</span><br><span class="line">        <span class="comment"># else raise error</span></span><br><span class="line">        <span class="keyword">raise</span> PoolEmptyException</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decrease</span><span class="params">(self, proxy: Proxy)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        decrease score of proxy, if small than PROXY_SCORE_MIN, delete it</span></span><br><span class="line"><span class="string">        :param proxy: proxy</span></span><br><span class="line"><span class="string">        :return: new score</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> IS_REDIS_VERSION_2:</span><br><span class="line">            self.db.zincrby(REDIS_KEY, proxy.string(), <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.db.zincrby(REDIS_KEY, <span class="number">-1</span>, proxy.string())</span><br><span class="line">        score = self.db.zscore(REDIS_KEY, proxy.string())</span><br><span class="line">        <span class="comment">#相当于"&#123;&#125;".format()</span></span><br><span class="line">        logger.info(<span class="string">f'<span class="subst">&#123;proxy.string()&#125;</span> score decrease 1, current <span class="subst">&#123;score&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> score &lt;= PROXY_SCORE_MIN:</span><br><span class="line">            logger.info(<span class="string">f'<span class="subst">&#123;proxy.string()&#125;</span> current score <span class="subst">&#123;score&#125;</span>, remove'</span>)</span><br><span class="line">            self.db.zrem(REDIS_KEY, proxy.string())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exists</span><span class="params">(self, proxy: Proxy)</span> -&gt; bool:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        if proxy exists</span></span><br><span class="line"><span class="string">        :param proxy: proxy</span></span><br><span class="line"><span class="string">        :return: if exists, bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> self.db.zscore(REDIS_KEY, proxy.string()) <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max</span><span class="params">(self, proxy: Proxy)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        set proxy to max score</span></span><br><span class="line"><span class="string">        :param proxy: proxy</span></span><br><span class="line"><span class="string">        :return: new score</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        logger.info(<span class="string">f'<span class="subst">&#123;proxy.string()&#125;</span> is valid, set to <span class="subst">&#123;PROXY_SCORE_MAX&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> IS_REDIS_VERSION_2:</span><br><span class="line">            <span class="keyword">return</span> self.db.zadd(REDIS_KEY, PROXY_SCORE_MAX, proxy.string())</span><br><span class="line">        <span class="keyword">return</span> self.db.zadd(REDIS_KEY, &#123;proxy.string(): PROXY_SCORE_MAX&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(self)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        get count of proxies</span></span><br><span class="line"><span class="string">        :return: count, int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.zcard(REDIS_KEY)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">all</span><span class="params">(self)</span> -&gt; List[Proxy]:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        get all proxies</span></span><br><span class="line"><span class="string">        :return: list of proxies</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> convert_proxy_or_proxies(self.db.zrangebyscore(REDIS_KEY, PROXY_SCORE_MIN, PROXY_SCORE_MAX))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">batch</span><span class="params">(self, cursor, count)</span> -&gt; List[Proxy]:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        get batch of proxies</span></span><br><span class="line"><span class="string">        :param cursor: scan cursor</span></span><br><span class="line"><span class="string">        :param count: scan count</span></span><br><span class="line"><span class="string">        :return: list of proxies</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cursor, proxies = self.db.zscan(REDIS_KEY, cursor, count=count)</span><br><span class="line">        <span class="keyword">return</span> cursor, convert_proxy_or_proxies([i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> proxies])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    conn = RedisClient()</span><br><span class="line">    result = conn.random()</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>获取模块<br>获取模块定时从代理网站获取代理，将获取的代理传递给存储模块，并保存到数据库。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># base.py</span></span><br><span class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> GET_TIMEOUT</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseCrawler</span><span class="params">(object)</span>:</span></span><br><span class="line">    urls = []</span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    retry装饰器：</span></span><br><span class="line"><span class="string">    stop_max_attempt_number：在停止之前尝试的最大次数，最后一次如果还是有异常则会抛出异常，停止运行，默认为5次</span></span><br><span class="line"><span class="string">    wait_random_min：在两次调用方法停留时长，停留最短时间，默认为0,单位毫秒</span></span><br><span class="line"><span class="string">    wait_random_max：在两次调用方法停留时长，停留最长时间，默认为1000毫秒</span></span><br><span class="line"><span class="string">    wait_fixed：设置在两次retrying之间的停留时间</span></span><br><span class="line"><span class="string">    stop_max_delay:从被装饰的函数开始执行的时间点开始到函数成功运行结束或失败报错中止的时间点。单位：毫秒</span></span><br><span class="line"><span class="string">    retry_on_result：指定一个函数，如果指定的函数返回True，则重试，否则抛出异常退出</span></span><br><span class="line"><span class="string">    retry_on_exception: 指定一个函数，如果此函数返回指定异常，则会重试，如果不是指定的异常则会退出</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"><span class="meta">    @retry(stop_max_attempt_number=3, retry_on_result=lambda x: x is None, wait_fixed=2000)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fetch</span><span class="params">(self, url, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取html源码</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            kwargs.setdefault(<span class="string">'timeout'</span>, GET_TIMEOUT)</span><br><span class="line">            kwargs.setdefault(<span class="string">'verify'</span>, <span class="literal">False</span>)</span><br><span class="line">            response = requests.get(url, **kwargs)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                response.encoding = <span class="string">'utf-8'</span></span><br><span class="line">                <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @logger.catch</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        crawl main method</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.urls:</span><br><span class="line">            logger.info(<span class="string">f'fetching <span class="subst">&#123;url&#125;</span>'</span>)</span><br><span class="line">            html = self.fetch(url)</span><br><span class="line">            <span class="keyword">for</span> proxy <span class="keyword">in</span> self.parse(html):</span><br><span class="line">                logger.info(<span class="string">f'fetched proxy <span class="subst">&#123;proxy.string()&#125;</span> from <span class="subst">&#123;url&#125;</span>'</span>)</span><br><span class="line">                <span class="keyword">yield</span> proxy</span><br><span class="line"></span><br><span class="line"><span class="comment"># daili66.py</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"><span class="keyword">from</span> proxypool.schemas.proxy <span class="keyword">import</span> Proxy</span><br><span class="line"><span class="keyword">from</span> proxypool.crawlers.base <span class="keyword">import</span> BaseCrawler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">'http://www.66ip.cn/&#123;page&#125;.html'</span></span><br><span class="line">MAX_PAGE = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 继承BaseCrawler</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Daili66Crawler</span><span class="params">(BaseCrawler)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    daili66 crawler, http://www.66ip.cn/1.html</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    urls = [BASE_URL.format(page=page) <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, MAX_PAGE + <span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, html)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        parse html file to get proxies</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        doc = PyQuery(html)</span><br><span class="line">        trs = doc(<span class="string">'.containerbox table tr:gt(0)'</span>).items()</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">            host = tr.find(<span class="string">'td:nth-child(1)'</span>).text()</span><br><span class="line">            port = int(tr.find(<span class="string">'td:nth-child(2)'</span>).text())</span><br><span class="line">            <span class="keyword">yield</span> Proxy(host=host, port=port)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    crawler = Daili66Crawler()</span><br><span class="line">    <span class="keyword">for</span> proxy <span class="keyword">in</span> crawler.crawl():</span><br><span class="line">        print(proxy)</span><br></pre></td></tr></table></figure></li><li><p>检测模块<br>检测模块定时通过存储模块获取所有代理，并对代理进行检测，根据不同的检测结果对代理设置不同的标识。<br>这里使用异步请求库aiohttp来进行检测。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">from</span> proxypool.schemas <span class="keyword">import</span> Proxy</span><br><span class="line"><span class="keyword">from</span> proxypool.storages.redis <span class="keyword">import</span> RedisClient</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> TEST_TIMEOUT, TEST_BATCH, TEST_URL, TEST_VALID_STATUS, TEST_ANONYMOUS</span><br><span class="line"><span class="keyword">from</span> aiohttp <span class="keyword">import</span> ClientProxyConnectionError, ServerDisconnectedError, ClientOSError, ClientHttpProxyError</span><br><span class="line"><span class="keyword">from</span> asyncio <span class="keyword">import</span> TimeoutError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EXCEPTIONS = (</span><br><span class="line">    ClientProxyConnectionError,</span><br><span class="line">    ConnectionRefusedError,</span><br><span class="line">    TimeoutError,</span><br><span class="line">    ServerDisconnectedError,</span><br><span class="line">    ClientOSError,</span><br><span class="line">    ClientHttpProxyError,</span><br><span class="line">    AssertionError</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tester</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    tester for testing proxies in queue</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        init redis</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.redis = RedisClient()</span><br><span class="line">        self.loop = asyncio.get_event_loop()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, proxy: Proxy)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        test single proxy</span></span><br><span class="line"><span class="string">        :param proxy: Proxy object</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=<span class="literal">False</span>)) <span class="keyword">as</span> session:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                logger.debug(<span class="string">f'testing <span class="subst">&#123;proxy.string()&#125;</span>'</span>)</span><br><span class="line">                <span class="comment"># if TEST_ANONYMOUS（匿名） is True, make sure that</span></span><br><span class="line">                <span class="comment"># the proxy has the effect of hiding the real IP</span></span><br><span class="line">                <span class="keyword">if</span> TEST_ANONYMOUS:</span><br><span class="line">                    url = <span class="string">'https://httpbin.org/ip'</span></span><br><span class="line">                    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url, timeout=TEST_TIMEOUT) <span class="keyword">as</span> response:</span><br><span class="line">                        resp_json = <span class="keyword">await</span> response.json()</span><br><span class="line">                        origin_ip = resp_json[<span class="string">'origin'</span>]</span><br><span class="line">                    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url, proxy=<span class="string">f'http://<span class="subst">&#123;proxy.string()&#125;</span>'</span>, timeout=TEST_TIMEOUT) <span class="keyword">as</span> response:</span><br><span class="line">                        resp_json = <span class="keyword">await</span> response.json()</span><br><span class="line">                        anonymous_ip = resp_json[<span class="string">'origin'</span>]</span><br><span class="line">                    <span class="keyword">assert</span> origin_ip != anonymous_ip</span><br><span class="line">                    <span class="keyword">assert</span> proxy.host == anonymous_ip</span><br><span class="line">                <span class="keyword">async</span> <span class="keyword">with</span> session.get(TEST_URL, proxy=<span class="string">f'http://<span class="subst">&#123;proxy.string()&#125;</span>'</span>, timeout=TEST_TIMEOUT,</span><br><span class="line">                                       allow_redirects=<span class="literal">False</span>) <span class="keyword">as</span> response:</span><br><span class="line">                    <span class="keyword">if</span> response.status <span class="keyword">in</span> TEST_VALID_STATUS:</span><br><span class="line">                        self.redis.max(proxy)</span><br><span class="line">                        logger.debug(<span class="string">f'proxy <span class="subst">&#123;proxy.string()&#125;</span> is valid, set max score'</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.redis.decrease(proxy)</span><br><span class="line">                        logger.debug(<span class="string">f'proxy <span class="subst">&#123;proxy.string()&#125;</span> is invalid, decrease score'</span>)</span><br><span class="line">            <span class="keyword">except</span> EXCEPTIONS:</span><br><span class="line">                self.redis.decrease(proxy)</span><br><span class="line">                logger.debug(<span class="string">f'proxy <span class="subst">&#123;proxy.string()&#125;</span> is invalid, decrease score'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @logger.catch</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        test main method</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># event loop of aiohttp</span></span><br><span class="line">        logger.info(<span class="string">'stating tester...'</span>)</span><br><span class="line">        count = self.redis.count()</span><br><span class="line">        logger.debug(<span class="string">f'<span class="subst">&#123;count&#125;</span> proxies to test'</span>)</span><br><span class="line">        cursor = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            logger.debug(<span class="string">f'testing proxies use cursor <span class="subst">&#123;cursor&#125;</span>, count <span class="subst">&#123;TEST_BATCH&#125;</span>'</span>)</span><br><span class="line">            cursor, proxies = self.redis.batch(cursor, count=TEST_BATCH)</span><br><span class="line">            <span class="keyword">if</span> proxies:</span><br><span class="line">                tasks = [self.test(proxy) <span class="keyword">for</span> proxy <span class="keyword">in</span> proxies]</span><br><span class="line">                self.loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cursor:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tester = Tester()</span><br><span class="line">    tester.run()</span><br></pre></td></tr></table></figure></li><li><p>接口模块<br>接口模块通过WebAPI提供服务接口，接口通过连接数据库并通过Web形式返回可用的代理。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, g</span><br><span class="line"><span class="keyword">from</span> proxypool.storages.redis <span class="keyword">import</span> RedisClient</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> API_HOST, API_PORT, API_THREADED</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__all__ = [<span class="string">'app'</span>]</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get redis client object</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(g, <span class="string">'redis'</span>):</span><br><span class="line">        g.redis = RedisClient()</span><br><span class="line">    <span class="keyword">return</span> g.redis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get home page, you can define your own templates</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'&lt;h2&gt;Welcome to Proxy Pool System&lt;/h2&gt;'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/random')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get a random proxy</span></span><br><span class="line"><span class="string">    :return: get a random proxy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    conn = get_conn()</span><br><span class="line">    <span class="keyword">return</span> conn.random().string()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/count')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_count</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get the count of proxies</span></span><br><span class="line"><span class="string">    :return: count, int</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    conn = get_conn()</span><br><span class="line">    <span class="keyword">return</span> str(conn.count())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(host=API_HOST, port=API_PORT, threaded=API_THREADED)</span><br></pre></td></tr></table></figure></li></ul><h4 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pipenv shell</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全部启动</span></span><br><span class="line">python run.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按需启动</span></span><br><span class="line">python run.py --processor getter</span><br><span class="line">python run.py --processor tester</span><br><span class="line">python run.py --processor server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 成功运行之后可以通过 http://localhost:5555/random 获取一个随机可用代理</span></span><br></pre></td></tr></table></figure><p>程序对接实现:</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxypool_url = <span class="string">'http://127.0.0.1:5555/random'</span></span><br><span class="line">target_url = <span class="string">'http://httpbin.org/get'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get random proxy from proxypool</span></span><br><span class="line"><span class="string">    :return: proxy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> requests.get(proxypool_url).text.strip()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(url, proxy)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    use proxy to crawl page</span></span><br><span class="line"><span class="string">    :param url: page url</span></span><br><span class="line"><span class="string">    :param proxy: proxy, such as 8.8.8.8:8888</span></span><br><span class="line"><span class="string">    :return: html</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    proxies = &#123;<span class="string">'http'</span>: <span class="string">'http://'</span> + proxy&#125;</span><br><span class="line">    <span class="keyword">return</span> requests.get(url, proxies=proxies).text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    main method, entry point</span></span><br><span class="line"><span class="string">    :return: none</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    proxy = get_random_proxy()</span><br><span class="line">    print(<span class="string">'get random proxy'</span>, proxy)</span><br><span class="line">    html = crawl(target_url, proxy)</span><br><span class="line">    print(html)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="付费代理的使用"><a href="#付费代理的使用" class="headerlink" title="付费代理的使用"></a>付费代理的使用</h3><h3 id="ADSL拨号代理"><a href="#ADSL拨号代理" class="headerlink" title="ADSL拨号代理"></a>ADSL拨号代理</h3><p>ADSL(Asymmetric Digital Subscriber Line，非对称数字用户环路），它的上行和下行带宽不对称，采用频分复用技术把普通的电话线分成了电话、上行和下行3个相对独立的信道，从而避免了相互之间的干扰。<br>ADSL通过拨号的方式上网，需要输入ADSL账号和密码，每次拨号就更换一个IP。IP分布在多个A段，如果IP都能使用，则意味着E量级可达千万。如果我们将ADSL主机作为代理，每隔一段时间主机拨号就换一个IP，这样可以有效防止IP被封禁。另外，主机的稳定性很好，代理响应速度很快。<br>参考项目：<a href="https://github.com/Python3WebSpider/AdslProxy" target="_blank" rel="noopener">https://github.com/Python3WebSpider/AdslProxy</a></p><ul><li>准备工作</li><li>设置代理服务器</li><li>动态获取IP</li><li>存储模块</li><li>拨号模块</li><li>接口模块</li></ul><h3 id="使用代理爬取微信公众号文章"><a href="#使用代理爬取微信公众号文章" class="headerlink" title="使用代理爬取微信公众号文章"></a>使用代理爬取微信公众号文章</h3><p>参考项目：<a href="https://github.com/Python3WebSpider/Weixin" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Weixin</a></p><ul><li>主要实现功能：<ul><li>修改代理池检测链接为搜狗微信站点。</li><li>构造Redis爬取队列，用队列实现请求的存取。</li><li>实现异常处理，失败的请求重新加入队列 。</li><li>实现翻页和提取文章列表，并把对应请求加入队列。</li><li>实现微信文章的信息的提取。</li><li>将提取到的信息保存到MySQL。</li></ul></li><li>构造请求<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> weixin.config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeixinRequest</span><span class="params">(Request)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url, callback, method=<span class="string">'GET'</span>, headers=None, need_proxy=False, fail_time=<span class="number">0</span>, timeout=TIMEOUT)</span>:</span></span><br><span class="line">        <span class="comment"># 调用父类Request的__init__()方法</span></span><br><span class="line">        Request.__init__(self, method, url, headers)</span><br><span class="line">        <span class="comment"># 添加需要的额外属性</span></span><br><span class="line">        self.callback = callback</span><br><span class="line">        self.need_proxy = need_proxy</span><br><span class="line">        self.fail_time = fail_time</span><br><span class="line">        self.timeout = timeout</span><br></pre></td></tr></table></figure></li><li>实现请求队列<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> StrictRedis</span><br><span class="line"><span class="keyword">from</span> weixin.config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pickle <span class="keyword">import</span> dumps, loads</span><br><span class="line"><span class="keyword">from</span> weixin.request <span class="keyword">import</span> WeixinRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisQueue</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化Redis</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.db = StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        向队列添加序列化后的Request</span></span><br><span class="line"><span class="string">        :param request: 请求对象</span></span><br><span class="line"><span class="string">        :param fail_time: 失败次数</span></span><br><span class="line"><span class="string">        :return: 添加结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(request, WeixinRequest):</span><br><span class="line">            <span class="keyword">return</span> self.db.rpush(REDIS_KEY, dumps(request))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        取出下一个Request并反序列化</span></span><br><span class="line"><span class="string">        :return: Request or None</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.db.llen(REDIS_KEY):</span><br><span class="line">            <span class="keyword">return</span> loads(self.db.lpop(REDIS_KEY))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.db.delete(REDIS_KEY)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">empty</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.db.llen(REDIS_KEY) == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    db = RedisQueue()</span><br><span class="line">    start_url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    weixin_request = WeixinRequest(url=start_url, callback=<span class="string">'hello'</span>, need_proxy=<span class="literal">True</span>)</span><br><span class="line">    db.add(weixin_request)</span><br><span class="line">    request = db.pop()</span><br><span class="line">    print(request)</span><br><span class="line">    print(request.callback, request.need_proxy)</span><br></pre></td></tr></table></figure></li><li>MySQL存储<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> weixin.config <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host=MYSQL_HOST, username=MYSQL_USER, password=MYSQL_PASSWORD, port=MYSQL_PORT,</span></span></span><br><span class="line"><span class="function"><span class="params">                 database=MYSQL_DATABASE)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        MySQL初始化</span></span><br><span class="line"><span class="string">        :param host:</span></span><br><span class="line"><span class="string">        :param username:</span></span><br><span class="line"><span class="string">        :param password:</span></span><br><span class="line"><span class="string">        :param port:</span></span><br><span class="line"><span class="string">        :param database:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.db = pymysql.connect(host, username, password, database, charset=<span class="string">'utf8'</span>, port=port)</span><br><span class="line">            self.cursor = self.db.cursor()</span><br><span class="line">        <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, table, data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        插入数据</span></span><br><span class="line"><span class="string">        :param table:</span></span><br><span class="line"><span class="string">        :param data:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">        values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">        sql_query = <span class="string">'insert into %s (%s) values (%s)'</span> % (table, keys, values)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.cursor.execute(sql_query, tuple(data.values()))</span><br><span class="line">            self.db.commit()</span><br><span class="line">        <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line">            self.db.rollback()</span><br></pre></td></tr></table></figure></li><li>修改代理池<br>将测试代理的网址替换为目标爬取网址。筛选出可用代理</li><li>调度请求（主方法）<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Session</span><br><span class="line"><span class="keyword">from</span> weixin.config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> weixin.db <span class="keyword">import</span> RedisQueue</span><br><span class="line"><span class="keyword">from</span> weixin.mysql <span class="keyword">import</span> MySQL</span><br><span class="line"><span class="keyword">from</span> weixin.request <span class="keyword">import</span> WeixinRequest</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> ReadTimeout, ConnectionError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider</span><span class="params">()</span>:</span></span><br><span class="line">    base_url = <span class="string">'http://weixin.sogou.com/weixin'</span></span><br><span class="line">    keyword = <span class="string">'NBA'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'</span>,</span><br><span class="line">        <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate'</span>,</span><br><span class="line">        <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8,en;q=0.6,ja;q=0.4,zh-TW;q=0.2,mt;q=0.2'</span>,</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">'max-age=0'</span>,</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</span><br><span class="line">        <span class="string">'Cookie'</span>: <span class="string">'IPLOC=CN1100; SUID=6FEDCF3C541C940A000000005968CF55; SUV=1500041046435211; ABTEST=0|1500041048|v1; SNUID=CEA85AE02A2F7E6EAFF9C1FE2ABEBE6F; weixinIndexVisited=1; JSESSIONID=aaar_m7LEIW-jg_gikPZv; ld=Wkllllllll2BzGMVlllllVOo8cUlllll5G@HbZllll9lllllRklll5@@@@@@@@@@; LSTMV=212%2C350; LCLKINT=4650; ppinf=5|1500042908|1501252508|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZTo1NDolRTUlQjQlOTQlRTUlQkElODYlRTYlODklOEQlRTQlQjglQTglRTklOUQlOTklRTglQTclODV8Y3J0OjEwOjE1MDAwNDI5MDh8cmVmbmljazo1NDolRTUlQjQlOTQlRTUlQkElODYlRTYlODklOEQlRTQlQjglQTglRTklOUQlOTklRTglQTclODV8dXNlcmlkOjQ0Om85dDJsdUJfZWVYOGRqSjRKN0xhNlBta0RJODRAd2VpeGluLnNvaHUuY29tfA; pprdig=ppyIobo4mP_ZElYXXmRTeo2q9iFgeoQ87PshihQfB2nvgsCz4FdOf-kirUuntLHKTQbgRuXdwQWT6qW-CY_ax5VDgDEdeZR7I2eIDprve43ou5ZvR0tDBlqrPNJvC0yGhQ2dZI3RqOQ3y1VialHsFnmTiHTv7TWxjliTSZJI_Bc; sgid=27-27790591-AVlo1pzPiad6EVQdGDbmwnvM; PHPSESSID=mkp3erf0uqe9ugjg8os7v1e957; SUIR=CEA85AE02A2F7E6EAFF9C1FE2ABEBE6F; sct=11; ppmdig=1500046378000000b7527c423df68abb627d67a0666fdcee; successCount=1|Fri, 14 Jul 2017 15:38:07 GMT'</span>,</span><br><span class="line">        <span class="string">'Host'</span>: <span class="string">'weixin.sogou.com'</span>,</span><br><span class="line">        <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    session = Session()</span><br><span class="line">    queue = RedisQueue()</span><br><span class="line">    mysql = MySQL()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        从代理池获取代理</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response = requests.get(PROXY_POOL_URL)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                print(<span class="string">'Get Proxy'</span>, response.text)</span><br><span class="line">                <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化工作</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 全局更新Headers</span></span><br><span class="line">        self.session.headers.update(self.headers)</span><br><span class="line">        start_url = self.base_url + <span class="string">'?'</span> + urlencode(&#123;<span class="string">'query'</span>: self.keyword, <span class="string">'type'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">        weixin_request = WeixinRequest(url=start_url, callback=self.parse_index, need_proxy=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 调度第一个请求</span></span><br><span class="line">        self.queue.add(weixin_request)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        解析索引页</span></span><br><span class="line"><span class="string">        :param response: 响应</span></span><br><span class="line"><span class="string">        :return: 新的响应</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        doc = pq(response.text)</span><br><span class="line">        items = doc(<span class="string">'.news-box .news-list li .txt-box h3 a'</span>).items()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            url = item.attr(<span class="string">'href'</span>)</span><br><span class="line">            weixin_request = WeixinRequest(url=url, callback=self.parse_detail)</span><br><span class="line">            <span class="keyword">yield</span> weixin_request</span><br><span class="line">        next = doc(<span class="string">'#sogou_next'</span>).attr(<span class="string">'href'</span>)</span><br><span class="line">        <span class="keyword">if</span> next:</span><br><span class="line">            url = self.base_url + str(next)</span><br><span class="line">            weixin_request = WeixinRequest(url=url, callback=self.parse_index, need_proxy=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">yield</span> weixin_request</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        解析详情页</span></span><br><span class="line"><span class="string">        :param response: 响应</span></span><br><span class="line"><span class="string">        :return: 微信公众号文章</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        doc = pq(response.text)</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'title'</span>: doc(<span class="string">'.rich_media_title'</span>).text(),</span><br><span class="line">            <span class="string">'content'</span>: doc(<span class="string">'.rich_media_content'</span>).text(),</span><br><span class="line">            <span class="string">'date'</span>: doc(<span class="string">'#post-date'</span>).text(),</span><br><span class="line">            <span class="string">'nickname'</span>: doc(<span class="string">'#js_profile_qrcode &gt; div &gt; strong'</span>).text(),</span><br><span class="line">            <span class="string">'wechat'</span>: doc(<span class="string">'#js_profile_qrcode &gt; div &gt; p:nth-child(3) &gt; span'</span>).text()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">yield</span> data</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">(self, weixin_request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行请求</span></span><br><span class="line"><span class="string">        :param weixin_request: 请求</span></span><br><span class="line"><span class="string">        :return: 响应</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> weixin_request.need_proxy:</span><br><span class="line">                proxy = self.get_proxy()</span><br><span class="line">                <span class="keyword">if</span> proxy:</span><br><span class="line">                    proxies = &#123;</span><br><span class="line">                        <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">                        <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">return</span> self.session.send(weixin_request.prepare(),</span><br><span class="line">                                             timeout=weixin_request.timeout, allow_redirects=<span class="literal">False</span>, proxies=proxies)</span><br><span class="line">            <span class="keyword">return</span> self.session.send(weixin_request.prepare(), timeout=weixin_request.timeout, allow_redirects=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">except</span> (ConnectionError, ReadTimeout) <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(self, weixin_request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        错误处理</span></span><br><span class="line"><span class="string">        :param weixin_request: 请求</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        weixin_request.fail_time = weixin_request.fail_time + <span class="number">1</span></span><br><span class="line">        print(<span class="string">'Request Failed'</span>, weixin_request.fail_time, <span class="string">'Times'</span>, weixin_request.url)</span><br><span class="line">        <span class="keyword">if</span> weixin_request.fail_time &lt; MAX_FAILED_TIME:</span><br><span class="line">            self.queue.add(weixin_request)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">schedule</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        调度请求</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> self.queue.empty():</span><br><span class="line">            weixin_request = self.queue.pop()</span><br><span class="line">            callback = weixin_request.callback</span><br><span class="line">            print(<span class="string">'Schedule'</span>, weixin_request.url)</span><br><span class="line">            response = self.request(weixin_request)</span><br><span class="line">            <span class="keyword">if</span> response <span class="keyword">and</span> response.status_code <span class="keyword">in</span> VALID_STATUSES:</span><br><span class="line">                results = list(callback(response))</span><br><span class="line">                <span class="keyword">if</span> results:</span><br><span class="line">                    <span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">                        print(<span class="string">'New Result'</span>, type(result))</span><br><span class="line">                        <span class="keyword">if</span> isinstance(result, WeixinRequest):</span><br><span class="line">                            self.queue.add(result)</span><br><span class="line">                        <span class="keyword">if</span> isinstance(result, dict):</span><br><span class="line">                            self.mysql.insert(<span class="string">'articles'</span>, result)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.error(weixin_request)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.error(weixin_request)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        入口</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.start()</span><br><span class="line">        self.schedule()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = Spider()</span><br><span class="line">    spider.run()</span><br></pre></td></tr></table></figure></li><li>运行</li></ul><h2 id="Ch-9-模拟登录"><a href="#Ch-9-模拟登录" class="headerlink" title="Ch 9 模拟登录"></a>Ch 9 模拟登录</h2><h3 id="模拟登录并爬取GitHub"><a href="#模拟登录并爬取GitHub" class="headerlink" title="模拟登录并爬取GitHub"></a>模拟登录并爬取GitHub</h3><h3 id="Cookie池的搭建"><a href="#Cookie池的搭建" class="headerlink" title="Cookie池的搭建"></a>Cookie池的搭建</h3><h2 id="Ch-10-APP的爬取"><a href="#Ch-10-APP的爬取" class="headerlink" title="Ch 10 APP的爬取"></a>Ch 10 APP的爬取</h2><h3 id="Charles的使用"><a href="#Charles的使用" class="headerlink" title="Charles的使用"></a>Charles的使用</h3><h3 id="mitmproxy的使用"><a href="#mitmproxy的使用" class="headerlink" title="mitmproxy的使用"></a>mitmproxy的使用</h3><h3 id="mitmdump爬取“得到”电子书信息"><a href="#mitmdump爬取“得到”电子书信息" class="headerlink" title="mitmdump爬取“得到”电子书信息"></a>mitmdump爬取“得到”电子书信息</h3><h3 id="APPium的基本使用"><a href="#APPium的基本使用" class="headerlink" title="APPium的基本使用"></a>APPium的基本使用</h3><h3 id="APPium爬取微信朋友圈"><a href="#APPium爬取微信朋友圈" class="headerlink" title="APPium爬取微信朋友圈"></a>APPium爬取微信朋友圈</h3><h3 id="APPium-mitmdump爬取京东商品"><a href="#APPium-mitmdump爬取京东商品" class="headerlink" title="APPium+mitmdump爬取京东商品"></a>APPium+mitmdump爬取京东商品</h3><h2 id="Ch-11-pyspider框架使用"><a href="#Ch-11-pyspider框架使用" class="headerlink" title="Ch 11 pyspider框架使用"></a>Ch 11 pyspider框架使用</h2><p><span style="border-bottom:2px solid red">pyspider框架应用场景较为单一，且可扩展程度不足，比较适合爬取固定分页展示内容的数据。</span></p><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><ul><li>pyspider是由国人binux编写的强大的网络爬虫系统，其GitHub地址为<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a>，官方文档地址为<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a>。</li><li>pyspider带有强大的 WebUI、脚本编辑器、任务监控器、项目管理器以及结果处理器，它支持多种数据库后端、多种消息队列、JavaScript渲染页面的爬取，使用起来非常方便。</li><li>pyspider开发快速便捷，适合中小型项目。</li></ul><h4 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h4><ul><li>提供方便易用的WebUI系统，可视化地编写和调试爬虫。</li><li>提供爬取进度监控、爬取结果查看、爬虫项目管理等功能。</li><li>支持多种后端数据库，如MySQL、MongoDB、Redis、SQLite、Elasticsearch、PostgreSQL。</li><li>支持多种消息队列，如RabbitMQ、Beanstalk、Redis、Kombu。</li><li>提供优先级控制、失败重试、定时抓取等功能。</li><li>对接了PhantomJS，可以抓取JavaScript渲染的页面。</li><li>支持单机和分布式部署，支持Docker部署。</li></ul><h4 id="与Scrapy比较"><a href="#与Scrapy比较" class="headerlink" title="与Scrapy比较"></a>与Scrapy比较</h4><ul><li>pyspider提供了WebUI，爬虫的编写、调试都是在WebUI中进行的。而Scrapy原生是不具备这个功能的，它采用的是代码和命令行操作，但可以通过对接Portia实现可视化配置。</li><li>pyspider调试非常方便，WebUI操作便捷直观。Scrapy则是使用parse命令进行调试，不够方便。</li><li>pyspider支持PhantomJS来进行JavaScript谊染页面的采集。Scrapy可以对接Scrapy-Splash组件，这需要额外配置。</li><li><span style="border-bottom:2px solid red">pyspider中内置了pyquery作为选择器</span>。Scrapy对接了XPath、css选择器和正则匹配。</li><li>pyspider的可扩展程度不足，可配制化程度不高。Scrapy可以通过对接Middleware、Pipeline、Extension等组件实现非常强大的功能，模块之间的耦合程度低，可扩展程度极高。</li></ul><h4 id="pyspider架构"><a href="#pyspider架构" class="headerlink" title="pyspider架构"></a>pyspider架构</h4><p>pyspider的架构主要分为Scheduler（调度器）、Fetcher（ 抓取器）、Processer（处理器）三个部分，整个爬取过程受到Monitor（监控器）的监控，抓取的结果被Result Worker（结果处理器）处理，如图：<br><img src="https://s3.ax1x.com/2021/01/17/sse47d.png" alt="pyspider 架构图"><br>Scheduler发起任务调度，Fetcher负责抓取网页内容，Processer负责解析网页内容，然后将新生成的Request发给Scheduler进行调度，将生成的提取结果输出保存。<br>执行过程如下：</p><ul><li>每个pyspider的项目对应一个Python脚本，该脚本中定义了一个Handler类，它有一个on_start()方法。爬取首先调用on_start()方法生成最初的抓取任务，然后发送给Scheduler进行调度。</li><li>Scheduler将抓取任务分发给Fetcher进行抓取，Fetcher执行并得到响应，随后将响应发送给Processer。</li><li>Processer处理响应并提取出新的URL生成新的抓取任务，然后通过消息队列的方式通知Schduler当前抓取任务执行情况，并将新生成的抓取任务发送给 Scheduler。如果生成了新的提取结果，则将其发送到结果队列等待Result Worker处理。</li><li>Scheduler接收到新的抓取任务，然后查询数据库，判断其如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其发送回Fetcher进行抓取。</li><li>不断重复以上工作，直到所有的任务都执行完毕，抓取结束。</li><li>抓取结束后，程序会回调on_finished()方法，这里可以定义后处理过程。</li></ul><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h4><ul><li>官方文档：<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a></li><li>PyPI：<a href="https://pypi.python.org/pypi/pyspider" target="_blank" rel="noopener">https://pypi.python.org/pypi/pyspider</a></li><li>GitHub：<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a></li><li>官方教程：<a href="http://docs.pyspider.org/en/latest/tutorial" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/tutorial</a></li><li>在线实例：<a href="http://demo.pyspider.org" target="_blank" rel="noopener">http://demo.pyspider.org</a></li></ul><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ul><li><code>pip3 install pyspider</code></li><li>PhantomJS安装：<a href="https://gitee.com/xxyrs/filehouse/raw/master/phantomjs-2.1.1-windows.zip" target="_blank" rel="noopener">phantomjs-2.1.1-windows.zip</a>，解压后将bin路径配置到用户Path中</li><li>常见错误及解决：<br><a href="https://www.cnblogs.com/Mayfly-nymph/p/10808088.html" target="_blank" rel="noopener">https://www.cnblogs.com/Mayfly-nymph/p/10808088.html</a><br><a href="https://github.com/binux/pyspider/issues/898" target="_blank" rel="noopener">https://github.com/binux/pyspider/issues/898</a><br><a href="https://www.cnblogs.com/shaosks/p/6856086.html" target="_blank" rel="noopener">https://www.cnblogs.com/shaosks/p/6856086.html</a></li><li>启动：<code>pyspider [all]</code></li></ul><h4 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspider.libs.base_handler <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config = &#123;</span><br><span class="line">        <span class="string">'itag'</span>: <span class="string">'v223'</span>,</span><br><span class="line">        <span class="string">"headers"</span>:&#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: UserAgent().random,</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @every(minutes=24 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.crawl(<span class="string">'http://travel.qunar.com/travelbook/list.htm'</span>, callback=self.index_page, fetch_type=<span class="string">'js'</span>, validate_cert=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @config(age=10 * 24 * 60 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.doc(<span class="string">'li &gt; .tit &gt; a'</span>).items():</span><br><span class="line">            <span class="comment">#fetch_type='js'，使用PhantomJS渲染</span></span><br><span class="line">            self.crawl(each.attr.href, callback=self.detail_page, validate_cert=<span class="literal">False</span>, fetch_type=<span class="string">'js'</span>)</span><br><span class="line">        <span class="comment">#获取下一页链接</span></span><br><span class="line">        next = response.doc(<span class="string">'.next'</span>).attr.href</span><br><span class="line">        self.crawl(next, callback=self.index_page, validate_cert=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @config(priority=2)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'url'</span>: response.url,</span><br><span class="line">            <span class="string">'title'</span>: response.doc(<span class="string">'#booktitle'</span>).text(),</span><br><span class="line">            <span class="string">'date'</span>: response.doc(<span class="string">'.when .data'</span>).text(),</span><br><span class="line">            <span class="string">'day'</span>: response.doc(<span class="string">'.howlong .data'</span>).text(),</span><br><span class="line">            <span class="string">'who'</span>: response.doc(<span class="string">'.who .data'</span>).text(),</span><br><span class="line">            <span class="string">'text'</span>: response.doc(<span class="string">'#b_panel_schedule'</span>).text(),</span><br><span class="line">            <span class="comment"># "btall": [(x.find('a').text(), x.find('a').eq(1).attr.href[0:59]) for x in response.doc('.dlist li').items()],</span></span><br><span class="line">            <span class="string">'image'</span>: response.doc(<span class="string">'.cover_img'</span>).attr.src</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h4 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h4><p><img src="https://s3.ax1x.com/2021/01/18/sy04PI.png" alt="新建项目"></p><h4 id="爬取首页"><a href="#爬取首页" class="headerlink" title="爬取首页"></a>爬取首页</h4><p><img src="https://s3.ax1x.com/2021/01/18/sywvjO.png" alt="爬取首页"></p><h4 id="爬取详情页"><a href="#爬取详情页" class="headerlink" title="爬取详情页"></a>爬取详情页</h4><p><img src="https://s3.ax1x.com/2021/01/18/sysBwD.png" alt="爬取详情页"><br>爬取的页面无法显示图片，出现此现象的原因是pyspider默认发送HTTP请求，请求的HTML文档本身就不包含img节点。但是在浏览器中我们看到了图片，这是因为这张图片是后期经过JavaScript出现的。<br>pyspider内部对接了PhantomJS，将index_page()中生成抓取详情页的请求方法添加一个参数 <code>fetch_type=&#39;js&#39;</code>即可</p><h4 id="启动爬虫"><a href="#启动爬虫" class="headerlink" title="启动爬虫"></a>启动爬虫</h4><ul><li>在最左侧可以定义项目的分组，以便管理。</li><li>rate/burst代表当前的爬取速率，rate代表每秒发出多少个请求，burst（并发数）相当于流量控制中的令牌桶算法的令牌数，rate和burst设置的越大，爬取速率越快。</li><li>process中的5m、1h、1d指的是最近5分、l小时、l天内的请求情况，all代表所有的请求情况。请求由不同颜色表示，蓝色的代表等待被执行的请求，绿色的代表成功的请求，黄色的代表请求失败后等待重试的请求，红色的代表失败次数过多被忽略的请求，这样可以直观知道爬取的进度和请求情况。</li><li>点击Active Tasks，可查看最近请求的详细状况。</li><li>点击Results，查看所有爬取结果。</li></ul><p><img src="https://s3.ax1x.com/2021/01/18/s61te0.png" alt=""></p><h3 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h3><p>参见官方文档：<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a></p><h4 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h4><p><a href="http://docs.pyspider.org/en/latest/Command-Line/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/Command-Line/</a></p><h4 id="crawl-方法"><a href="#crawl-方法" class="headerlink" title="crawl()方法"></a>crawl()方法</h4><p><a href="http://docs.pyspider.org/en/latest/apis/self.crawl/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/apis/self.crawl/</a></p><h4 id="任务区分"><a href="#任务区分" class="headerlink" title="任务区分"></a>任务区分</h4><p>在pyspider中判断两个任务是否是重复的，使用的是该任务对应的URL的MD5值作为任务的唯一ID，如果ID相同，那么两个任务就会判定为相同，其中一个就不会爬取。这时可以重写task_id()方法，改变这个ID的计算方式来实现不同任务的区分，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pyspider. libs. utils <span class="keyword">import</span> mdsstring</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_taskid</span><span class="params">(self, task)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> mdsstring(task[<span class="string">'url ’]+json.dumps(task['</span>fetch<span class="string">'].get ('</span>data <span class="string">' , " )) )</span></span><br></pre></td></tr></table></figure><h4 id="全局配置"><a href="#全局配置" class="headerlink" title="全局配置"></a>全局配置</h4><p>pyspider可以使用crawl_config来指定全局的配置，配置中的参数会和crawl()方法创建任务时的参数合井。<br>如要全局配置一个Headers，可以定义如下代码：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">  crawl_config = &#123;</span><br><span class="line">      <span class="string">'headers'</span>:&#123;</span><br><span class="line">          <span class="string">'User-Agent'</span>:<span class="string">'GoogleBot'</span>,</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="定时爬取"><a href="#定时爬取" class="headerlink" title="定时爬取"></a>定时爬取</h4><p>通过every属性来设置爬取的时间间隔：<br>在有效时间内爬取不会重复。所以要把有效时间设置得比重复时间更短，即age的时间小于minutes的时间这样才可以实现定时爬取。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#minutes为爬取的时间间隔，单位为分钟</span></span><br><span class="line"><span class="comment">#或写为seconds秒数</span></span><br><span class="line"><span class="meta">@every(minutes=24 * 60)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">  self.crawl(<span class="string">'http://www.example.org'</span>, callback=self.index_page)</span><br><span class="line"></span><br><span class="line"><span class="comment">#age为任务的有效时间，单位为秒</span></span><br><span class="line"><span class="meta">@config(age=10*24*60*60)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self)</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="项目状态"><a href="#项目状态" class="headerlink" title="项目状态"></a>项目状态</h4><ul><li>TODO：它是项目刚刚被创建还未实现时的状态。</li><li>STOP：如果想停止某项目的抓取，可以将项目的状态设置为STOP 。</li><li>CHECKING：正在运行的项目被修改后就会变成CHECKING状态，项目在中途出错需要调整的时候会遇到这种情况。</li><li>DEBUG/RUNNING：这两个状态对项目的运行没有影响，状态设置为任意一个，项目都可以运行，但是可以用二者来区分项目是否已经测试通过。</li><li>PAUSE：当爬取过程中出现连续多次错误时，项目会自动设置为PAUSE状态，并等待一定时间后继续爬取。</li></ul><h4 id="删除项目"><a href="#删除项目" class="headerlink" title="删除项目"></a>删除项目</h4><p>pyspider中没有直接删除项目的选项。如要删除任务，那么将项目的状态设置为STOP，将分组的名称设置为delete，等待24小时，则项目会自动删除。</p><h2 id="Ch-12-Scrapy框架使用"><a href="#Ch-12-Scrapy框架使用" class="headerlink" title="Ch 12 Scrapy框架使用"></a>Ch 12 Scrapy框架使用</h2><h2 id="Ch-13-分布式爬虫"><a href="#Ch-13-分布式爬虫" class="headerlink" title="Ch 13 分布式爬虫"></a>Ch 13 分布式爬虫</h2><h2 id="Ch-14-分布式爬虫的部署"><a href="#Ch-14-分布式爬虫的部署" class="headerlink" title="Ch 14 分布式爬虫的部署"></a>Ch 14 分布式爬虫的部署</h2></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>不负骤雨</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://xxyr.cc/post/Python/python-spider/" title="Python爬虫基础">https://xxyr.cc/post/Python/python-spider/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python/" rel="tag"># Python</a> <a href="/tags/Spider/" rel="tag"># Spider</a> <a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"># 爬虫</a></div><div class="post-nav"><div class="post-nav-item"><a href="/post/Networks/wireshark/" rel="prev" title="WireShark网络分析"><i class="fa fa-chevron-left"></i> WireShark网络分析</a></div><div class="post-nav-item"><a href="/post/Tips/common-commands/" rel="next" title="常用命令">常用命令 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-1-爬虫基础"><span class="nav-number">1.</span> <span class="nav-text">Ch 1 爬虫基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HTTP基本原理"><span class="nav-number">1.1.</span> <span class="nav-text">HTTP基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#URI和URL"><span class="nav-number">1.1.1.</span> <span class="nav-text">URI和URL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#超文本"><span class="nav-number">1.1.2.</span> <span class="nav-text">超文本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTP和HTTPS"><span class="nav-number">1.1.3.</span> <span class="nav-text">HTTP和HTTPS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTP请求过程"><span class="nav-number">1.1.4.</span> <span class="nav-text">HTTP请求过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#请求"><span class="nav-number">1.1.5.</span> <span class="nav-text">请求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#响应"><span class="nav-number">1.1.6.</span> <span class="nav-text">响应</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网页基础"><span class="nav-number">1.2.</span> <span class="nav-text">网页基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#网页的组成"><span class="nav-number">1.2.1.</span> <span class="nav-text">网页的组成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网页的结构"><span class="nav-number">1.2.2.</span> <span class="nav-text">网页的结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点树及节点间关系"><span class="nav-number">1.2.3.</span> <span class="nav-text">节点树及节点间关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#选择器"><span class="nav-number">1.2.4.</span> <span class="nav-text">选择器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫的基本原理"><span class="nav-number">1.3.</span> <span class="nav-text">爬虫的基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#爬虫概述"><span class="nav-number">1.3.1.</span> <span class="nav-text">爬虫概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#能抓怎样的数据"><span class="nav-number">1.3.2.</span> <span class="nav-text">能抓怎样的数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JavaScript渲染页面"><span class="nav-number">1.3.3.</span> <span class="nav-text">JavaScript渲染页面</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#会话和Cookies"><span class="nav-number">1.4.</span> <span class="nav-text">会话和Cookies</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#静态网页和动态网页"><span class="nav-number">1.4.1.</span> <span class="nav-text">静态网页和动态网页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#无状态HTTP"><span class="nav-number">1.4.2.</span> <span class="nav-text">无状态HTTP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见误区"><span class="nav-number">1.4.3.</span> <span class="nav-text">常见误区</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代理的基本原理"><span class="nav-number">1.5.</span> <span class="nav-text">代理的基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本原理"><span class="nav-number">1.5.1.</span> <span class="nav-text">基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代理的作用"><span class="nav-number">1.5.2.</span> <span class="nav-text">代理的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代理分类"><span class="nav-number">1.5.3.</span> <span class="nav-text">代理分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见代理设置"><span class="nav-number">1.5.4.</span> <span class="nav-text">常见代理设置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-2-基本库的使用"><span class="nav-number">2.</span> <span class="nav-text">Ch 2 基本库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用urllib"><span class="nav-number">2.1.</span> <span class="nav-text">使用urllib</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#发送请求"><span class="nav-number">2.1.1.</span> <span class="nav-text">发送请求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#处理异常"><span class="nav-number">2.1.2.</span> <span class="nav-text">处理异常</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解析链接"><span class="nav-number">2.1.3.</span> <span class="nav-text">解析链接</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分析Robots协议"><span class="nav-number">2.1.4.</span> <span class="nav-text">分析Robots协议</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用requests"><span class="nav-number">2.2.</span> <span class="nav-text">使用requests</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本用法"><span class="nav-number">2.2.1.</span> <span class="nav-text">基本用法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#高级用法"><span class="nav-number">2.2.2.</span> <span class="nav-text">高级用法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则表达式"><span class="nav-number">2.3.</span> <span class="nav-text">正则表达式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#抓取猫眼电影排行"><span class="nav-number">2.4.</span> <span class="nav-text">抓取猫眼电影排行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-3-解析库的使用"><span class="nav-number">3.</span> <span class="nav-text">Ch 3 解析库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用XPath"><span class="nav-number">3.1.</span> <span class="nav-text">使用XPath</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#XPath概览"><span class="nav-number">3.1.1.</span> <span class="nav-text">XPath概览</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Xpath常用规则"><span class="nav-number">3.1.2.</span> <span class="nav-text">Xpath常用规则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实例引入"><span class="nav-number">3.1.3.</span> <span class="nav-text">实例引入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点选择"><span class="nav-number">3.1.4.</span> <span class="nav-text">节点选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#文本获取"><span class="nav-number">3.1.5.</span> <span class="nav-text">文本获取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#属性操作"><span class="nav-number">3.1.6.</span> <span class="nav-text">属性操作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用Beautiful-Soup"><span class="nav-number">3.2.</span> <span class="nav-text">使用Beautiful Soup</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介"><span class="nav-number">3.2.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解析器"><span class="nav-number">3.2.2.</span> <span class="nav-text">解析器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基本用法-1"><span class="nav-number">3.2.3.</span> <span class="nav-text">基本用法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点选择器"><span class="nav-number">3.2.4.</span> <span class="nav-text">节点选择器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#方法选择器"><span class="nav-number">3.2.5.</span> <span class="nav-text">方法选择器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CSS选择器"><span class="nav-number">3.2.6.</span> <span class="nav-text">CSS选择器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用pyquery"><span class="nav-number">3.3.</span> <span class="nav-text">使用pyquery</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#初始化"><span class="nav-number">3.3.1.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基本CSS选择器"><span class="nav-number">3.3.2.</span> <span class="nav-text">基本CSS选择器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#查找结点"><span class="nav-number">3.3.3.</span> <span class="nav-text">查找结点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#遍历"><span class="nav-number">3.3.4.</span> <span class="nav-text">遍历</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#获取信息"><span class="nav-number">3.3.5.</span> <span class="nav-text">获取信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#节点操作"><span class="nav-number">3.3.6.</span> <span class="nav-text">节点操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#伪类选择器"><span class="nav-number">3.3.7.</span> <span class="nav-text">伪类选择器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-4-数据存储"><span class="nav-number">4.</span> <span class="nav-text">Ch 4 数据存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#文件存储"><span class="nav-number">4.1.</span> <span class="nav-text">文件存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TXT文本存储"><span class="nav-number">4.1.1.</span> <span class="nav-text">TXT文本存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JSON文件存储"><span class="nav-number">4.1.2.</span> <span class="nav-text">JSON文件存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CSV文件存储"><span class="nav-number">4.1.3.</span> <span class="nav-text">CSV文件存储</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关系型数据库存储"><span class="nav-number">4.2.</span> <span class="nav-text">关系型数据库存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MySQL存储"><span class="nav-number">4.2.1.</span> <span class="nav-text">MySQL存储</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非关系型数据库存储"><span class="nav-number">4.3.</span> <span class="nav-text">非关系型数据库存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MongoDB存储"><span class="nav-number">4.3.1.</span> <span class="nav-text">MongoDB存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Redis存储"><span class="nav-number">4.3.2.</span> <span class="nav-text">Redis存储</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-5-Ajax数据爬取"><span class="nav-number">5.</span> <span class="nav-text">Ch 5 Ajax数据爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是Ajax"><span class="nav-number">5.1.</span> <span class="nav-text">什么是Ajax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ajax分析方法"><span class="nav-number">5.2.</span> <span class="nav-text">Ajax分析方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ajax结果提取"><span class="nav-number">5.3.</span> <span class="nav-text">Ajax结果提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分析Ajax爬取今日头条节拍美图"><span class="nav-number">5.4.</span> <span class="nav-text">分析Ajax爬取今日头条节拍美图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-6-动态渲染页面爬取"><span class="nav-number">6.</span> <span class="nav-text">Ch 6 动态渲染页面爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Selenium的使用"><span class="nav-number">6.1.</span> <span class="nav-text">Selenium的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Splash的使用"><span class="nav-number">6.2.</span> <span class="nav-text">Splash的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Splash负载均衡配置"><span class="nav-number">6.3.</span> <span class="nav-text">Splash负载均衡配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用Selenium爬取淘宝商品"><span class="nav-number">6.4.</span> <span class="nav-text">使用Selenium爬取淘宝商品</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-7-验证码的识别"><span class="nav-number">7.</span> <span class="nav-text">Ch 7 验证码的识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#图形验证码识别"><span class="nav-number">7.1.</span> <span class="nav-text">图形验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#滑动验证码识别"><span class="nav-number">7.2.</span> <span class="nav-text">滑动验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#点触验证码识别"><span class="nav-number">7.3.</span> <span class="nav-text">点触验证码识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#宫格验证码识别"><span class="nav-number">7.4.</span> <span class="nav-text">宫格验证码识别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-8-代理的使用"><span class="nav-number">8.</span> <span class="nav-text">Ch 8 代理的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代理的设置"><span class="nav-number">8.1.</span> <span class="nav-text">代理的设置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#urllib"><span class="nav-number">8.1.1.</span> <span class="nav-text">urllib</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#requests"><span class="nav-number">8.1.2.</span> <span class="nav-text">requests</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Selenium"><span class="nav-number">8.1.3.</span> <span class="nav-text">Selenium</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代理池的维护"><span class="nav-number">8.2.</span> <span class="nav-text">代理池的维护</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本架构"><span class="nav-number">8.2.1.</span> <span class="nav-text">基本架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现"><span class="nav-number">8.2.2.</span> <span class="nav-text">实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#运行"><span class="nav-number">8.2.3.</span> <span class="nav-text">运行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#付费代理的使用"><span class="nav-number">8.3.</span> <span class="nav-text">付费代理的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ADSL拨号代理"><span class="nav-number">8.4.</span> <span class="nav-text">ADSL拨号代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用代理爬取微信公众号文章"><span class="nav-number">8.5.</span> <span class="nav-text">使用代理爬取微信公众号文章</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-9-模拟登录"><span class="nav-number">9.</span> <span class="nav-text">Ch 9 模拟登录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模拟登录并爬取GitHub"><span class="nav-number">9.1.</span> <span class="nav-text">模拟登录并爬取GitHub</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie池的搭建"><span class="nav-number">9.2.</span> <span class="nav-text">Cookie池的搭建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-10-APP的爬取"><span class="nav-number">10.</span> <span class="nav-text">Ch 10 APP的爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Charles的使用"><span class="nav-number">10.1.</span> <span class="nav-text">Charles的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mitmproxy的使用"><span class="nav-number">10.2.</span> <span class="nav-text">mitmproxy的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mitmdump爬取“得到”电子书信息"><span class="nav-number">10.3.</span> <span class="nav-text">mitmdump爬取“得到”电子书信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium的基本使用"><span class="nav-number">10.4.</span> <span class="nav-text">APPium的基本使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium爬取微信朋友圈"><span class="nav-number">10.5.</span> <span class="nav-text">APPium爬取微信朋友圈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#APPium-mitmdump爬取京东商品"><span class="nav-number">10.6.</span> <span class="nav-text">APPium+mitmdump爬取京东商品</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-11-pyspider框架使用"><span class="nav-number">11.</span> <span class="nav-text">Ch 11 pyspider框架使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍"><span class="nav-number">11.1.</span> <span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本功能"><span class="nav-number">11.1.1.</span> <span class="nav-text">基本功能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#与Scrapy比较"><span class="nav-number">11.1.2.</span> <span class="nav-text">与Scrapy比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pyspider架构"><span class="nav-number">11.1.3.</span> <span class="nav-text">pyspider架构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基本使用"><span class="nav-number">11.2.</span> <span class="nav-text">基本使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#相关链接"><span class="nav-number">11.2.1.</span> <span class="nav-text">相关链接</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#安装"><span class="nav-number">11.2.2.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#源码"><span class="nav-number">11.2.3.</span> <span class="nav-text">源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#新建项目"><span class="nav-number">11.2.4.</span> <span class="nav-text">新建项目</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#爬取首页"><span class="nav-number">11.2.5.</span> <span class="nav-text">爬取首页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#爬取详情页"><span class="nav-number">11.2.6.</span> <span class="nav-text">爬取详情页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动爬虫"><span class="nav-number">11.2.7.</span> <span class="nav-text">启动爬虫</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#详解"><span class="nav-number">11.3.</span> <span class="nav-text">详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#命令行"><span class="nav-number">11.3.1.</span> <span class="nav-text">命令行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#crawl-方法"><span class="nav-number">11.3.2.</span> <span class="nav-text">crawl()方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#任务区分"><span class="nav-number">11.3.3.</span> <span class="nav-text">任务区分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#全局配置"><span class="nav-number">11.3.4.</span> <span class="nav-text">全局配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#定时爬取"><span class="nav-number">11.3.5.</span> <span class="nav-text">定时爬取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#项目状态"><span class="nav-number">11.3.6.</span> <span class="nav-text">项目状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#删除项目"><span class="nav-number">11.3.7.</span> <span class="nav-text">删除项目</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-12-Scrapy框架使用"><span class="nav-number">12.</span> <span class="nav-text">Ch 12 Scrapy框架使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-13-分布式爬虫"><span class="nav-number">13.</span> <span class="nav-text">Ch 13 分布式爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ch-14-分布式爬虫的部署"><span class="nav-number">14.</span> <span class="nav-text">Ch 14 分布式爬虫的部署</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="不负骤雨" src="https://i.loli.net/2019/12/15/ev4RZy7Iakn9WHl.jpg"><p class="site-author-name" itemprop="name">不负骤雨</p><div class="site-description" itemprop="description">reading coding keeping</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">文章</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zhishui1?tab=repositories" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhishui1?tab&#x3D;repositories" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/349434614" title="BiliBili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;349434614" rel="noopener" target="_blank"><i class="fa fa-fw fa-custom bili"></i></a> </span><span class="links-of-author-item"><a href="https://steamcommunity.com/id/zhishui_x" title="Steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;zhishui_x" rel="noopener" target="_blank"><i class="fa fa-fw fa-steam"></i></a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">不负骤雨</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">127k</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: true,
      notify: true,
      appId: 'hOTS2YD4rcQy3T7bS94tywjn-gzGzoHsz',
      appKey: '4fQPAOtQk7tqm6wJ8kJxODvT',
      placeholder: "Just go go",
      avatar: 'wavatar',
      meta: guest,
      pageSize: '10' || 10,
      visitor: true,
      lang: 'zh-cn' || 'zh-cn',
      path: location.pathname,
      recordIP: true,
      serverURLs: ''
    });
  }, window.Valine);
});</script></body></html>