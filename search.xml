<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>算法笔记</title>
    <url>/post/Algorithm/algorithm-note/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>算法笔记<a id="more"></a><br><a href="https://pintia.cn/problem-sets/994805260223102976/problems/type/7" target="_blank" rel="noopener">PAT链接</a></p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>排序<br>查找<br>入门模拟<br>算法初步<br>数学问题<br>树<br>图<br>动态规划</p><h3 id="1001-害死人不偿命的-3n-1-猜想"><a href="#1001-害死人不偿命的-3n-1-猜想" class="headerlink" title="1001 害死人不偿命的(3n+1)猜想"></a>1001 害死人不偿命的(3n+1)猜想</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> n, count = <span class="number">0</span>;</span><br><span class="line">   <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">   <span class="keyword">while</span>(n != <span class="number">1</span>)&#123;</span><br><span class="line">      <span class="keyword">if</span>(n % <span class="number">2</span> == <span class="number">0</span>) n = n / <span class="number">2</span>;</span><br><span class="line">      <span class="keyword">else</span> n = (<span class="number">3</span> * n + <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">      count++;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"%d"</span>, count);</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1032-挖掘机技术哪家强"><a href="#1032-挖掘机技术哪家强" class="headerlink" title="1032 挖掘机技术哪家强"></a>1032 挖掘机技术哪家强</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">1e5</span>;</span><br><span class="line"><span class="keyword">int</span> school[maxn] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> n, schoolid, score;</span><br><span class="line">   <span class="keyword">int</span> k = <span class="number">1</span>, <span class="built_in">max</span> = <span class="number">-1</span>;</span><br><span class="line">   <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">   <span class="keyword">if</span>(n &gt; maxn) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">      <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;schoolid, &amp;score);</span><br><span class="line">      school[schoolid] += score;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">      <span class="keyword">if</span>(school[i] &gt; <span class="built_in">max</span>)&#123;</span><br><span class="line">         <span class="built_in">max</span> = school[i];</span><br><span class="line">         k = i;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"%d %d\n"</span>, k, <span class="built_in">max</span>);</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1036-跟奥巴马一起编程"><a href="#1036-跟奥巴马一起编程" class="headerlink" title="1036 跟奥巴马一起编程"></a>1036 跟奥巴马一起编程</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> n, row;</span><br><span class="line">   <span class="keyword">char</span> c;</span><br><span class="line">   <span class="built_in">scanf</span>(<span class="string">"%d %c"</span>, &amp;n, &amp;c);</span><br><span class="line">   <span class="keyword">if</span>(n &lt; <span class="number">3</span> || n &gt; <span class="number">20</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   row = round(n / <span class="keyword">double</span>(<span class="number">2</span>));</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; row; i++)&#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n; j++)&#123;</span><br><span class="line">         <span class="keyword">if</span>(i == <span class="number">0</span> || i == row<span class="number">-1</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(j ==n<span class="number">-1</span>) <span class="built_in">printf</span>(<span class="string">"%c\n"</span>, c);</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%c"</span>, c);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(j == <span class="number">0</span>) <span class="built_in">printf</span>(<span class="string">"%c"</span>, c);</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(j == n<span class="number">-1</span>) <span class="built_in">printf</span>(<span class="string">"%c\n"</span>, c);</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%c"</span>, <span class="string">' '</span>);</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1022-D进制的A-B"><a href="#1022-D进制的A-B" class="headerlink" title="1022 D进制的A+B"></a>1022 D进制的A+B</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> a, b, d, i = <span class="number">0</span>, num[<span class="number">40</span>];</span><br><span class="line">   <span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;a, &amp;b, &amp;d);</span><br><span class="line">   <span class="keyword">int</span> sum = a + b;</span><br><span class="line">   <span class="keyword">do</span>&#123;</span><br><span class="line">      num[i++] = sum % d;</span><br><span class="line">      sum = sum / d;</span><br><span class="line">   &#125;<span class="keyword">while</span>(sum != <span class="number">0</span>);</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> j = i<span class="number">-1</span>; j &gt;= <span class="number">0</span>; j--)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"%d"</span>, num[j]);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1009-说反话"><a href="#1009-说反话" class="headerlink" title="1009 说反话"></a>1009 说反话</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">char</span> str[<span class="number">90</span>], ans[<span class="number">90</span>][<span class="number">90</span>];</span><br><span class="line">   gets(str);</span><br><span class="line">   <span class="keyword">int</span> len = <span class="built_in">strlen</span>(str), row = <span class="number">0</span>, col = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">      <span class="keyword">if</span>(str[i] != <span class="string">' '</span>) ans[row][col++] = str[i];</span><br><span class="line">      <span class="keyword">else</span>&#123;</span><br><span class="line">         ans[row][col] = <span class="string">'\0'</span>;   <span class="comment">//字符数组末尾必须添加结束符'\0'，数字数组不需要</span></span><br><span class="line">         row++;</span><br><span class="line">         col = <span class="number">0</span>;    <span class="comment">//行+1，列恢复至0</span></span><br><span class="line">      &#125; </span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> j = row; j &gt;= <span class="number">0</span>; j--)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"%s"</span>, ans[j]);</span><br><span class="line">      <span class="keyword">if</span>(j &gt; <span class="number">0</span>) <span class="built_in">printf</span>(<span class="string">" "</span>);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> <span class="keyword">word</span> = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">char</span> ans[<span class="number">90</span>][<span class="number">90</span>];</span><br><span class="line">   <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%s"</span>, ans[<span class="keyword">word</span>]) != EOF)&#123;</span><br><span class="line">      <span class="keyword">word</span>++;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="keyword">word</span><span class="number">-1</span>; j &gt;= <span class="number">0</span>; j--)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"%s"</span>, ans[j]);</span><br><span class="line">      <span class="keyword">if</span>(j &gt; <span class="number">0</span>) <span class="built_in">printf</span>(<span class="string">" "</span>);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1006-换个格式输出整数"><a href="#1006-换个格式输出整数" class="headerlink" title="1006 换个格式输出整数"></a>1006 换个格式输出整数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> a, n = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">char</span> ans[<span class="number">30</span>];</span><br><span class="line">   <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;a);</span><br><span class="line">   <span class="keyword">if</span>(a &lt;= <span class="number">0</span> || a &gt;= <span class="number">1000</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span>; i &lt; a/<span class="number">100</span>; i++) ans[n++] = <span class="string">'B'</span>;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> j =<span class="number">0</span>; j &lt; a%<span class="number">100</span>/<span class="number">10</span>; j++) ans[n++] = <span class="string">'S'</span>;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> k =<span class="number">1</span>; k &lt;= a%<span class="number">10</span>; k++) ans[n++] = k+<span class="string">'0'</span>;    <span class="comment">//int转化为char，char(67)是转化成ascii码对应字符</span></span><br><span class="line">   <span class="built_in">puts</span>(ans);</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n,i,h,t,u;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">    h = n / <span class="number">100</span>;<span class="comment">//百位</span></span><br><span class="line">    t = (n % <span class="number">100</span>) / <span class="number">10</span>;<span class="comment">//十位</span></span><br><span class="line">    u = n % <span class="number">10</span>;<span class="comment">//个位</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; h; i++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"B"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"S"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; u; i++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d"</span>,i+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Java基础复习</title>
    <url>/post/Java/java-basis/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Java学习笔记<a id="more"></a></p>]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>深入浅出Docker</title>
    <url>/post/Docker/docker-notes/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>《深入浅出Docker》笔记<a id="more"></a></p><h2 id="Docker概览"><a href="#Docker概览" class="headerlink" title="Docker概览"></a>Docker概览</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="Windows版"><a href="#Windows版" class="headerlink" title="Windows版"></a>Windows版</h4><h4 id="Linux版"><a href="#Linux版" class="headerlink" title="Linux版"></a>Linux版</h4><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><p>查看版本：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker version</span><br></pre></td></tr></table></figure><p>查看image：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker image ls</span><br></pre></td></tr></table></figure><p>查看container：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker container ls</span><br></pre></td></tr></table></figure><p>查看docker系统信息：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker system info</span><br></pre></td></tr></table></figure><h3 id="Docker引擎升级"><a href="#Docker引擎升级" class="headerlink" title="Docker引擎升级"></a>Docker引擎升级</h3><h3 id="Docker存储驱动的选择"><a href="#Docker存储驱动的选择" class="headerlink" title="Docker存储驱动的选择"></a>Docker存储驱动的选择</h3><h2 id="Docker引擎"><a href="#Docker引擎" class="headerlink" title="Docker引擎"></a>Docker引擎</h2><h2 id="Docker镜像"><a href="#Docker镜像" class="headerlink" title="Docker镜像"></a>Docker镜像</h2><h2 id="Docker容器"><a href="#Docker容器" class="headerlink" title="Docker容器"></a>Docker容器</h2><h2 id="应用的容器化"><a href="#应用的容器化" class="headerlink" title="应用的容器化"></a>应用的容器化</h2><h2 id="使用Docker-Compass部署应用"><a href="#使用Docker-Compass部署应用" class="headerlink" title="使用Docker Compass部署应用"></a>使用Docker Compass部署应用</h2><h2 id="Docker-Swarm"><a href="#Docker-Swarm" class="headerlink" title="Docker Swarm"></a>Docker Swarm</h2><h2 id="Docker网络"><a href="#Docker网络" class="headerlink" title="Docker网络"></a>Docker网络</h2><h2 id="Docker覆盖网络"><a href="#Docker覆盖网络" class="headerlink" title="Docker覆盖网络"></a>Docker覆盖网络</h2><h2 id="卷与持久化数据"><a href="#卷与持久化数据" class="headerlink" title="卷与持久化数据"></a>卷与持久化数据</h2><h2 id="使用Docker-Stack部署应用"><a href="#使用Docker-Stack部署应用" class="headerlink" title="使用Docker Stack部署应用"></a>使用Docker Stack部署应用</h2><h2 id="Docker安全"><a href="#Docker安全" class="headerlink" title="Docker安全"></a>Docker安全</h2><h2 id="企业版工具"><a href="#企业版工具" class="headerlink" title="企业版工具"></a>企业版工具</h2><h2 id="企业级特性"><a href="#企业级特性" class="headerlink" title="企业级特性"></a>企业级特性</h2>]]></content>
      <categories>
        <category>Doker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>《Redis开发与运维》笔记</title>
    <url>/post/Redis/redis-notes/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>《Redis开发与运维》学习笔记<a id="more"></a></p><h2 id="Ch-1-初识Redis"><a href="#Ch-1-初识Redis" class="headerlink" title="Ch 1 初识Redis"></a>Ch 1 初识Redis</h2><p>Redis（REmote Dictionary Server）——一种基于键值对的NoSQL数据库。</p><h3 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h3><ul><li><strong>速度快</strong><ul><li>所有数据存放在内存中。</li><li>基于C语言实现。</li><li>单线程架构。</li><li>源代码优化好。</li></ul></li><li><strong>基于键值对的数据结构服务器</strong><ul><li>Redis的值主要支持5种数据结构：字符串（string）、哈希（hash）、列表（list）、集合（set）、有序集合（zset）。</li><li>同时在字符串基础上演变了位图（Bitmaps）、HyperLogLog以及GEO（地理信息定位）等数据结构。</li></ul></li><li><strong>丰富的功能</strong><ul><li>键过期功能，实现缓存。</li><li>发布订阅功能，实现消息系统。</li><li>支持LUA脚本功能，用LUA创造出新的Redis命令。</li><li>简单的事务功能。</li><li>流水线功能，客户端能将命令一次性传到Redis。</li></ul></li><li><strong>简单稳定</strong><ul><li>源码相对较少。</li><li>使用单线程模型。</li><li>不依赖OS中的类库。</li><li>自己实现了事件处理的相关功能。</li></ul></li><li><strong>客户端语言多</strong><ul><li>提供简单的TCP通信协议</li><li>支持Java、PHP、Python、C、C++、Nodejs等. <a href="http://redis.io/clients" target="_blank" rel="noopener">http://redis.io/clients</a></li></ul></li><li><strong>持久化</strong><ul><li>RDB和AOF。</li></ul></li><li><strong>主从复制</strong><ul><li>分布式Redis的基础，实现多个相同数据的Redis副本。</li></ul></li><li><strong>高可用和分布式</strong></li></ul><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><ul><li>缓存</li><li>排行榜系统（列表和有序集合）</li><li>计数器应用（播放数、浏览数等）</li><li>社交网络（赞踩、粉丝、共同好友等）</li><li>消息队列系统</li><li>数据规模不宜过大、适用热数据（经常访问）</li></ul><h3 id="Redis安装与启动"><a href="#Redis安装与启动" class="headerlink" title="Redis安装与启动"></a>Redis安装与启动</h3><h4 id="Redis安装"><a href="#Redis安装" class="headerlink" title="Redis安装"></a>Redis安装</h4><p><a href="https://redis.io/download" target="_blank" rel="noopener">https://redis.io/download</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://download.redis.io/releases/redis-6.0.9.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar xzf redis-6.0.9.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ln -s redis-6.0.9 redis   <span class="comment">#创建软链接</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> redis-6.0.9</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make install</span></span><br></pre></td></tr></table></figure><p>若显示 <code>install: 无法创建普通文件&#39;/usr/local/bin/redis-server&#39;: 权限不够</code><br>则执行<code>su root</code>获取root权限<br>不记得密码执行<code>sudo passwd root</code>修改密码<br>安装成功查看版本号</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> redis-cli -v</span></span><br><span class="line">redis-cli 6.0.9</span><br></pre></td></tr></table></figure><h4 id="Redis配置、启动、操作和关闭"><a href="#Redis配置、启动、操作和关闭" class="headerlink" title="Redis配置、启动、操作和关闭"></a><span id="jump">Redis配置、启动、操作和关闭</span></h4><ul><li>三种启动方式<ul><li>默认配置启动<code>redis-server</code></li><li>运行启动（加上需修改的参数）<code>redis-server --port 6380</code></li><li>配置文件启动<code>redis-sever /opt/redis/redis.conf</code></li></ul></li><li>Redis命令行客户端<ul><li>交互式：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli -h 127.0.0.1 -p 6379</span><br><span class="line">127.0.0.1:6379&gt; set good day</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get good</span><br><span class="line">"day"</span><br></pre></td></tr></table></figure></li><li>命令方式：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli -h 127.0.0.1 -p 6379 get good</span><br><span class="line">"day"</span><br></pre></td></tr></table></figure></li></ul></li><li>停止Redis服务<br><code>redis-cli shutdown</code> （可选参数nosave|save是否生成持久化文件）</li></ul><h2 id="Ch-2-API的理解和使用"><a href="#Ch-2-API的理解和使用" class="headerlink" title="Ch 2 API的理解和使用"></a>Ch 2 API的理解和使用</h2><h3 id="基础知识、操作"><a href="#基础知识、操作" class="headerlink" title="基础知识、操作"></a>基础知识、操作</h3><h4 id="全局命令"><a href="#全局命令" class="headerlink" title="全局命令"></a>全局命令</h4><ul><li>查看所有键<br><code>keys *</code><br>keys命令会遍历所有键，时间复杂度为O（n），线上环境禁止使用。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; keys *</span><br><span class="line">1) "good"</span><br><span class="line">2) "hello"</span><br></pre></td></tr></table></figure></li><li>键总数<br><code>dbsize</code><br>直接获取Redis内置的键总数变量，时间复杂度为O（1）。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; dbsize</span><br><span class="line">(integer) 2</span><br></pre></td></tr></table></figure></li><li>检查键是否存在<br><code>exists key</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; exists hello</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; exists no_hello</span><br><span class="line">(integer) 0</span><br></pre></td></tr></table></figure></li><li>删除键<br><code>del key1 key2 key3~</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; del good hello</span><br><span class="line">(integer) 2                       #返回成功删除键的个数</span><br></pre></td></tr></table></figure></li><li>键过期<br><code>expire key seconds</code><br>对键添加过期时间，过期自动删除<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; expire hello 10   #单位：s</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; ttl hello</span><br><span class="line">(integer) 5                       #剩余过期时间</span><br><span class="line">127.0.0.1:6379&gt; ttl hello</span><br><span class="line">(integer) -2                      #-2表示键不存在，-1表示键没有设置过期时间</span><br><span class="line">127.0.0.1:6379&gt; get hello</span><br><span class="line">(nil)</span><br></pre></td></tr></table></figure></li><li>键的数据结构类型<br><code>type key</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; rpush mylist a b c d e</span><br><span class="line">(integer) 6</span><br><span class="line">127.0.0.1:6379&gt; type mylist</span><br><span class="line">list</span><br><span class="line">127.0.0.1:6379&gt; type a</span><br><span class="line">none</span><br></pre></td></tr></table></figure></li></ul><h4 id="数据结构和内部编码"><a href="#数据结构和内部编码" class="headerlink" title="数据结构和内部编码"></a>数据结构和内部编码</h4><p>优点：改进内部编码而不影响外部数据结构和命令，适应各种场景更加灵活，如图。<br><img src="https://s3.ax1x.com/2020/12/31/rjAmUx.png" alt=""></p><h4 id="单线程架构"><a href="#单线程架构" class="headerlink" title="单线程架构"></a>单线程架构</h4><ul><li>单线程首先不会出现并发问题，可以简化数据结构和算法的实现，并且避免了线程切换和竞态产生的消耗，但不适合某个命令执行时间过长的场景。</li><li>纯内存访问，重要基础。</li><li>非阻塞I/O，使用epoll作为I/O多路复用技术的实现以及自身事件模型，如图。<br><img src="https://s3.ax1x.com/2020/12/31/rjnP8x.png" alt=""></li></ul><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>Redis最基础的数据结构，键是字符串类型，字符串类型的值可以是字符串（JSON、XML等）、数字、二进制，值最大不能超过512MB。</p><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><ul><li>常用命令<ul><li>设置值<br><code>set key value [ex seconds] [px milliseconds] [nx|xx]</code><br>ex seconds：为键设置秒级过期时间。<br>px milliseconds：为键设置毫秒级过期时间。<br>nx：等价于<code>setnx key value</code>，键必须不存在才能设置成功，用于添加（可以用于实现分布式锁）。<br>xx：等价于<code>setex key seconds value</code>，键必须存在才能设置成功，用于更新。</li><li>获取值<br><code>get key</code></li><li>批量设置值<br><code>mset key value key value···</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; mset a 1 b 2 c 3</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li><li>批量获取值<br><code>mget key key key···</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; mget a b c d</span><br><span class="line">1) "1"</span><br><span class="line">2) "2"</span><br><span class="line">3) "3"</span><br><span class="line">4) (nil)</span><br></pre></td></tr></table></figure></li><li>计数<br><code>incr key</code><br>用于对值做自增操作。<br>值必须是整数.<br>键不存在，按值为0自增，返回结果为1。<br>还有：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">decr key                    #自减</span><br><span class="line">incrby key incerement       #自增指定数字</span><br><span class="line">decrby key decrement        #自减指定数字</span><br><span class="line">incrbyfloat key incerement  #自增浮点数</span><br></pre></td></tr></table></figure></li></ul></li><li>不常用命令<ul><li>追加值<br><code>append key value</code><br>向字符串尾部追加值</li><li>字符串长度<br><code>strlen key</code><br>返回字节数</li><li>设置并返回原值<br><code>getset key value</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; getset nice day</span><br><span class="line">(nil)</span><br><span class="line">127.0.0.1:6379&gt; getset nice try</span><br><span class="line">"day"</span><br></pre></td></tr></table></figure></li><li>设置指定位置的字符<br><code>setrange key offset value</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set redis pest</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; setrange redis 0 b</span><br><span class="line">(integer) 4</span><br><span class="line">127.0.0.1:6379&gt; get redis</span><br><span class="line">"best"</span><br></pre></td></tr></table></figure></li><li>获取部分字符串<br><code>getrange key start end</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; getrange redis 0 1</span><br><span class="line">"be"</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>各命令时间复杂度如图。<br><img src="https://s3.ax1x.com/2021/01/05/sF3wuT.png" alt=""></p><h4 id="内部编码"><a href="#内部编码" class="headerlink" title="内部编码"></a>内部编码</h4><ul><li>int<br>8个字节的长整型。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set key 8653</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; object encoding key</span><br><span class="line">"int"</span><br></pre></td></tr></table></figure></li><li>embstr<br>小于等于39个字节的字符串。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set key "hello world"</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; object encoding key</span><br><span class="line">"embstr"</span><br></pre></td></tr></table></figure></li><li>raw<br>大于39个字节的字符串。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set key "hello world···"</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; object encoding key</span><br><span class="line">"raw"</span><br></pre></td></tr></table></figure></li></ul><h4 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a>使用场景</h4><ul><li>缓存功能<br>Redis作为缓存层，MySQL作为存储层，如下图。<br><img src="https://s3.ax1x.com/2020/12/31/rvp9KI.png" alt=""><br>例如下面用于获取用户信息的伪代码。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">UserInfo getUserInfo(long id)&#123;</span><br><span class="line">  userRedisKey = <span class="string">"user:info:"</span> + id</span><br><span class="line">  value = redis.get(userRedisKey);</span><br><span class="line">  UserInfo userInfo;</span><br><span class="line">  <span class="keyword">if</span> (value != null) &#123;</span><br><span class="line">    userInfo = deserialize(value);    <span class="comment">#反序列化</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    userInfo = mysql.get(id);</span><br><span class="line">    <span class="keyword">if</span> (userInfo != null)</span><br><span class="line">      redis.setex(userRedisKey, <span class="number">3600</span>, serialize(userInfo));   <span class="comment">#一小时过期时间</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> userInfo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>计数<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">long incrVideoCounter(long id) &#123;</span><br><span class="line">  key = <span class="string">"video:playCount:"</span> + id;</span><br><span class="line">  <span class="keyword">return</span> redis.incr(key);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>共享Session<br>分布式Web服务使用Redis将用户的Session进行集中管理。<br><img src="https://s3.ax1x.com/2020/12/31/rvEkCV.png" alt=""></li><li>限速<br>例如网站限制一个IP地址在一定时间内的访问次数等。<br>用户获取验证码一分钟不超过5次的伪代码如下。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">phoneNum = <span class="string">"138xxxxxxxx"</span>;</span><br><span class="line">key = <span class="string">"shortMsg:limit:"</span> + phoneNum;</span><br><span class="line">// SET key value EX <span class="number">60</span> NX</span><br><span class="line">isExists = redis.set(key,<span class="number">1</span>,<span class="string">"EX 60"</span>,<span class="string">"NX"</span>);</span><br><span class="line"><span class="keyword">if</span>(isExists != null || redis.incr(key) &lt;=<span class="number">5</span>)&#123;</span><br><span class="line">  // 通过</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">  // 限速</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="哈希"><a href="#哈希" class="headerlink" title="哈希"></a>哈希</h3><p>键值本身又是一对键值对结构，形如</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">value = &#123;&#123;filed1,value1&#125;,&#123;field2,value2&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="命令-1"><a href="#命令-1" class="headerlink" title="命令"></a>命令</h4><ul><li>设置值<br><code>hset key filed value</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hset user:1 name tom</span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure></li><li>获取值<br><code>hget key filed</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hget user:1 name</span><br><span class="line">"tom"</span><br></pre></td></tr></table></figure></li><li>删除field<br><code>hdel key filed1 field2...</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hdel user:1 name age</span><br><span class="line">(integer) 2</span><br></pre></td></tr></table></figure></li><li>计算field个数<br><code>hlen key</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hlen user:1</span><br><span class="line">(integer) 0</span><br></pre></td></tr></table></figure></li><li>批量设置或获取filed-value<br><code>hmset key filed1 value1 filed2 value2</code><br><code>hmget key filed1 field2...</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hmset user:1 name tom age 18</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; hmget user:1 name age</span><br><span class="line">1) "tom"</span><br><span class="line">2) "18"</span><br></pre></td></tr></table></figure></li><li>判断filed是否存在<br><code>hexists key filed</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hexists user:1 name</span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure></li><li>获取所有field<br><code>hkeys key</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hkeys user:1</span><br><span class="line">1) "name"</span><br><span class="line">2) "age"</span><br></pre></td></tr></table></figure></li><li>获取所有value<br><code>hvals key</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hvals user:1</span><br><span class="line">1) "tom"</span><br><span class="line">2) "18"</span><br></pre></td></tr></table></figure></li><li>获取所有的field-value<br><code>hgetall key</code>（个数较多会导致阻塞）<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hgetall user:1</span><br><span class="line">1) "name"</span><br><span class="line">2) "tom"</span><br><span class="line">3) "age"</span><br><span class="line">4) "18"</span><br></pre></td></tr></table></figure></li><li>自增<br><code>hincrby key filed</code><br><code>hincrbyfloat key filed</code></li><li>计算value的字符串长度<br><code>hstrlen key filed</code>（Redis&gt;3.2）<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hstrlen user:1 name</span><br><span class="line">(integer) 3</span><br></pre></td></tr></table></figure></li></ul><p>各命令时间复杂度如图。<br><img src="https://s3.ax1x.com/2021/01/05/sF8CPs.png" alt=""></p><h4 id="内部编码-1"><a href="#内部编码-1" class="headerlink" title="内部编码"></a>内部编码</h4><ul><li>ziplist（压缩列表）<br>使用更紧凑的结构实现多个元素的连续存储，更加节省内存。<br>使用场景：<ul><li>哈希元素类型个数小于hash-max-ziplist-entries配置（默认512个——field个数）</li><li>同时所有的值小于hash-max-ziplist-value配置（默认64字节——value值大小）</li></ul></li><li>hashtable（哈希表）<br>不满足ziplist时采用，O(1)。</li></ul><h4 id="使用场景-2"><a href="#使用场景-2" class="headerlink" title="使用场景"></a>使用场景</h4><p>哈希类型是稀疏的，而关系型数据库是完全结构化的，且Redis不适合做复杂的关系查询，<br>以下为获取用户信息的伪代码。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">UserInfo getUserInfo(long id)&#123;</span><br><span class="line">  // 用户id作为key后缀</span><br><span class="line">  userRedisKey = <span class="string">"user:info:"</span> + id;</span><br><span class="line">  // 使用hgetall获取所有用户信息映射关系</span><br><span class="line">  userInfoMap = redis.hgetAll(userRedisKey);</span><br><span class="line">  UserInfo userInfo;</span><br><span class="line">  <span class="keyword">if</span> (userInfoMap != null) &#123;</span><br><span class="line">    // 将映射关系转换为UserInfo</span><br><span class="line">    userInfo = transferMapToUserInfo(userInfoMap);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    // 从MySQL中获取用户信息</span><br><span class="line">    userInfo = mysql.get(id);</span><br><span class="line">    // 将userInfo变为映射关系使用hmset保存到Redis中</span><br><span class="line">    redis.hmset(userRedisKey, transferUserInfoToMap(userInfo));</span><br><span class="line">    // 添加过期时间</span><br><span class="line">    redis.expire(userRedisKey, <span class="number">3600</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> userInfo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>三种缓存用户信息方法对比。</p><ul><li>原生字符串类型<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set user:1 name tom</span><br><span class="line">set user:1 age 18</span><br></pre></td></tr></table></figure>每个属性一个键，简单直观，但内存占用大，用户信息内聚性差。</li><li>序列化字符串类型<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set user:1 serialize(userinfo)</span><br></pre></td></tr></table></figure>简化编程，但序列化和反序列化有一定开销，且每次更新都要进行反序列化和序列化。</li><li>哈希类型<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hmset user:1 name tom age 18</span><br></pre></td></tr></table></figure>简单直观，合理使用可减少内存占用，但要注意两种内部编码的转换，hashtable会消耗更多内存。</li></ul><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>可以存储多个<strong>有序</strong>、<strong>重复</strong>的字符串，每个字符串称为元素（个数不大于2^32-1），可以充当<strong>栈</strong>和<strong>队列</strong>。</p><h4 id="命令-2"><a href="#命令-2" class="headerlink" title="命令"></a>命令</h4><ul><li>添加<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rpush key value1 value2           #从右侧插入</span><br><span class="line">lpush key value1 value2           #从左侧插入</span><br><span class="line">linsert key before|after me you   #在me之前或之后插入you</span><br></pre></td></tr></table></figure></li><li>查找<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">lrange key start end              #从左至右为0到N-1，从右至左为-1到-N，end包含自身，0到-1为全部</span><br><span class="line">lindex key index                  #获取指定下标的元素</span><br><span class="line">llen key                          #获取列表长度</span><br></pre></td></tr></table></figure></li><li>删除<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rpop key                          #从右侧删除</span><br><span class="line">lpop key                          #从右侧删除</span><br><span class="line">lrem key count value              #删除值等于value的元素</span><br><span class="line">                                  #count&gt;0，从左至右，最多删除count个元素</span><br><span class="line">                                  #count&lt;0，从右至左，最多删除count绝对值个元素</span><br><span class="line">                                  #count=0，删除所有</span><br><span class="line">ltrim key start end               #保留列表中start到end的元素</span><br></pre></td></tr></table></figure></li><li>修改<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">lset key index newValue</span><br></pre></td></tr></table></figure></li><li>阻塞式弹出<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">blpop key1 key2 timeout</span><br><span class="line">brpop key1 key2 timeout</span><br></pre></td></tr></table></figure>列表为空时，timeout=3则客户端等到3s后返回，timeout=0则一直阻塞。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; brpop list:test 3</span><br><span class="line">(nil)</span><br><span class="line">(3.09s)</span><br><span class="line">127.0.0.1:6379&gt; brpop list:test 0</span><br><span class="line">...阻塞中...</span><br></pre></td></tr></table></figure>此期间添加了数据element1，客户端立即返回。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; lpush list:test element1</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; brpop list:test 0</span><br><span class="line">1) "list:test"</span><br><span class="line">2) "element1"</span><br><span class="line">(90.81s)</span><br></pre></td></tr></table></figure>列表不为空，客户端立即返回。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; brpop list:test 0</span><br><span class="line">1) "list:test"</span><br><span class="line">2) "element1"</span><br></pre></td></tr></table></figure>若多个客户端对同一个键执行brpop，那么最先执行brpop命令的客户端可以获取弹出的值。</li></ul><p>各命令时间复杂度如图。<br><img src="https://s3.ax1x.com/2021/01/05/sF8WJs.png" alt=""></p><h4 id="内部编码-2"><a href="#内部编码-2" class="headerlink" title="内部编码"></a>内部编码</h4><ul><li>ziplist（压缩列表）<br>使用更紧凑的结构实现多个元素的连续存储，更加节省内存。<br>使用场景：<ul><li>哈希元素类型个数小于list-max-ziplist-entries配置（默认512个——field个数）</li><li>同时所有的值小于list-max-ziplist-value配置（默认64字节——value值大小）</li></ul></li><li>linkedlist（链表）<br>不满足ziplist时采用。</li></ul><p>注：Redis 3.2 版本提供了quicklist编码，是以一个ziplist为节点的linkedlist，结合了两者的优势。</p><h4 id="使用场景-3"><a href="#使用场景-3" class="headerlink" title="使用场景"></a>使用场景</h4><ul><li>消息队列<br>使用lpush和brpop命令组合实现阻塞队列，如图。<br><img src="https://s3.ax1x.com/2021/01/03/s9MDTP.png" alt=""></li><li>文章列表<br>如分页展示文章列表。<ul><li>每篇文章使用哈希存储<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hmset acticle:1 title xx timestamp 1476536196 content xxxx</span><br></pre></td></tr></table></figure></li><li>向用户文章列表添加文章<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">lpush user:1:acticles article:1 article:2</span><br></pre></td></tr></table></figure></li><li>分页获取用户文章列表<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">articles = lrange user:1:articles 0 9</span><br><span class="line">for article in &#123;articles&#125;</span><br><span class="line">  hgetall &#123;article&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>主要应用场景：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">lpush+lpop=Stack（ 栈）</span><br><span class="line">lpush+rpop=Queue（ 队列）</span><br><span class="line">lpsh+ltrim=Capped Collection（ 有限集合）</span><br><span class="line">lpush+brpop=Message Queue（ 消息队列）</span><br></pre></td></tr></table></figure><h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><p>集合（set）用来保存多个<strong>无序</strong>字符串元素，且不允许有重复的元素，不支持下标索引。<br>一个集合可以存储2^32-1个元素<br>Redis支持集合内的增删改查以及集合间取交集、并集和差集。</p><h4 id="命令-3"><a href="#命令-3" class="headerlink" title="命令"></a>命令</h4><ul><li>集合内操作<ul><li>添加元素<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sadd key element1 element2</span><br></pre></td></tr></table></figure></li><li>删除元素<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">srem key element1 element2</span><br></pre></td></tr></table></figure></li><li>计算元素个数<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scard key  #时间复杂度O(1)，直接调用Redis内部变量</span><br></pre></td></tr></table></figure></li><li>判断元素是否在集合中<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sismember key element</span><br></pre></td></tr></table></figure></li><li>随机从集合返回指定个数的元素<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">srandmember key count  #count默认为1</span><br></pre></td></tr></table></figure></li><li>从集合随机弹出count个元素<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spop key count #Redis&gt;3.2，count默认为1</span><br></pre></td></tr></table></figure></li><li>获取所有元素<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">smembers key</span><br></pre></td></tr></table></figure></li></ul></li><li>集合间操作<ul><li>交集<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sinter key1 key2</span><br></pre></td></tr></table></figure></li><li>并集<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sunion key1 key2</span><br></pre></td></tr></table></figure></li><li>差集<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sdiff key1 key2</span><br></pre></td></tr></table></figure></li><li>将交集、并集和差集的结果保存<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">将key1和key2运算的结果保存在destination_key中</span></span><br><span class="line">sinterstore destination_key key1 key2</span><br><span class="line">sunionstore destination_key key1 key2</span><br><span class="line">sdiffstore destination_key key1 key2</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>各命令时间复杂度如图。<br><img src="https://s3.ax1x.com/2021/01/05/sFG9eO.png" alt=""></p><h4 id="内部编码-3"><a href="#内部编码-3" class="headerlink" title="内部编码"></a>内部编码</h4><ul><li>intset（整数集合）<br>更加节省内存。<br>使用场景：<ul><li>集合中都是整数且元素个数小于set-max-intset-entries配置（默认512个）</li></ul></li><li>hashtable（哈希表）<br>不满足intset时采用。</li></ul><h4 id="使用场景-4"><a href="#使用场景-4" class="headerlink" title="使用场景"></a>使用场景</h4><p>集合典型的使用场景是<strong>标签</strong>（用于提升用户体验和增强用户黏度）。<br>注：用户和标签的关系维护应在一个事务内执行，防止部分命令失败造成的数据不一致。</p><ul><li>给用户添加标签<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sadd user:1:tags tag1 tag2</span><br></pre></td></tr></table></figure>给标签添加用户<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sadd tag1:users user:1 user:2</span><br></pre></td></tr></table></figure></li><li>删除用户下的标签<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">srem user:1:tags tag1 tag2</span><br></pre></td></tr></table></figure>删除标签下的用户<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">srem tag1:users user:1 user:2</span><br></pre></td></tr></table></figure></li><li>计算用户共同感兴趣的标签<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sinter user:1:tags user:2:tags</span><br></pre></td></tr></table></figure></li></ul><p>主要应用场景：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sadd=Tagging（标签）</span><br><span class="line">spop/srandmember=Random item（生成随机数， 比如抽奖）</span><br><span class="line">sadd+sinter=Social Graph（社交需求）</span><br></pre></td></tr></table></figure><h3 id="有序集合"><a href="#有序集合" class="headerlink" title="有序集合"></a>有序集合</h3><p>相较于集合，有序集合中的元素依据分数进行排序，且元素不能重复但分数可以重复。</p><h4 id="命令-4"><a href="#命令-4" class="headerlink" title="命令"></a>命令</h4><ul><li>集合内<ul><li>添加成员<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zadd key score1 member1 score2 member2</span><br><span class="line"></span><br><span class="line"><span class="meta">Redis&gt;</span><span class="bash">3.2，添加了如下参数：</span></span><br><span class="line">nx：member必须不存在才可以添加成功，用于添加。</span><br><span class="line">xx：member必须存在才可以添加成功，用于更新。</span><br><span class="line">ch：返回此次操作后，元素和分数发生变化的个数。</span><br><span class="line">incr：对score做增加，等于zincrby</span><br></pre></td></tr></table></figure></li><li>计算成员个数<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zcard key</span><br></pre></td></tr></table></figure></li><li>计算某个成员的分数<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zscore key member</span><br></pre></td></tr></table></figure></li><li>计算成员的排名<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zrank key member  #分数从低到高</span><br><span class="line">zrevrank key member  #分数从高到低</span><br></pre></td></tr></table></figure></li><li>增加成员的分数<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zincrby key increment member</span><br></pre></td></tr></table></figure></li><li>返回指定排名范围的成员<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zrange key start end [withscores]  #从低到高，withscores可选</span><br><span class="line">zrevrange key start end withscores</span><br></pre></td></tr></table></figure></li><li>返回指定分数范围的成员<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zrangebyscore key min max [withscores] [limit offset count]  #限制起始的位置和个数</span><br><span class="line">zrevrangebyscore key min max [withscores] [limit offset count]</span><br><span class="line"></span><br><span class="line">limit offset count 可选参数，限制起始的位置和个数</span><br><span class="line">min和max支持开区间()和闭区间[]，-inf和+inf代表无穷小和无穷大</span><br></pre></td></tr></table></figure></li><li>返回指定分数范围的成员个数<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zcount key min max</span><br></pre></td></tr></table></figure></li><li>删除成员<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zrem key member1 member2</span><br></pre></td></tr></table></figure></li><li>删除指定排名内的升序元素<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zremrangebyrank key start end</span><br></pre></td></tr></table></figure></li><li>删除指定分数范围的成员<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zremrangebyscore key min max</span><br></pre></td></tr></table></figure></li></ul></li><li>集合间<ul><li>交集<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zinterstore destination numkeys key1 key2 weight1 weight2 [aggregate sum|min|max]</span><br><span class="line"></span><br><span class="line">destinatin：保存计算结果的键</span><br><span class="line">numkeys：参与计算键的个数</span><br><span class="line">weight：每个键的权重，每个键的member会将自己的分数乘以这个权重。</span><br><span class="line">aggregate sum|min|max：计算成员交集后，分数可以按照sum、min和max做汇总，默认sum。</span><br></pre></td></tr></table></figure></li><li>并集<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zunionstore destination numkeys key1 key2 weight1 weight2 [aggregate sum|min|max]</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>各命令时间复杂度如图。<br><img src="https://s3.ax1x.com/2021/01/05/sFG1Yj.png" alt=""></p><h4 id="内部编码-4"><a href="#内部编码-4" class="headerlink" title="内部编码"></a>内部编码</h4><ul><li>ziplist（压缩列表）<br>使用更紧凑的结构实现多个元素的连续存储，更加节省内存。<br>使用场景：<ul><li>哈希元素类型个数小于zset-max-ziplist-entries配置（默认128个）</li><li>同时所有的值小于zset-max-ziplist-value配置（默认64字节）</li></ul></li><li>skiplist（跳跃表）<br>不满足ziplist时采用。</li></ul><h4 id="使用场景-5"><a href="#使用场景-5" class="headerlink" title="使用场景"></a>使用场景</h4><p>有序集合的典型使用场景为排行榜系统。榜单的维度可以是多方面的。<br>例如视频网站对用户上传到视频做排行榜，使用赞数这个维度。</p><ul><li>添加用户赞数<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zadd user:ranking:2021_01_05 3 mike  #用户mike上传了一个视频并获得了3个赞</span><br><span class="line">zincrby user:ranking:2021_01_05 1 mike  #之后又获得了一个赞</span><br></pre></td></tr></table></figure></li><li>取消用户赞数<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zrem user:ranking:2021_01_05 mike</span><br></pre></td></tr></table></figure></li><li>展示获赞数最多前十用户<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zrevrangebyrank user:ranking:2021_01_05 0 9</span><br></pre></td></tr></table></figure></li><li>展示用户信息及分数<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hgetall user:info:tom</span><br><span class="line">zscore user:ranking:2021_01_05 mike</span><br><span class="line">zrank user:ranking:2021_01_05 mike</span><br></pre></td></tr></table></figure></li></ul><h3 id="键管理"><a href="#键管理" class="headerlink" title="键管理"></a>键管理</h3><p>关于键的一些通用命令介绍。<br>接2.1.1全局命令的一些其他命令。</p><h4 id="单个键管理"><a href="#单个键管理" class="headerlink" title="单个键管理"></a>单个键管理</h4><ul><li><p>键重命名</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rename key newkey</span><br></pre></td></tr></table></figure><p>（1）若rename的newkey已存在，那么该已存在的newkey的值会变成被rename的key的值。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set old a</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set new b</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; rename old new</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get old</span><br><span class="line">(nil)</span><br><span class="line">127.0.0.1:6379&gt; get new</span><br><span class="line">"a"</span><br></pre></td></tr></table></figure><p>（2）为防止被强行rename，Redis提供了<code>renamenx</code>命令，只有newkey不存在时才能rename成功。<br>（3）由于重命名键期间会执行del删除旧的键，如果键对应的值较大，可能会导致阻塞。<br>（4）<code>rename key key</code>在3.2版本前会报错。</p></li><li><p>随机返回一个键</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">randomkey</span><br></pre></td></tr></table></figure></li><li><p>键过期</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">expire key seconds                      #键在seconds秒后过期</span><br><span class="line">pexpire key millionseconds              #毫秒级</span><br><span class="line">                                        #key不存在返回0</span><br><span class="line">                                        #seconds为负数，键会立即被删除</span><br><span class="line"></span><br><span class="line">expireat key timestamp                  #键在秒级时间戳timestamp后过期</span><br><span class="line">pexpireat key millionseconds-timestamp  #毫秒级</span><br><span class="line"></span><br><span class="line">ttl key                                 #查询键的剩余过期时间，单位秒。</span><br><span class="line">pttl key                                #毫秒级</span><br><span class="line">                                        #-1表示键没有设置过期时间，-2表示键不存在</span><br><span class="line"></span><br><span class="line">persist key                             #消除键的过期时间</span><br></pre></td></tr></table></figure><p>（1）无论何种方式，在Redis内部最终使用的是pexpireat。<br>（2）对于字符串类型的键，执行<code>set</code>命令会消除过期时间，详见源码中set命令的函数setKey。<br>（3）Redis不支持二级数据结构（哈希、列表等）内部元素的过期功能。<br>（4）<code>setex</code>作为set+expire的组合，不单是原子执行，且减少了一次网络通讯的时间。</p></li><li><p>迁移键</p><ul><li><p>move</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">move key db</span><br></pre></td></tr></table></figure><p>Redis内部可以有多个数据库，且彼此相互隔离。move命令用于在Redis内部进行数据迁移。</p></li><li><p>dump+store</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dump key</span><br><span class="line">restore key ttl value  #ttl为过期时间，=0不设置</span><br></pre></td></tr></table></figure><p>dump+store可实现不同Redis实例之间的数据迁移,整个迁移过程是非原子性的过程，且需要开启两个客户端连接。分为两步：<br>（1）在源Redis上执行dump，将键值序列化，为RDB格式。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">redis-source&gt;</span><span class="bash"> <span class="built_in">set</span> hello world</span></span><br><span class="line">OK</span><br><span class="line"><span class="meta">redis-source&gt;</span><span class="bash"> dump hello</span></span><br><span class="line">"\x00\x05world\x06\x00\x8f&lt;T\x04%\xfcNQ"</span><br></pre></td></tr></table></figure><p>（2）在目标Redis上执行restore，进行复原。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">redis-target&gt;</span><span class="bash"> get hello</span></span><br><span class="line">(nil)</span><br><span class="line"><span class="meta">redis-target&gt;</span><span class="bash"> restore hello 0 <span class="string">"\x00\x05world\x06\x00\x8f&lt;T\x04%\xfcNQ"</span></span></span><br><span class="line">OK</span><br><span class="line"><span class="meta">redis-target&gt;</span><span class="bash"> get hello</span></span><br><span class="line">"world</span><br></pre></td></tr></table></figure><p>对应伪代码为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Redis sourceRedis = new Redis("sourceMachine", 6379)</span><br><span class="line">Redis targetRedis = new Redis("targetMachine", 6379)</span><br><span class="line">targetRedis.restore("hello", 0, sourceRedis.dump(key))</span><br></pre></td></tr></table></figure></li><li><p>migrate</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">migrate host port key|"" destination-db timeout [copy] [replace] [keys key [key...]]</span><br><span class="line"></span><br><span class="line">host：              目标Redis的IP地址。</span><br><span class="line">port：              目标Redis的端口。</span><br><span class="line">key|""：            在Redis3.0.6版本之前，migrate只支持迁移一个键，所以此处是要迁移的键，</span><br><span class="line">                    在Redis3.0.6版本之后支持迁移多个键，如果当前需要迁移多个键，此处为空字符串""。</span><br><span class="line">destination-db：    目标Redis的数据库索引，例如要迁移到0号数据库，这里就写0。</span><br><span class="line">timeout：           迁移的超时时间（单位为毫秒）。</span><br><span class="line">[copy]：            如果添加此选项，迁移后并不删除源键。</span><br><span class="line">[replace]：         如果添加此选项，migrate不管目标Redis是否存在该键都会正常迁移进行数据覆盖。</span><br><span class="line">[keys key[key...]]：迁移多个键，例如要迁移key1、key2、key3，此处填写“keys key1 key2 key3”。</span><br></pre></td></tr></table></figure><p>（1）migrate命令也是用于在Redis实例间进行数据迁移，实际上migrate命令就是将dump、restore、del三个命令进行组合，从而简化了操作流程，而且从Redis3.0.6版本以后已经支持迁移多个键的功能，有效地提高了迁移效率。<br>（2）第一，整个过程是原子执行的，不需要在多个Redis实例上开启客户端的，只需要在源Redis上执行migrate命令即可。第二，migrate命令的数据传输直接在源Redis和目标Redis上完成。第三，目标Redis完成restore后会发送OK给源Redis，源Redis接收后会根据migrate对应的选项来决定是否在源Redis上删除对应的键。</p><p>示例：</p><ul><li>源Redis有键hello，目标Redis没有<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; migrate 127.0.0.1 6380 hello 0 1000</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li><li>源Redis和目标Redis都有键hello<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; migrate 127.0.0.1 6379 hello 0 1000</span><br><span class="line">(error) ERR Target instance replied with error: BUSYKEY Target key name already exists.</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; migrate 127.0.0.1 6379 hello 0 1000 replace</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li><li>源Redis没有键hello。如下所示，此种情况会收到nokey的提示<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; migrate 127.0.0.1 6380 hello 0 1000</span><br><span class="line">NOKEY</span><br></pre></td></tr></table></figure></li><li>源Redis执行如下命令完成多个键的迁移<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; migrate 127.0.0.1 6380 "" 0 5000 keys key1 key2 key3</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="遍历键"><a href="#遍历键" class="headerlink" title="遍历键"></a>遍历键</h4><ul><li><p>全量遍历键</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">keys pattern  #patterm使用glob风格通配符</span><br><span class="line"></span><br><span class="line">* 代表匹配所有任意字符</span><br><span class="line">? 代表匹配一个任意字符</span><br><span class="line">[1,3] 代表匹配1和3，[1-3] 代表匹配1到3的任意数字</span><br><span class="line">\x 转义字符</span><br></pre></td></tr></table></figure><p>考虑到Redis的单线程架构，keys命令一般只在以下三种情况使用：</p><ul><li>在一个不对外提供服务的Redis从节点上执行，这样不会阻塞到客户端的请求，但是会影响到主从复制。</li><li>如果确认键值总数确实比较少，可以执行该命令。</li><li>使用scan命令渐进式的遍历所有键。</li></ul></li><li><p>渐进式遍历</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan cursor [match pattern] [count number]</span><br><span class="line"></span><br><span class="line">cursor：是一个游标，第一次遍历从0开始，每次scan遍历完都会返回当前游标的值，直到游标值为0，表示遍历结束。</span><br><span class="line">match pattern：可选参数，作用是做模式匹配。</span><br><span class="line">count number：可选参数，作用是表明每次要遍历的键个数，默认值是10，可以适当增大。</span><br></pre></td></tr></table></figure><p>Redis存储键值对实际使用hashtable作为内部编码。<br>示例：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "15"</span><br><span class="line">2)  1) "new"</span><br><span class="line">    2) "c"</span><br><span class="line">    3) "a"</span><br><span class="line">    4) "key"</span><br><span class="line">    5) "nice"</span><br><span class="line">    6) "b"</span><br><span class="line">    7) "mylist"</span><br><span class="line">    8) "d"</span><br><span class="line">    9) "redis"</span><br><span class="line">  10) "user:1"</span><br><span class="line">127.0.0.1:6379&gt; scan 15</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "hello"</span><br></pre></td></tr></table></figure><p>除了scan以外，Redis提供了面向哈希类型、集合类型、有序集合的扫描遍历命令，解决诸如hgetall、smembers、zrange可能产生的阻塞问题，对应的命令分别是hscan、sscan、zscan。<br>需要注意的是scan过程中如果有键的变化，那么可能会遍历重复的键，而遍历不到新增的键。</p></li></ul><h4 id="数据库管理"><a href="#数据库管理" class="headerlink" title="数据库管理"></a>数据库管理</h4><ul><li><p>切换数据库</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">select dbIndex</span><br></pre></td></tr></table></figure><p>Redis只用数字来标识数据库，默认有16个，各数据库独立。<br>但是该功能已逐步废弃了，原因如下：</p><ul><li>Redis是单线程的。如果使用多个数据库，那么这些数据库仍然是使用一个CPU，彼此之间还是会受到影响。</li><li>多数据库的使用方式，会让调试和运维不同业务的数据库变的困难，假如有一个慢查询存在，依然会影响其他数据库，这样会使得别的业务方定位问题非常的困难。</li><li>部分Redis的客户端根本就不支持这种方式。即使支持，在开发的时候来回切换数字形式的数据库，很容易弄乱。<br>所以在需要多个数据库功能的情况下，可以在一台机器上部署多个Redis实例，以端口来区分。</li></ul></li><li><p>flushdb和flushall<br>flushdb只清除当前数据库，flushall会清除所有数据库。<br>带来的问题：</p><ul><li>flushdb/flushall命令会将所有数据清除，一旦误操作后果不堪设想。</li><li>如果当前数据库键值数量比较多，flushdb/flushall存在阻塞Redis的可能性。</li></ul></li></ul><h2 id="Ch-3-小功能大用处"><a href="#Ch-3-小功能大用处" class="headerlink" title="Ch 3 小功能大用处"></a>Ch 3 小功能大用处</h2><h3 id="慢查询分析"><a href="#慢查询分析" class="headerlink" title="慢查询分析"></a>慢查询分析</h3><p>发送命令&gt;命令排队&gt;命令执行&gt;返回结果<br>慢查询只统计命令执行的时间，并不反映客户端的超时问题。</p><h4 id="两个配置参数"><a href="#两个配置参数" class="headerlink" title="两个配置参数"></a>两个配置参数</h4><ul><li>预设阈值<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">config set slowlog-log-slower-than 20000</span><br><span class="line">config rewrite  #将配置持久化到本地配置文件</span><br><span class="line"></span><br><span class="line">单位：微秒，默认值=10000</span><br><span class="line">等于0会记录所有记录，小于0不记录任何命令</span><br></pre></td></tr></table></figure></li><li>慢查询记录存放位置<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">config set slowlog-max-len 1000</span><br><span class="line">config rewrite  #将配置持久化到本地配置文件</span><br><span class="line"></span><br><span class="line">Redis使用一个列表来存储慢查询日志（类似队列），</span><br><span class="line">slowlog-max-len指定列表最大长度</span><br></pre></td></tr></table></figure></li></ul><h4 id="操作命令"><a href="#操作命令" class="headerlink" title="操作命令"></a>操作命令</h4><ul><li>获取慢查询日志<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">slowlog get [n]  #n可选，指定条数</span><br><span class="line"></span><br><span class="line">每个慢查询记录有4个属性，分别为：</span><br><span class="line">标识id、发生时间戳、命令耗时以及执行的命令和参数。</span><br></pre></td></tr></table></figure></li><li>获取慢查询日志列表当前长度<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">slowlog len</span><br></pre></td></tr></table></figure></li><li>慢查询日志重置<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">slowlog reset</span><br></pre></td></tr></table></figure></li></ul><h4 id="实践注意事项"><a href="#实践注意事项" class="headerlink" title="实践注意事项"></a>实践注意事项</h4><ul><li><strong>slowlog-max-len配置建议</strong>：线上建议调大慢查询列表，可设置为1000以上，记录慢查询时Redis会对长命令做截断操作，并不会占用大量内存，增大慢查询列表可以减缓慢查询被剔除的可能。</li><li><strong>slowlog-log-slower-than配置建议</strong>：默认值超过10毫秒判定为慢查询，需要根据Redis并发量调整该值。由于Redis采用单线程响应命令，对于高流量的场景，如果命令执行时间在1毫秒以上，那么Redis最多可支撑OPS不到1000，因此对于高OPS场景的Redis建议设置为1毫秒。</li><li><strong>慢查询只记录命令执行时间</strong>，并不包括命令排队和网络传输时间，因此客户端执行命令的时间会大于命令实际执行时间。因为命令执行排队机制，慢查询会导致其他命令级联阻塞，因此当客户端出现请求超时，需要检查该时间点是否有对应的慢查询，从而分析出是否为慢查询导致的命令级联阻塞。</li><li><strong>慢查询日志是一个先进先出的队列</strong>，如果慢查询比较多的情况下，可能会丢失部分慢查询命令，为了防止这种情况发生，可以定期执行slow get命令将慢查询日志持久化到其他存储中（例如MySQL），然后可以制作可视化界面进行查询，第13章介绍的Redis私有云CacheCloud提供了这样的功能，好的工具可以让问题排查事半功倍。</li></ul><h3 id="Redis-Shell"><a href="#Redis-Shell" class="headerlink" title="Redis Shell"></a>Redis Shell</h3><h4 id="redis-cli"><a href="#redis-cli" class="headerlink" title="redis-cli"></a>redis-cli</h4><ul><li>-r<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli -r 3 ping</span><br><span class="line"></span><br><span class="line">将ping命令执行3次</span><br></pre></td></tr></table></figure></li><li>-i（interval）<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli -r 3 -i 1 ping</span><br><span class="line"></span><br><span class="line">每隔几秒执行一次一次命令，单位：秒</span><br><span class="line">-i必须与-r一起用</span><br></pre></td></tr></table></figure></li><li>-x<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "world" | redis-cli -x set hello</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">-x选项代表从标准输入（stdin）读取数据作为redis-cli的最后一个参数。</span><br></pre></td></tr></table></figure></li><li>-c（cluster）<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">连接Redis Cluster节点时需要使用，-c选项可以防止moved和ask异常，有关Redis Cluster将在第10章介绍。</span><br></pre></td></tr></table></figure></li><li>-a（auth）<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">如果Redis配置了密码，可以用-a（auth）选项，有了这个选项就不需要手动输入auth命令。</span><br></pre></td></tr></table></figure></li><li>–scan和–pattern<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--scan选项和--pattern选项用于扫描指定模式的键，相当于使用scan命令。</span><br></pre></td></tr></table></figure></li><li>–slave<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--slave选项是把当前客户端模拟成当前Redis节点的从节点，可以用来获取当前Redis节点的更新操作，</span><br><span class="line">有关于Redis复制将在第6章进行详细介绍。</span><br><span class="line">合理的利用这个选项可以记录当前连接Redis节点的一些更新操作，</span><br><span class="line">这些更新操作很可能是实际开发业务时需要的数据。</span><br></pre></td></tr></table></figure></li><li>–rdb<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--rdb选项会请求Redis实例生成并发送RDB持久化文件，保存在本地。可使用它做持久化文件的定期备份。</span><br><span class="line">有关Redis持久化将在第5章进行详细介绍。</span><br></pre></td></tr></table></figure></li><li>–pipe<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--pipe选项用于将命令封装成Redis通信协议定义的数据格式，批量发送给Redis执行，</span><br><span class="line">有关Redis通信协议将在第4章进行详细介绍。</span><br></pre></td></tr></table></figure></li><li>–bigkeys<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--bigkeys选项使用scan命令对Redis的键进行采样，从中找到内存占用比较大的键值，这些键可能是系统的瓶颈。</span><br></pre></td></tr></table></figure></li><li>–eval<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--eval选项用于执行指定Lua脚本，有关Lua脚本的使用将在3.4节介绍。</span><br></pre></td></tr></table></figure></li><li>–latency<br>用于检测网络延迟，有三个选项<ul><li>–lantency<br>该选项可以测试客户端到目标Redis的网络延迟，<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli -h &#123;server&#125; --latency</span><br></pre></td></tr></table></figure></li><li>–latency-history</li><li>-latency的执行结果只有一条，如果想以分时段的形式了解延迟信息，可以使用–latency-history选项，可以通过-i参数控制间隔时间。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli -h 10.10.xx.xx --latency-history -i 10</span><br></pre></td></tr></table></figure></li><li>–latency-dist<br>该选项会使用统计图表的形式从控制台输出延迟统计信息。</li></ul></li><li>–stat<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--stat选项可以实时获取Redis的重要统计信息.</span><br></pre></td></tr></table></figure></li><li>–raw和–no-raw<br>–no-raw选项要求命令的返回结果必须是原始的格式，–raw返回格式化后的结果。</li></ul><h4 id="redis-server"><a href="#redis-server" class="headerlink" title="redis-server"></a>redis-server</h4><p>redis-server启动Redis：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">默认配置启动`redis-server`</span><br><span class="line">运行启动（加上需修改的参数）`redis-server --port 6380`</span><br><span class="line">配置文件启动`redis-sever /opt/redis/redis.conf`</span><br></pre></td></tr></table></figure><p>redis-server除了启动Redis外，还有一个–test-memory选项，可以用来检测当前操作系统能否稳定地分配指定容量的内存给Redis，通过这种检测可以有效避免因为内存问题造成Redis崩溃。<br>下面的命令检测当前操作系统能否提供1G的内存给Redis：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-server --test-memory 1024</span><br></pre></td></tr></table></figure><h4 id="redis-benchmark"><a href="#redis-benchmark" class="headerlink" title="redis-benchmark"></a>redis-benchmark</h4><p>redis-benchmark可以为Redis做基准性能测试。</p><ul><li>-c（clients）<br>-c选项代表客户端的并发数量（默认是50）。</li><li>-n（num）<br>-n选项代表客户端请求总量（默认是100000）。<br>示例：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-benchmark -c 100 -n 20000</span><br><span class="line"></span><br><span class="line">代表测试100个客户端同时请求Redis，共执行20000次时的性能</span><br><span class="line">output：</span><br><span class="line">====== GET ======</span><br><span class="line">20000 requests completed in 0.27 seconds</span><br><span class="line">100 parallel clients</span><br><span class="line">3 bytes payload</span><br><span class="line">keep alive: 1</span><br><span class="line"></span><br><span class="line">99.11% &lt;= 1 milliseconds</span><br><span class="line">100.00% &lt;= 1 milliseconds</span><br><span class="line">73529.41 requests per second</span><br></pre></td></tr></table></figure></li><li>-q<br>-q选项仅仅显示redis-benchmark的requests per second信息。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-benchmark -c 100 -n 20000 -q</span><br></pre></td></tr></table></figure></li><li>-r<br>在一个空的Redis上执行了redis-benchmark会发现只有3个键：<code>counter:__rand_int__</code>、<code>mylist</code>和<code>key:__rand_int__</code>。<br>如果想向Redis插入更多的键，可以执行使用-r（random）选项，可以向Redis插入更多随机的键。<br>-r选项会在key、counter键上加一个12位的后缀，-r 10000代表只对后四位做随机处理（-r不是随机数的个数）。</li><li>-P<br>-P选项代表每个请求pipeline的数据量（默认为1）。</li><li>-k<br>-k选项代表客户端是否使用keepalive，1为使用，0为不使用，默认值为1。</li><li>-t<br>-t选项可以对指定命令进行基准测试。<br>示例：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-benchmark -t get,set -q</span><br><span class="line">SET: 98619.32 requests per second</span><br><span class="line">GET: 97560.98 requests per second</span><br></pre></td></tr></table></figure></li><li>–csv<br>–csv选项会将结果按照csv格式输出，便于后续处理，如导出到Excel等。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-benchmark -t get,set --csv</span><br><span class="line">"SET","81300.81"</span><br><span class="line">"GET","79051.38"</span><br></pre></td></tr></table></figure></li></ul><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul><li>Redis客户端从发送命令到接收到返回结果的时间为一次RTT，Redis提供的批量操作命令能减少RTT，但大部分命令并不支持批量操作。</li><li>Pipeline（流水线）机制，能将一组Redis命令进行组装，通过一次RTT传输给Redis，再将这组Redis命令的执行结果按顺序返回给客户端。</li><li>redis-cli的–pipe选项实际上就是使用Pipeline机制，例如下面操作将set hello world和incr counter两条命令组装：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo -en '*3\r\n$3\r\nSET\r\n$5\r\nhello\r\n$5\r\nworld\r\n*2\r\n$4\r\nincr\r\</span><br><span class="line">    n$7\r\ncounter\r\n' | redis-cli --pipe</span><br></pre></td></tr></table></figure></li></ul><h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><ul><li>Pipeline执行速度一般比逐条执行要快。</li><li>客户端和服务端的网络延时越大，Pipeline的效果越明显。</li></ul><h4 id="原生批量命令与Pipeline对比"><a href="#原生批量命令与Pipeline对比" class="headerlink" title="原生批量命令与Pipeline对比"></a>原生批量命令与Pipeline对比</h4><ul><li>原生批量命令是原子的，Pipeline是非原子的。</li><li>原生批量命令是一个命令对应多个key，Pipeline支持多个命令。</li><li>原生批量命令是Redis服务端支持实现的，而Pipeline需要服务端和客户端的共同实现。</li></ul><h4 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h4><ul><li>每次Pipeline组装的命令个数不能没有节制，否则一次组装Pipeline数据量过大，一方面会增加客户端的等待时间，另一方面会造成一定的网络阻塞，可以将一次包含大量命令的Pipeline拆分成多次较小的Pipeline来完成。</li><li>Pipeline虽然只能操作一个Redis实例，但是即使在分布式Redis场景中，也可以作为批量操作的重要优化手段，具体细节见第11章。</li></ul><h3 id="事务与Lua"><a href="#事务与Lua" class="headerlink" title="事务与Lua"></a>事务与Lua</h3><p>为了保证多条命令组合的原子性，Redis提供了简单的事务功能以及集成Lua脚本来解决这个问题。</p><h4 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h4><p>将一组需一起执行的命令放在<code>multi</code>和<code>exec</code>之间，若要停止事务的执行，使用<code>discard</code>命令代替<code>exec</code>。<br>若事务中出现错误，Redis有不同的处理机制：</p><ul><li>命令错误<br>命令写错造成的语法错误，整个事务无法执行。</li><li>运行时错误<br>命令写错但仍是可执行的命令，<strong>Redis不支持回滚操作</strong>，只能自行修复。<br>有些应用场景需要在事务之前，确保事务中的key没有被其他客户端修改过，才执行事务，否则不执行（类似乐观锁，其他客户端的修改会执行）。Redis提供了watch命令来解决这类问题。</li></ul><h4 id="Lua用法简述"><a href="#Lua用法简述" class="headerlink" title="Lua用法简述"></a>Lua用法简述</h4><p>Lua语言在1993年由巴西一个大学研究小组发明，其设计目标是作为嵌入式程序移植到其他应用程序，由C语言实现，虽然简单小巧但是功能强大，许多应用都选用它作为脚本语言。官网：<a href="http://www.lua.org/" target="_blank" rel="noopener">http://www.lua.org/</a></p><ul><li><p>数据类型及其逻辑处理<br>Lua语言提供了如下几种数据类型：booleans（布尔）、numbers（数值）、strings（字符串）、tables（表格）。<br>Lua的基本数据类型和逻辑处理示例如下：</p><ul><li>字符串<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">local</span> strings val = <span class="string">"test"</span>  <span class="comment">--local代表局部变量</span></span><br></pre></td></tr></table></figure></li><li>数组<br>使用tables类型，数组下标从1开始。<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">local</span> tables myArray = &#123;<span class="string">"redis"</span>, <span class="string">"jedis"</span>, <span class="literal">true</span>, <span class="number">88.0</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(myArray[<span class="number">3</span>])</span><br><span class="line"><span class="comment">--true</span></span><br></pre></td></tr></table></figure><ul><li>for<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>, #myArray  <span class="comment">--#获取myArray（tables类型）长度</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">print</span>(myArray[i])</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>或是<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> index,value <span class="keyword">in</span> <span class="built_in">ipairs</span>(myArray)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">print</span>(index)</span><br><span class="line">  <span class="built_in">print</span>(value)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li>while<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">local</span> int sum = <span class="number">0</span></span><br><span class="line"><span class="keyword">local</span> int i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt;= <span class="number">100</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  sum = sum +i</span><br><span class="line">  i = i + <span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">print</span>(sum)</span><br><span class="line"><span class="comment">--输出结果为5050</span></span><br></pre></td></tr></table></figure></li><li>if else<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">  <span class="keyword">do</span> something</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="keyword">do</span> something</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li></ul></li><li>哈希<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">local</span> tables user_1 = &#123;age = <span class="number">28</span>, name = <span class="string">"tome"</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(user_1[<span class="string">"age"</span>])</span><br><span class="line"><span class="comment">--28</span></span><br><span class="line">```    </span><br><span class="line">遍历：</span><br><span class="line">```lua</span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> <span class="built_in">pairs</span>(user_1)</span><br><span class="line"><span class="keyword">do</span> <span class="built_in">print</span>(key .. value)  <span class="comment">--..用来连接字符串</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>函数定义</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">contact</span><span class="params">(str1, str2)</span></span></span><br><span class="line">  <span class="keyword">return</span> str1 .. str2</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="Redis与Lua"><a href="#Redis与Lua" class="headerlink" title="Redis与Lua"></a>Redis与Lua</h4><ul><li><p>在Redis中使用Lua<br>在Redis中执行Lua脚本有以下两种方法：</p><ul><li>eval<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">eval 脚本内容 key个数 key列表 参数列表</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; eval 'return "hello " .. KEYS[1] .. ARGV[1]' 1 redis world</span><br><span class="line">"hello redisworld"</span><br></pre></td></tr></table></figure>此外还可使用<code>redis-cli --eval</code>直接执行Lua文件。</li><li>evalsha<br>首先将Lua脚本加载到Redis服务端，得到该脚本的SHA1校验和，evalsha命令使用SHA1作为参数可以直接执行对应Lua脚本，避免每次发送Lua脚本的开销。这样客户端就不需要每次执行脚本内容，而脚本也会常驻在服务端，脚本功能得到了复用。<ul><li>加载脚本<br>script load命令可以将脚本内容加载到Redis内存中，得到SHA1<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli script load "$(cat lua_get.lua)"</span><br><span class="line">"7413dc2440db1fea7c0a0bde841fa68eefaf149c"</span><br></pre></td></tr></table></figure></li><li>执行脚本<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">evalsha 脚本SHA1值 key个数 key列表 参数列表</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; evalsha 7413dc2440db1fea7c0a0bde841fa68eefaf149c 1 redis world</span><br><span class="line">"hello redisworld"</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>Lua的RedisAPI</p><ul><li>Lua可以使用<code>redis.call</code>函数实现对Redis的访问，例如下面代码是Lua使用redis.call调用了Redis的set和get操作：<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">redis.call(<span class="string">"set"</span>, <span class="string">"hello"</span>, <span class="string">"world"</span>)</span><br><span class="line">redis.call(<span class="string">"get"</span>, <span class="string">"hello"</span>)</span><br></pre></td></tr></table></figure>放在Redis中的效果如下：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; eval 'return redis.call("get", KEYS[1])' 1 hello</span><br><span class="line">"world"</span><br></pre></td></tr></table></figure></li><li>如果<code>redis.call</code>执行失败，那么脚本执行结束会直接返回错误，而<code>redis.pcall</code>会忽略错误继续执行脚本，所以在实际开发中要根据具体的应用场景进行函数的选择。</li><li>Lua可以使用redis.log函数将Lua脚本的日志输出到Redis的日志文件中，但是一定要控制日志级别。Redis3.2提供了Lua Script Debugger功能用来调试复杂的Lua脚本，具体可以参考：<a href="http://redis.io/topics/ldb" target="_blank" rel="noopener">http://redis.io/topics/ldb</a>。</li></ul></li></ul><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p>Lua脚本功能为Redis开发和运维人员带来如下三个好处：</p><ul><li>Lua脚本在Redis中是原子执行的，执行过程中间不会插入其他命令。</li><li>Lua脚本可以帮助开发和运维人员创造出自己定制的命令，并可以将这些命令常驻在Redis内存中，实现复用的效果。</li><li>Lua脚本可以将多条命令一次性打包，有效地减少网络开销。</li></ul><p>示例：<br>当前列表记录着热门用户的id，假设这个列表有5个元素，如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; lrange hot:user:list 0 -1</span><br><span class="line">1) "user:1:ratio"</span><br><span class="line">2) "user:8:ratio"</span><br><span class="line">3) "user:3:ratio"</span><br><span class="line">4) "user:99:ratio"</span><br><span class="line">5) "user:72:ratio"</span><br></pre></td></tr></table></figure><p>user:{id}:ratio代表用户的热度，它本身又是一个字符串类型的键：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; mget user:1:ratio user:8:ratio user:3:ratio user:99:ratio</span><br><span class="line">user:72:ratio</span><br><span class="line">1) "986"</span><br><span class="line">2) "762"</span><br><span class="line">3) "556"</span><br><span class="line">4) "400"</span><br><span class="line">5) "101"</span><br></pre></td></tr></table></figure><p>现要求将列表内所有的键对应热度做加1操作，并且保证是原子执行，此功能可以利用Lua脚本来实现：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">local</span> mylist = redis.call(<span class="string">"lrange"</span>, KEYS[<span class="number">1</span>], <span class="number">0</span>, <span class="number">-1</span>)  <span class="comment">--将列表中所有元素取出， 赋值给mylist</span></span><br><span class="line"><span class="keyword">local</span> count = <span class="number">0</span> <span class="comment">--定义局部变量count=0，这个count就是最后incr的总次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--遍历mylist中所有元素，每次做完count自增，最后返回count</span></span><br><span class="line"><span class="keyword">for</span> index,key <span class="keyword">in</span> <span class="built_in">ipairs</span>(mylist)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  redis.call(<span class="string">"incr"</span>,key)</span><br><span class="line">  count = count + <span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">return</span> count</span><br></pre></td></tr></table></figure><p>将上述脚本写入lrange_and_mincr.lua文件中，并执行：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli --eval lrange_and_mincr.lua hot:user:list</span><br><span class="line">(integer) 5</span><br></pre></td></tr></table></figure><h4 id="Redis如何管理Lua脚本"><a href="#Redis如何管理Lua脚本" class="headerlink" title="Redis如何管理Lua脚本"></a>Redis如何管理Lua脚本</h4><ul><li>script load<br><code>script load script</code><br>此命令用于将Lua脚本加载到Redis内存中</li><li>script exists<br><code>scripts exists sha1 [sha1 …]</code><br>此命令用于判断sha1是否已经加载到Redis内存中：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; script exists a5260dd66ce02462c5b5231c727b3f7772c0bcc5</span><br><span class="line">1) (integer) 1</span><br></pre></td></tr></table></figure></li><li>script flush<br><code>script flush</code><br>此命令用于清除Redis内存已经加载的所有Lua脚本。</li><li>script kill<br><code>script kill</code><br>此命令用于杀掉正在执行的Lua脚本。<br>但是有一点需要注意，如果当前Lua脚本正在执行写操作，那么<code>script kill</code>将不会生效。<br>此时，可以执行<code>shutdown save|nosave</code>停掉Redis服务。</li></ul><h3 id="Bitmaps"><a href="#Bitmaps" class="headerlink" title="Bitmaps"></a>Bitmaps</h3><h4 id="数据结构模型"><a href="#数据结构模型" class="headerlink" title="数据结构模型"></a>数据结构模型</h4><p><img src="https://s3.ax1x.com/2021/01/10/s1V1QH.png" alt="字符串&quot;big&quot;用二进制表示"></p><ul><li>Bitmaps本身不是一种数据结构，实际上它就是字符串，但是它可以对字符串的位进行操作。</li><li>Bitmaps单独提供了一套命令，所以在Redis中使用Bitmaps和使用字符串的方法不太相同。可以把Bitmaps想象成一个以位为单位的数组，数组的每个单元只能存储0和1，数组的下标在Bitmaps中叫做偏移量。</li></ul><h4 id="命令-5"><a href="#命令-5" class="headerlink" title="命令"></a>命令</h4><p>本节将每个独立用户是否访问过网站存放在Bitmaps中，将访问的用户记做1，没有访问的用户记做0，用偏移量作为用户的id。</p><ul><li>设置值<br>设置键的第offset个位的值（从0算起）<br><code>setbit key offset value</code><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">setbit unique:users:2016-04-05 0 1</span><br></pre></td></tr></table></figure>很多应用的用户id以一个指定数字（例如10000）开头，直接将用户id和Bitmaps的偏移量对应势必会造成一定的浪费，通常的做法是每次做setbit操作时将用户id减去这个指定数字。在第一次初始化Bitmaps时，假如偏移量非常大，那么整个初始化过程执行会比较慢，可能会造成Redis的阻塞。</li><li>获取值<br><code>getbit key offset</code></li><li>获取Bitmaps指定范围值为1的个数<br><code>bitcount key [start end]</code><br>start和end代表起始和结束字节数</li><li>Bitmaps间的运算<br><code>bitop op destinatin_key key [key...]</code><br>op：and、or、not、xor（异或）。<br>结果保存在destinatin_key中。</li><li>计算Bitmaps中第一个值为targitBit的偏移量<br><code>bitpos key targetBit [start] [end]</code><br>计算第0个字节到第1个字节之间， 第一个值为0的偏移量:<br><code>bitpos unique:users:2016-04-04 0 0 1</code></li></ul><h4 id="BitMaps分析"><a href="#BitMaps分析" class="headerlink" title="BitMaps分析"></a>BitMaps分析</h4><ul><li>假设网站有1亿用户，每天独立访问的用户有5千万，如果每天用集合类型和Bitmaps分别存储活跃用户可以得到:<br><img src="https://s3.ax1x.com/2021/01/10/s1Znns.png" alt="set和Bitmaps存储一天活跃用户的对比"><br>随着时间推移节省的内存是非常可观的:<br><img src="https://s3.ax1x.com/2021/01/10/s1ZG3F.png" alt="set和Bitmaps存储独立用户空间对比"></li><li>但Bitmaps并不是万金油，假如该网站每天的独立访问用户很少，例如只有10万（大量的僵尸用户），那么两者的对比如图：<br><img src="https://s3.ax1x.com/2021/01/10/s1ZhUP.png" alt="set和Bitmaps存储一天活跃用户的对比（独立用户比较少）"></li></ul><h3 id="HyperLogLog"><a href="#HyperLogLog" class="headerlink" title="HyperLogLog"></a>HyperLogLog</h3><p>HyperLogLog并不是一种新的数据结构（实际类型为字符串类型），而是一种基数算法，通过HyperLogLog可以利用极小的内存空间完成独立总数的统计，数据集可以是IP、Email、ID等。HyperLogLog提供了3个命令：pfadd、pfcount、pfmerge。</p><ul><li>添加<br><code>pfadd key element [element...]</code></li><li>计算独立用户数<br><code>pfcount key [key...]</code><br>使用脚本向HyperLogLog插入100万个id，插入前记录一下info memory：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info memory</span><br><span class="line"><span class="meta">#</span><span class="bash"> Memory</span></span><br><span class="line">used_memory:835144</span><br><span class="line">used_memory_human:815.57K</span><br></pre></td></tr></table></figure>向2016_05_01:unique:ids插入100万个用户，每次插入1000条：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">elements=""</span><br><span class="line">key="2016_05_01:unique:ids"</span><br><span class="line">for i in `seq 1 1000000`</span><br><span class="line">do</span><br><span class="line">  elements="$&#123;elements&#125; uuid-"$&#123;i&#125;</span><br><span class="line">if [[ $((i%1000)) == 0 ]];</span><br><span class="line">then</span><br><span class="line">  redis-cli pfadd $&#123;key&#125; $&#123;elements&#125;</span><br><span class="line">  elements=""</span><br><span class="line">fi</span><br><span class="line">done</span><br></pre></td></tr></table></figure>当上述代码执行完成后，可以看到内存只增加了15K左右：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info memory</span><br><span class="line"><span class="meta">#</span><span class="bash"> Memory</span></span><br><span class="line">used_memory:850616</span><br><span class="line">used_memory_human:830.68K</span><br></pre></td></tr></table></figure>但同时可以看到pfcount的执行结果并不是100万：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; pfcount 2016_05_01:unique:ids</span><br><span class="line">(integer) 1009838</span><br></pre></td></tr></table></figure>可以对100万个uuid使用集合类型进行测试，代码如下：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">elements=""</span><br><span class="line">key="2016_05_01:unique:ids:set"</span><br><span class="line">for i in `seq 1 1000000`</span><br><span class="line">do</span><br><span class="line">  elements="$&#123;elements&#125; "$&#123;i&#125;</span><br><span class="line">if [[ $((i%1000)) == 0 ]];</span><br><span class="line">then</span><br><span class="line">  redis-cli sadd $&#123;key&#125; $&#123;elements&#125;</span><br><span class="line">  elements=""</span><br><span class="line">fi</span><br><span class="line">done</span><br></pre></td></tr></table></figure>可以看到内存使用了84MB：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info memory</span><br><span class="line"><span class="meta">#</span><span class="bash"> Memory</span></span><br><span class="line">used_memory:88702680</span><br><span class="line">used_memory_human:84.59M</span><br></pre></td></tr></table></figure>但独立用户数为100万：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; scard 2016_05_01:unique:ids:set</span><br><span class="line">(integer) 1000000</span><br></pre></td></tr></table></figure>HyperLogLog内存占用量小得惊人，但是用如此小空间来估算如此巨大的数据，必然不是100%的正确，其中一定存在误差率。Redis官方给出的数字是0.81%的失误率，因此选用原则主要为：<ul><li>只为了计算独立总数，不需要获取单条数据。</li><li>可以容忍一定误差率，毕竟HyperLogLog在内存的占用量上有很大的优势。</li></ul></li><li>合并<br>求并集。<br><code>pfmerge destinatin_key sourcekey [sourcekey ...]</code></li></ul><h3 id="发布订阅"><a href="#发布订阅" class="headerlink" title="发布订阅"></a>发布订阅</h3><p>Redis提供了基于“发布/订阅”模式的消息机制，此种模式下，消息发布者和订阅者不进行直接通信，发布者客户端向指定的频道（channel）发布消息， 订阅该频道的每个客户端都可以收到该消息，如图。<br><img src="https://s3.ax1x.com/2021/01/11/s3PLPe.png" alt="Redis发布订阅模型"></p><h4 id="命令-6"><a href="#命令-6" class="headerlink" title="命令"></a>命令</h4><ul><li>发布消息<br><code>publish channel message</code></li><li>订阅消息<br><code>subscribe channel [channel ...]</code><br>有关订阅命令有两点需要注意：<ul><li>客户端在执行订阅命令之后进入了订阅状态，只能接收subscribe、psubscribe、unsubscribe、punsubscribe的四个命令。</li><li>新开启的订阅客户端，无法收到该频道之前的消息，因为Redis不会对发布的消息进行持久化。</li></ul></li><li>取消订阅<br><code>unsubscribe channel [channel ...]</code></li><li>按照模式订阅和取消订阅<br>pattern支持glob风格。<br><code>psubscribe pattern [pattern ...]</code><br><code>punsubscribe [pattern [pattern ...]]</code></li><li>查询订阅<ul><li>查看活跃的频道<br>至少有一个订阅者的频道。<br><code>pubsub channels [pattern]</code></li><li>查看频道订阅数<br><code>pubsub numsub [channel ...]</code></li><li>查看模式订阅数<br>通过模式来订阅的客户端数。<br><code>pubsub numpat</code></li></ul></li></ul><h4 id="使用场景-6"><a href="#使用场景-6" class="headerlink" title="使用场景"></a>使用场景</h4><p>聊天室、公告牌、服务之间利用消息解耦都可以使用发布订阅模式。<br>如图，图中有两套业务，上面为视频管理系统，负责管理视频信息；下面为视频服务面向客户，用户可以通过各种客户端（手机、浏览器、接口）获取到视频信息。<br><img src="https://s3.ax1x.com/2021/01/11/s3ZWoq.png" alt="发布订阅用于视频信息变化通知"></p><ul><li>视频服务订阅video:changes频道如下：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">subscribe video:changes</span><br></pre></td></tr></table></figure></li><li>视频管理系统发布消息到video:changes频道如下：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">publish video:changes "video1,video3,video5"</span><br></pre></td></tr></table></figure></li><li>当视频服务收到消息，对视频信息进行更新，如下所示：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for video in video1,video3,video5</span><br><span class="line">  update &#123;video&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="GEO"><a href="#GEO" class="headerlink" title="GEO"></a>GEO</h3><p>Redis3.2版本提供了GEO（地理信息定位）功能，支持存储地理位置信息用来实现诸如附近位置、摇一摇这类依赖于地理位置信息的功能，<br>GEO功能是Redis的另一位作者Matt Stancliff借鉴NoSQL数据库Ardb实现的，Ardb的作者来自中国，它提供了优秀的GEO功能。</p><ul><li><p>增加地理位置信息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">geoadd key longitude latitude member [longitude latitude member ...]</span><br><span class="line"></span><br><span class="line">longitude、latitude、member分别是该地理位置的经度、纬度、成员：</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; geoadd cities:locations 116.28 39.55 beijing</span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure><p>如果需要更新地理位置信息，仍然可以使用geoadd命令，虽然返回结果为0。</p></li><li><p>获取地理位置信息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">geopos key member [member ...]</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; geopos cities:locations tianjin</span><br><span class="line">1) 1) "117.12000042200088501"</span><br><span class="line">2) "39.0800000535766543"</span><br></pre></td></tr></table></figure></li><li><p>获取两个地理位置的距离</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">geodist key member1 member2 [unit]</span><br><span class="line"></span><br><span class="line">unit代表返回结果的单位，包含四种：</span><br><span class="line">m（meters）代表米。</span><br><span class="line">km（kilometers）代表公里。</span><br><span class="line">mi（miles）代表英里。</span><br><span class="line">ft（ feet）代表尺。</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; geodist cities:locations tianjin beijing km</span><br><span class="line">"89.2061"</span><br></pre></td></tr></table></figure></li><li><p>获取指定位置范围内的地理信息位置集合</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">georadius key longitude latitude radiusm|km|ft|mi [withcoord] [withdist]</span><br><span class="line">  [withhash] [COUNT count] [asc|desc] [store key] [storedist key]</span><br><span class="line">georadiusbymember key member radiusm|km|ft|mi [withcoord] [withdist]</span><br><span class="line">  [withhash] [COUNT count] [asc|desc] [store key] [storedist key]</span><br></pre></td></tr></table></figure><p>georadius命令的中心位置给出了具体的经纬度，georadiusbymember只需给出成员即可。其中radiusm|km|ft|mi是必需参数，指定了半径（带单位）。剩下的参数如下：</p><ul><li>withcoord：返回结果中包含经纬度。</li><li>withdist：返回结果中包含离中心节点位置的距离。</li><li>withhash：返回结果中包含geohash，有关geohash后面介绍。</li><li>COUNT count：指定返回结果的数量。</li><li>asc|desc：返回结果按照离中心节点的距离做升序或者降序。</li><li>store key：将返回结果的地理位置信息保存到指定键。</li><li>storedist key：将返回结果离中心节点的距离保存到指定键。</li></ul><p>下面操作计算五座城市中， 距离北京150公里以内的城市：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; georadiusbymember cities:locations beijing 150 km</span><br><span class="line">1) "beijing"</span><br><span class="line">2) "tianjin"</span><br><span class="line">3) "tangshan"</span><br><span class="line">4) "baoding"</span><br></pre></td></tr></table></figure></li><li><p>获取geohash<br>Redis使用geohash将二维经纬度转换为一维字符串</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">geohash key member [member ...]</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; geohash cities:locations beijing</span><br><span class="line">1) "wx4ww02w070"</span><br></pre></td></tr></table></figure><p>geohash有如下特点：</p><ul><li>GEO的数据类型为zset，Redis将所有地理位置信息的geohash存放在zset中。</li><li>字符串越长，表示的位置更精确。<br><img src="https://s3.ax1x.com/2021/01/11/s31HXV.png" alt=""></li><li>两个字符串越相似，它们之间的距离越近，Redis利用字符串前缀匹配算法实现相关的命令。</li><li>geohash编码和经纬度是可以相互转换的。</li></ul></li><li><p>删除地理位置信息<br>GEO没有提供删除成员的命令，但是因为GEO的底层实现是zset，所以可以借用zrem命令实现对地理位置信息的删除。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zrem key member</span><br></pre></td></tr></table></figure></li></ul><h2 id="Ch-4-客户端"><a href="#Ch-4-客户端" class="headerlink" title="Ch 4 客户端"></a>Ch 4 客户端</h2><p>几乎所有的主流编程语言都有Redis的客户端<a href="http://redis.io/clients" target="_blank" rel="noopener">http://redis.io/clients</a>，不考虑Redis非常流行的原因，如果站在技术的角度看原因还有两个：</p><ul><li>客户端与服务端之间的通信协议是在TCP协议之上构建的。</li><li>Redis制定了RESP（REdis Serialization Protocol， Redis序列化协议）实现客户端与服务端的正常交互，这种协议简单高效 既能够被机器解析，又容易被人类识别。</li></ul><h3 id="客户端通信协议"><a href="#客户端通信协议" class="headerlink" title="客户端通信协议"></a>客户端通信协议</h3><h4 id="发送命令格式"><a href="#发送命令格式" class="headerlink" title="发送命令格式"></a>发送命令格式</h4><p>RESP规定一条命令的格式如下，CRLF代表”\r\n”。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">*&lt;参数数量&gt; CRLF</span><br><span class="line"><span class="meta">$</span><span class="bash">&lt;参数1的字节数量&gt; CRLF</span></span><br><span class="line">&lt;参数1&gt; CRLF</span><br><span class="line">...</span><br><span class="line"><span class="meta">$</span><span class="bash">&lt;参数N的字节数量&gt; CRLF</span></span><br><span class="line">&lt;参数N&gt; CRLF</span><br></pre></td></tr></table></figure><p>以<code>set hello world</code>为例：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">SET</span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">hello</span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">world</span><br></pre></td></tr></table></figure><p>实际传输格式为：<code>*3\r\n$3\r\nSET\r\n$5\r\nhello\r\n$5\r\nworld\r\n</code></p><h4 id="返回结果格式"><a href="#返回结果格式" class="headerlink" title="返回结果格式"></a>返回结果格式</h4><p>Redis的返回结果类型分为以下五种:</p><ul><li>状态回复<br>在RESP中第一个字节为”+”。</li><li>错误回复<br>在RESP中第一个字节为”-“。</li><li>整数回复<br>在RESP中第一个字节为”:”。</li><li>字符串回复<br>在RESP中第一个字节为”$”。</li><li>多条字符串回复<br>在RESP中第一个字节为”*”。</li></ul><p>redis-cli只能看到最终的执行结果，是因为redis-cli本身就是按照RESP进行结果解析的，所以看不到中间结果，redis-cli.c源码对命令结果的解析结构如下：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> sds <span class="title">cliFormatReplyTTY</span><span class="params">(redisReply *r, <span class="keyword">char</span> *prefix)</span> </span>&#123;</span><br><span class="line">sds out = sdsempty();</span><br><span class="line"><span class="keyword">switch</span> (r-&gt;type) &#123;</span><br><span class="line"><span class="keyword">case</span> REDIS_REPLY_ERROR:</span><br><span class="line"><span class="comment">// 处理错误回复</span></span><br><span class="line"><span class="keyword">case</span> REDIS_REPLY_STATUS:</span><br><span class="line"><span class="comment">// 处理状态回复</span></span><br><span class="line"><span class="keyword">case</span> REDIS_REPLY_INTEGER:</span><br><span class="line"><span class="comment">// 处理整数回复</span></span><br><span class="line"><span class="keyword">case</span> REDIS_REPLY_STRING:</span><br><span class="line"><span class="comment">// 处理字符串回复</span></span><br><span class="line"><span class="keyword">case</span> REDIS_REPLY_NIL:</span><br><span class="line"><span class="comment">// 处理空</span></span><br><span class="line"><span class="keyword">case</span> REDIS_REPLY_ARRAY:</span><br><span class="line"><span class="comment">// 处理多条字符串回复</span></span><br><span class="line"><span class="keyword">return</span> out;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如执行<code>set hello world</code>，返回结果是OK，并不能看到加号<code>+OK</code>。<br>为了看到Redis服务端返回的“真正”结果，可以使用nc命令、telnet命令、甚至写一个socket程序进行模拟。下面以nc命令进行演示：</p><ul><li>从源代码安装netcat<br>安装包：<a href="https://gitee.com/xxyrs/filehouse/raw/master/netcat-0.7.1.tar.gz" target="_blank" rel="noopener">netcat-0.7.1.tar.gz</a><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://sourceforge.net/projects/netcat/files/netcat/0.7.1/netcat-0.7.1.tar.gz</span><br><span class="line">tar -xzvf netcat-0.7.1.tar.gz</span><br><span class="line">cd netcat-0.7.1</span><br><span class="line">./configure</span><br><span class="line">sudo make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure></li><li>首先使用nc 127.0.0.1 6379连接到Redis：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nc 127.0.0.1 6379</span><br></pre></td></tr></table></figure></li><li>状态回复：<code>set hello world</code>的返回结果为+OK：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hello world</span><br><span class="line">+OK</span><br></pre></td></tr></table></figure></li><li>错误回复：由于sethx这条命令不存在，那么返回结果就是”-“号加上错误消息：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sethx</span><br><span class="line">-ERR unknown command 'sethx'</span><br></pre></td></tr></table></figure></li><li>整数回复：当命令的执行结果是整数时，返回结果就是整数回复，例如incr、exists、del、dbsize返回结果都是整数，例如执行<code>incr counter</code>返回结果就是“：”加上整数：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">incr counter</span><br><span class="line">:1</span><br></pre></td></tr></table></figure></li><li>字符串回复：当命令的执行结果是字符串时，返回结果就是字符串回复。get、hget返回结果都是字符串，例如<code>get hello</code>的结果为“$5\r\nworld\r\n”：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get hello</span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">world</span><br></pre></td></tr></table></figure></li><li>多条字符串回复：当命令的执行结果是多条字符串时，返回结果就是多条字符串回复。mget、hgetall、lrange等命令会返回多个结果，例如下面操作：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mset java jedis python redis-py</span><br><span class="line">+OK</span><br><span class="line"></span><br><span class="line">mget java python</span><br><span class="line">*2</span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">jedis</span><br><span class="line"><span class="meta">$</span><span class="bash">8</span></span><br><span class="line">redis-py</span><br></pre></td></tr></table></figure></li><li>注意，无论是字符串回复还是多条字符串回复，如果有nil值，那么会返回$-1。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get not_exist_key</span><br><span class="line"><span class="meta">$</span><span class="bash">-1</span></span><br></pre></td></tr></table></figure></li><li>如果批量操作中包含一条为nil值的结果，那么返回结果如下：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mget hello not_exist_key java</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">world</span><br><span class="line"><span class="meta">$</span><span class="bash">-1</span></span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">jedis</span><br></pre></td></tr></table></figure></li></ul><p>有了RESP提供的发送命令和返回结果的协议格式，各种编程语言就可以利用其来实现相应的Redis客户端。</p><h3 id="Java客户端Jedis"><a href="#Java客户端Jedis" class="headerlink" title="Java客户端Jedis"></a>Java客户端Jedis</h3><p>Java有很多优秀的Redis客户端，详见：<a href="http://redis.io/clients#java" target="_blank" rel="noopener">http://redis.io/clients#java</a></p><h4 id="获取Jedis"><a href="#获取Jedis" class="headerlink" title="获取Jedis"></a>获取Jedis</h4><h4 id="Jedis的基本使用"><a href="#Jedis的基本使用" class="headerlink" title="Jedis的基本使用"></a>Jedis的基本使用</h4><h4 id="Jedis连接池使用"><a href="#Jedis连接池使用" class="headerlink" title="Jedis连接池使用"></a>Jedis连接池使用</h4><h4 id="Jedis中的Pipeline使用"><a href="#Jedis中的Pipeline使用" class="headerlink" title="Jedis中的Pipeline使用"></a>Jedis中的Pipeline使用</h4><h4 id="Jedis的Lua脚本使用"><a href="#Jedis的Lua脚本使用" class="headerlink" title="Jedis的Lua脚本使用"></a>Jedis的Lua脚本使用</h4><h3 id="Python客户端redis-python"><a href="#Python客户端redis-python" class="headerlink" title="Python客户端redis-python"></a>Python客户端redis-python</h3><p>Redis官网提供了很多Python语言的客户端：<a href="http://redis.io/clients#python" target="_blank" rel="noopener">http://redis.io/clients#python</a></p><h4 id="获取redis-py"><a href="#获取redis-py" class="headerlink" title="获取redis-py"></a>获取redis-py</h4><ul><li>使用pip进行安装：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install redis</span><br></pre></td></tr></table></figure></li><li>使用easy_install进行安装：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">easy_install redis</span><br></pre></td></tr></table></figure></li><li>使用源码安装：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https:// github.com/andymccurdy/redis-py/archive/2.10.5.zip</span><br><span class="line">unzip redis-2.10.5.zip</span><br><span class="line">cd redis-2.10.5</span><br><span class="line"><span class="meta">#</span><span class="bash">安装redis-py</span></span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></li></ul><h4 id="redis-py基本使用方法"><a href="#redis-py基本使用方法" class="headerlink" title="redis-py基本使用方法"></a>redis-py基本使用方法</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line">client = redis.StrictRedis(host=<span class="string">'127.0.0.1'</span>, port=<span class="number">6379</span>)</span><br><span class="line">key = <span class="string">"hello"</span></span><br><span class="line">setResult = client.set(key, <span class="string">"python-redis"</span>)</span><br><span class="line">print(setResult)</span><br><span class="line">value = client.get(key)</span><br><span class="line">print(<span class="string">"key:"</span> + key + <span class="string">", value:"</span> + value)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="literal">True</span></span><br><span class="line">key:hello, value:python-redis</span><br></pre></td></tr></table></figure><p>下面代码给出redis-py操作Redis五种数据结构的示例:</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1.string</span></span><br><span class="line"><span class="comment">#输出结果： True</span></span><br><span class="line">client.set(<span class="string">"hello"</span>,<span class="string">"world"</span>)</span><br><span class="line"><span class="comment">#输出结果： world</span></span><br><span class="line">client.get(<span class="string">"hello"</span>)</span><br><span class="line"><span class="comment">#输出结果： 1</span></span><br><span class="line">client.incr(<span class="string">"counter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.hash</span></span><br><span class="line">client.hset(<span class="string">"myhash"</span>,<span class="string">"f1"</span>,<span class="string">"v1"</span>)</span><br><span class="line">client.hset(<span class="string">"myhash"</span>,<span class="string">"f2"</span>,<span class="string">"v2"</span>)</span><br><span class="line"><span class="comment">#输出结果： &#123;'f1': 'v1', 'f2': 'v2'&#125;</span></span><br><span class="line">client.hgetall(<span class="string">"myhash"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#3.list</span></span><br><span class="line">client.rpush(<span class="string">"mylist"</span>,<span class="string">"1"</span>)</span><br><span class="line">client.rpush(<span class="string">"mylist"</span>,<span class="string">"2"</span>)</span><br><span class="line">client.rpush(<span class="string">"mylist"</span>,<span class="string">"3"</span>)</span><br><span class="line"><span class="comment">#输出结果： ['1', '2', '3']</span></span><br><span class="line">client.lrange(<span class="string">"mylist"</span>, <span class="number">0</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.set</span></span><br><span class="line">client.sadd(<span class="string">"myset"</span>,<span class="string">"a"</span>)</span><br><span class="line">client.sadd(<span class="string">"myset"</span>,<span class="string">"b"</span>)</span><br><span class="line">client.sadd(<span class="string">"myset"</span>,<span class="string">"a"</span>)</span><br><span class="line"><span class="comment">#输出结果： set(['a', 'b'])</span></span><br><span class="line">client.smembers(<span class="string">"myset"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.zset</span></span><br><span class="line">client.zadd(<span class="string">"myzset"</span>,<span class="string">"99"</span>,<span class="string">"tom"</span>)</span><br><span class="line">client.zadd(<span class="string">"myzset"</span>,<span class="string">"66"</span>,<span class="string">"peter"</span>)</span><br><span class="line">client.zadd(<span class="string">"myzset"</span>,<span class="string">"33"</span>,<span class="string">"james"</span>)</span><br><span class="line"><span class="comment">#输出结果： [('james', 33.0), ('peter', 66.0), ('tom', 99.0)]</span></span><br><span class="line">client.zrange(<span class="string">"myzset"</span>, <span class="number">0</span>, <span class="number">-1</span>, withscores=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="redis-py的Pipline使用"><a href="#redis-py的Pipline使用" class="headerlink" title="redis-py的Pipline使用"></a>redis-py的Pipline使用</h4><p>用redis-py的Pipeline实现mdel功能：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mdel</span><span class="params">(keys)</span>:</span></span><br><span class="line">  client = redis.StrictRedis(host=<span class="string">'127.0.0.1'</span>, port=<span class="number">6379</span>)</span><br><span class="line">  pipeline = client.pipeline(transaction=<span class="literal">False</span>)   <span class="comment">#不使用事务</span></span><br><span class="line">  <span class="keyword">for</span> key <span class="keyword">in</span> keys:</span><br><span class="line">    <span class="keyword">print</span> pipeline.delete(key)</span><br><span class="line">  <span class="keyword">return</span> pipeline.execute()</span><br></pre></td></tr></table></figure><h4 id="redis-py的Lua脚本使用"><a href="#redis-py的Lua脚本使用" class="headerlink" title="redis-py的Lua脚本使用"></a>redis-py的Lua脚本使用</h4><p>redis-py提供了三个重要的函数实现Lua脚本的执行：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">eval(String script, int keyCount, String... params)</span><br><span class="line"></span><br><span class="line">script_load(String script)</span><br><span class="line"></span><br><span class="line">evalsha(String sha1, int keyCount, String... params)</span><br></pre></td></tr></table></figure><p>eval函数有三个参数，分别是：</p><ul><li>script：Lua脚本内容。</li><li>keyCount：键的个数。</li><li>params：相关参数KEYS和ARGV。</li></ul><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line">client = redis.StrictRedis(host=<span class="string">'127.0.0.1'</span>, port=<span class="number">6379</span>)</span><br><span class="line">script = <span class="string">"return redis.call('get',KEYS[1])"</span></span><br><span class="line"><span class="comment">#输出结果为world</span></span><br><span class="line"><span class="keyword">print</span> client.eval(script,<span class="number">1</span>,<span class="string">"hello"</span>)</span><br></pre></td></tr></table></figure><p>script_load和evalsha函数要一起使用，首先使用script_load将脚本加载到Redis中，evalsha函数用来执行脚本的哈希值，它需要三个参数：</p><ul><li>scriptSha：脚本的SHA1。</li><li>keyCount：键的个数。</li><li>params：相关参数KEYS和ARGV。</li></ul><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"></span><br><span class="line">client = redis.StrictRedis(host=<span class="string">'127.0.0.1'</span>, port=<span class="number">6379</span>)</span><br><span class="line">script = <span class="string">"return redis.call('get',KEYS[1])"</span></span><br><span class="line">scriptSha = client.script_load(script)</span><br><span class="line"><span class="keyword">print</span> client.evalsha(scriptSha, <span class="number">1</span>, <span class="string">"hello"</span>)</span><br></pre></td></tr></table></figure><h3 id="客户端管理"><a href="#客户端管理" class="headerlink" title="客户端管理"></a>客户端管理</h3><h4 id="客户端API"><a href="#客户端API" class="headerlink" title="客户端API"></a>客户端API</h4><ul><li><p>client list<br>client list命令能列出与Redis服务端相连的所有客户端连接信息。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; client list</span><br><span class="line">id=421 addr=127.0.0.1:54264 fd=8 name= </span><br><span class="line">age=6 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 </span><br><span class="line">argv-mem=10 obl=0 oll=0 omem=0 tot-mem=61466 events=r cmd=client user=default</span><br></pre></td></tr></table></figure><ul><li><p>标识：addr、id、fd、name<br>id：客户端连接的唯一标识，这个id是随着Redis的连接自增的，重启Redis后会重置为0。<br>addr：客户端连接的ip和端口。<br>fd：socket的文件描述符，与lsof命令结果中的fd是同一个，如果fd=-1代表当前客户端不是外部客户端，而是Redis内部的伪装客户端。<br>name：客户端的名字。</p></li><li><p>输入缓冲区：qbuf、qbuf-free<br>Redis为每个客户端分配了输入缓冲区，作用是将客户端发送的命令临时保存，同时Redis会从输入缓冲区拉取命令并执行。</p></li><li><p>输出缓冲区：obl、oll、omem</p></li><li><p>客户端存货状态：age、idle</p></li><li><p>客户端类型：flag</p></li><li><p>其他</p></li></ul></li><li><p>client setName和client getName</p></li><li><p>client kill</p></li><li><p>client pause</p></li></ul><h4 id="客户端相关配置"><a href="#客户端相关配置" class="headerlink" title="客户端相关配置"></a>客户端相关配置</h4><h4 id="客户端统计片段"><a href="#客户端统计片段" class="headerlink" title="客户端统计片段"></a>客户端统计片段</h4><h3 id="客户端常见异常"><a href="#客户端常见异常" class="headerlink" title="客户端常见异常"></a>客户端常见异常</h3><h4 id="无法从连接池获取到连接"><a href="#无法从连接池获取到连接" class="headerlink" title="无法从连接池获取到连接"></a>无法从连接池获取到连接</h4><h4 id="客户端读写超时"><a href="#客户端读写超时" class="headerlink" title="客户端读写超时"></a>客户端读写超时</h4><h4 id="客户端连接超时"><a href="#客户端连接超时" class="headerlink" title="客户端连接超时"></a>客户端连接超时</h4><h4 id="客户端缓冲区异常"><a href="#客户端缓冲区异常" class="headerlink" title="客户端缓冲区异常"></a>客户端缓冲区异常</h4><h4 id="Lua脚本正在执行"><a href="#Lua脚本正在执行" class="headerlink" title="Lua脚本正在执行"></a>Lua脚本正在执行</h4><h4 id="Redis正在加载持久化文件"><a href="#Redis正在加载持久化文件" class="headerlink" title="Redis正在加载持久化文件"></a>Redis正在加载持久化文件</h4><h4 id="Redis使用的内存超过maxmemory配置"><a href="#Redis使用的内存超过maxmemory配置" class="headerlink" title="Redis使用的内存超过maxmemory配置"></a>Redis使用的内存超过maxmemory配置</h4><h4 id="客户端连接数过大"><a href="#客户端连接数过大" class="headerlink" title="客户端连接数过大"></a>客户端连接数过大</h4><h3 id="客户端案例分析"><a href="#客户端案例分析" class="headerlink" title="客户端案例分析"></a>客户端案例分析</h3><h4 id="Redis内存陡增"><a href="#Redis内存陡增" class="headerlink" title="Redis内存陡增"></a>Redis内存陡增</h4><h4 id="客户端周期性超时"><a href="#客户端周期性超时" class="headerlink" title="客户端周期性超时"></a>客户端周期性超时</h4><h2 id="Ch-5-持久化"><a href="#Ch-5-持久化" class="headerlink" title="Ch 5 持久化"></a>Ch 5 持久化</h2><h3 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h3><p>RDB持久化是把当前进程数据生成快照保存到硬盘的过程，触发RDB持久化过程分为手动触发和自动触发。</p><h4 id="触发机制"><a href="#触发机制" class="headerlink" title="触发机制"></a>触发机制</h4><ul><li>手动触发<ul><li>save命令（已废弃）<br>save命令：阻塞当前Redis服务器，直到RDB过程完成为止，对于内存比较大的实例会造成长时间阻塞，线上环境不建议使用。<br>运行save命令对应的Redis日志为：<code>* DB saved on disk</code></li><li>bgsave命令<br>Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。<br>运行bgsave命令对应的Redis日志为：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">* Background saving started by pid 3151</span><br><span class="line">* DB saved on disk</span><br><span class="line">* RDB: 0 MB of memory used by copy-on-write</span><br><span class="line">* Background saving terminated with success</span><br></pre></td></tr></table></figure></li></ul></li><li>自动触发<ul><li>使用save相关配置，如<code>save m n</code>。表示m秒内数据集存在n次修改时，自动触发bgsave。</li><li>如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点，更多细节见Ch6介绍的复制原理。</li><li>执行debug reload命令重新加载Redis时，也会自动触发save操作。</li><li>默认情况下执行shutdown命令时，如果没有开启AOF持久化功能则自动执行bgsave。</li></ul></li></ul><h4 id="流程说明"><a href="#流程说明" class="headerlink" title="流程说明"></a>流程说明</h4><p><img src="https://s3.ax1x.com/2021/01/28/ySdX1U.png" alt="bgsave命令的运作流程"></p><ul><li>执行bgsave命令，Redis父进程判断当前是否存在正在执行的子进程，如RDB/AOF子进程，如果存在bgsave命令直接返回。</li><li>父进程执行fork操作创建子进程，fork操作过程中父进程会阻塞，通过info stats命令查看latest_fork_usec选项，可以获取最近一个fork操作的耗时，单位为微秒。</li><li>父进程fork完成后，bgsave命令返回“Background saving started”信息并不再阻塞父进程，可以继续响应其他命令。</li><li>子进程创建RDB文件，根据父进程内存生成临时快照文件，完成后对原有文件进行原子替换。执行lastsave命令可以获取最后一次生成RDB的时间，对应info统计的rdb_last_save_time选项。</li><li>进程发送信号给父进程表示完成，父进程更新统计信息，具体见info Persistence下的rdb_*相关选项。</li></ul><h4 id="RDB文件的处理"><a href="#RDB文件的处理" class="headerlink" title="RDB文件的处理"></a>RDB文件的处理</h4><ul><li>保存<br>RDB文件保存在dir配置指定的目录下，文件名通过dbfilename配置指定。可以通过执行<code>config set dir {newDir}</code>和<code>config set dbfilename {newFileName}</code>运行期动态执行，当下次运行时RDB文件会保存到新目录。<br><strong>注</strong>：当遇到坏盘或磁盘写满等情况时，可以通过<code>config set dir{newDir}</code>在线修改文件路径到可用的磁盘路径，之后执行bgsave进行磁盘切换，同样适用于AOF持久化文件。</li><li>压缩<br>Redis默认采用LZF算法对生成的RDB文件做压缩处理，压缩后的文件远远小于内存大小，默认开启，可以通过参数<code>config set rdbcompression {yes|no}</code>动态修改。<br><strong>注</strong>：虽然压缩RDB会消耗CPU，但可大幅降低文件的体积， 方便保存到硬盘或通过网络发送给从节点， 因此线上建议开启。</li><li>校验<br>如果Redis加载损坏的RDB文件时拒绝启动， 并打印如下日志：<br><code># Short read or OOM loading DB. Unrecoverable error, aborting now.</code><br>这时可以使用Redis提供的redis-check-dump工具检测RDB文件并获取对应的错误报告。</li></ul><h4 id="RDB的优缺点"><a href="#RDB的优缺点" class="headerlink" title="RDB的优缺点"></a>RDB的优缺点</h4><ul><li>RDB的优点：<ul><li>RDB是一个紧凑压缩的二进制文件，代表Redis在某个时间点上的数据快照。非常适用于备份，全量复制等场景。比如每6小时执行bgsave备份，并把RDB文件拷贝到远程机器或者文件系统中（如hdfs），用于灾难恢复。</li><li>Redis加载RDB恢复数据远远快于AOF的方式。</li></ul></li><li>RDB的缺点：<ul><li>RDB方式数据没办法做到实时持久化/秒级持久化。因为bgsave每次运行都要执行fork操作创建子进程，属于重量级操作，频繁执行成本过高。</li><li>RDB文件使用特定二进制格式保存，Redis版本演进过程中有多个格式的RDB版本，存在老版本Redis服务无法兼容新版RDB格式的问题。针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决。</li></ul></li></ul><h3 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h3><p>AOF（append only file）持久化：以独立日志的方式记录每次写命令，重启时再重新执行AOF文件中的命令达到恢复数据的目的。<br>AOF的主要作用是解决了<strong>数据持久化的实时性</strong>，目前已经是Redis持久化的主流方式。理解掌握好AOF持久化机制对我们兼顾数据安全性和性能非常有帮助。</p><h4 id="使用AOF"><a href="#使用AOF" class="headerlink" title="使用AOF"></a>使用AOF</h4><ul><li>开启AOF功能需要设置配置：<ul><li>appendonly yes，默认不开启。</li><li>AOF文件名通过appendfilename配置设置，默认文件名是appendonly.aof。</li><li>保存路径同RDB持久化方式一致，通过dir配置指定。</li></ul></li><li>AOF的工作流程操作：命令写入（append）、文件同步（sync）、文件重写（rewrite）、重启加载（load），如图所示：<br><img src="https://s3.ax1x.com/2021/01/28/ySDgbQ.png" alt="AOF工作流程"><ul><li>所有的写入命令会追加到aof_buf（缓冲区）中。</li><li>AOF缓冲区根据对应的策略向硬盘做同步操作。</li><li>随着AOF文件越来越大，需要定期对AOF文件进行重写，达到压缩的目的。</li><li>当Redis服务器重启时，可以加载AOF文件进行数据恢复。</li></ul></li></ul><h4 id="命令写入"><a href="#命令写入" class="headerlink" title="命令写入"></a>命令写入</h4><p>AOF命令写入的内容直接是文本协议格式，Redis协议格式具体说明见4.1客户端协议小节。</p><ul><li>AOF为什么直接采用文本协议格式？可能的理由如下：<ul><li>文本协议具有很好的兼容性。</li><li>开启AOF后，所有写入命令都包含追加操作，直接采用协议格式，避免了二次处理开销。</li><li>文本协议具有可读性，方便直接修改和处理。</li></ul></li><li>AOF为什么把命令追加到aof_buf中？<br>Redis使用单线程响应命令，如果每次写AOF文件命令都直接追加到硬盘，那么性能完全取决于当前硬盘负载。先写入缓冲区aof_buf中，还有另一个好处，Redis可以提供多种缓冲区同步硬盘的策略，在性能和安全性方面做出平衡。</li></ul><h4 id="文件同步"><a href="#文件同步" class="headerlink" title="文件同步"></a>文件同步</h4><p>Redis提供了多种AOF缓冲区同步文件策略，由参数appendfsync控制，不同值的含义如下：<br><img src="https://s3.ax1x.com/2021/01/28/y9AMEn.png" alt="AOF缓冲区同步文件策略"></p><ul><li>系统调用write和fsync说明：<ul><li>write操作会触发延迟写（delayed write）机制。Linux在内核提供页缓冲区用来提高硬盘IO性能。write操作在写入系统缓冲区后直接返回。同步硬盘操作依赖于系统调度机制，例如：缓冲区页空间写满或达到特定时间周期。同步文件之前，如果此时系统故障宕机，缓冲区内数据将丢失。</li><li>fsync针对单个文件操作（比如AOF文件），做强制硬盘同步，fsync将阻塞直到写入硬盘完成后返回，保证了数据持久化。</li><li>除了write、fsync，Linux还提供了sync、fdatasync操作，具体API说明参见：<br><a href="http://linux.die.net/man/2/write" target="_blank" rel="noopener">http://linux.die.net/man/2/write</a><br><a href="http://linux.die.net/man/2/fsync" target="_blank" rel="noopener">http://linux.die.net/man/2/fsync</a><br><a href="http://linux.die.net/man/2/fdatasync" target="_blank" rel="noopener">http://linux.die.net/man/2/fdatasync</a></li></ul></li><li>配置为always时，每次写入都要同步AOF文件，在一般的SATA硬盘上，Redis只能支持大约几百TPS写入，显然跟Redis高性能特性背道而驰，不建议配置。</li><li>配置为no，由于操作系统每次同步AOF文件的周期不可控，而且会加大每次同步硬盘的数据量，虽然提升了性能，但数据安全性无法保证。</li><li>配置为everysec，是建议的同步策略，也是默认配置，做到兼顾性能和数据安全性。理论上只有在系统突然宕机的情况下丢失1秒的数据。（严格来说最多丢失1秒数据是不准确的，5.3节会做具体介绍到。）</li></ul><h4 id="重写机制"><a href="#重写机制" class="headerlink" title="重写机制"></a>重写机制</h4><ul><li>随着命令不断写入AOF，文件会越来越大，为了解决这个问题，Redis引入AOF重写机制压缩文件体积。</li><li>AOF文件重写是把Redis进程内的数据转化为写命令同步到新AOF文件的过程。</li><li>重写后的AOF文件为什么可以变小？ 有如下原因：<ul><li>进程内已经超时的数据不再写入文件。</li><li>旧的AOF文件含有无效命令，如del key1、hdel key2、srem keys、set a 111、set a 222等。重写使用进程内数据直接生成，这样新的AOF文件只保留最终数据的写入命令。</li><li>多条写命令可以合并为一个，如：lpush list a、lpush list b、lpush list c可以转化为：lpush list a b c。为了防止单条命令过大造成客户端缓冲区溢出，对于list、set、hash、zset等类型操作，以64个元素为界拆分为多条。AOF重写降低了文件占用空间，除此之外，另一个目的是：更小的AOF文件可以更快地被Redis加载。</li></ul></li><li>AOF重写过程可以手动触发和自动触发：<ul><li>手动触发：直接调用bgrewriteaof命令。</li><li>自动触发：根据auto-aof-rewrite-min-size和auto-aof-rewrite-percentage参数确定自动触发时机。<ul><li>auto-aof-rewrite-min-size：表示运行AOF重写时文件最小体积，默认为64MB。</li><li>auto-aof-rewrite-percentage：代表当前AOF文件空间（aof_current_size）和上一次重写后AOF文件空间（ aof_base_size）的比值。</li><li>自动触发时机 = aof_current_size &gt; auto-aof-rewrite-minsize &amp;&amp;<br>(aof_current_size-aof_base_size) / aof_base_size &gt;= auto-aof-rewritepercentage<br>其中aof_current_size和aof_base_size可以在info Persistence统计信息中查看。</li></ul></li></ul></li><li>当触发AOF重写时，内部的运行流程如下：<br><img src="https://s3.ax1x.com/2021/01/28/y9MrG9.png" alt="AOF重写运作流程"></li><li>执行AOF重写请求。<br>如果当前进程正在执行AOF重写，请求不执行并返回如下响应：ERR Background append only file rewriting already in progress<br>如果当前进程正在执行bgsave操作，重写命令延迟到bgsave完成之后再执行，返回如下响应：Background append only file rewriting scheduled</li><li>父进程执行fork创建子进程，开销等同于bgsave过程。</li><li>主进程fork操作完成后，继续响应其他命令。所有修改命令依然写入AOF缓冲区并根据appendfsync策略同步到硬盘，保证原有AOF机制正确性。</li><li>由于fork操作运用写时复制技术，子进程只能共享fork操作时的内存数据。由于父进程依然响应命令 Redis使用“AOF重写缓冲区”保存这部分新数据，防止新AOF文件生成期间丢失这部分数据。</li><li>子进程根据内存快照，按照命令合并规则写入到新的AOF文件。每次批量写入硬盘数据量由配置aof-rewrite-incremental-fsync控制，默认为32MB，防止单次刷盘数据过多造成硬盘阻塞。</li><li>新AOF文件写入完成后，子进程发送信号给父进程，父进程更新统计信息，具体见info persistence下的aof_*相关统计。</li><li>父进程把AOF重写缓冲区的数据写入到新的AOF文件。</li><li>使用新AOF文件替换老文件，完成AOF重写。</li></ul><h4 id="重启加载"><a href="#重启加载" class="headerlink" title="重启加载"></a>重启加载</h4><ul><li>AOF和RDB文件都可以用于服务器重启时的数据恢复。Redis持久化文件加载流程如图。<br><img src="https://s3.ax1x.com/2021/01/28/y9l5jI.png" alt="Redis持久化文件加载流程"></li><li>流程说明：<ul><li>AOF持久化开启且存在AOF文件时，优先加载AOF文件，打印如下日志：<br><code>* DB loaded from append only file: 5.841 seconds</code></li><li>AOF关闭或者AOF文件不存在时，加载RDB文件，打印如下日志：<br><code>* DB loaded from disk: 5.586 seconds</code></li><li>加载AOF/RDB文件成功后，Redis启动成功。</li><li>AOF/RDB文件存在错误时，Redis启动失败并打印错误信息。</li></ul></li></ul><h4 id="文件校验"><a href="#文件校验" class="headerlink" title="文件校验"></a>文件校验</h4><p>加载损坏的AOF文件时会拒绝启动，并打印如下日志：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Bad file format reading the append only file: make a backup of your AOF file,</span></span><br><span class="line">then use ./redis-check-aof --fix &lt;filename&gt;</span><br></pre></td></tr></table></figure><p><strong>注</strong>：对于错误格式的AOF文件，先进行备份，然后采用redis-check-aof–fix命令进行修复，修复后使用diff -u对比数据的差异，找出丢失的数据，有些可以人工修改补全。<br>AOF文件可能存在结尾不完整的情况，比如机器突然断电导致AOF尾部文件命令写入不全。Redis为提供了aof-load-truncated配置来兼容这种情况，默认开启。加载AOF时，当遇到此问题时会忽略并继续启动，同时打印如下警告日志：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> !!! Warning: short <span class="built_in">read</span> <span class="keyword">while</span> loading the AOF file !!!</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> !!! Truncating the AOF at offset 397856725 !!!</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> AOF loaded anyway because aof-load-truncated is enabled</span></span><br></pre></td></tr></table></figure><h3 id="问题定位与优化"><a href="#问题定位与优化" class="headerlink" title="问题定位与优化"></a>问题定位与优化</h3><h4 id="fork操作"><a href="#fork操作" class="headerlink" title="fork操作"></a>fork操作</h4><ul><li>当Redis做RDB或AOF重写时，一个必不可少的操作就是执行fork操作创建子进程，对于大多数操作系统来说fork是个重量级操作。虽然fork创建的子进程不需要拷贝父进程的物理内存空间，但是会复制父进程的空间内存页表。例如对于10GB的Redis进程，需要复制大约20MB的内存页表，因此fork操作耗时跟进程总内存量息息相关，如果使用虚拟化技术，特别是Xen虚拟机，fork操作会更耗时。</li><li><strong>fork耗时问题定位</strong>：对于高流量的Redis实例OPS可达5万以上，如果fork操作耗时在秒级别将拖慢Redis几万条命令执行，对线上应用延迟影响非常明显。正常情况下fork耗时应该是每GB消耗20毫秒左右。可以在info stats统计中查latest_fork_usec指标获取最近一次fork操作耗时，单位微秒。</li><li>如何改善fork操作的耗时：<ul><li>优先使用物理机或者高效支持fork操作的虚拟化技术，避免使用Xen。</li><li>控制Redis实例最大可用内存，fork耗时跟内存量成正比，线上建议每个Redis实例内存控制在10GB以内。</li><li>合理配置Linux内存分配策略，避免物理内存不足导致fork失败，具体细节见12.1节“Linux配置优化”。</li><li>降低fork操作的频率，如适度放宽AOF自动触发时机，避免不必要的全量复制等。</li></ul></li></ul><h4 id="子进程开销监控与优化"><a href="#子进程开销监控与优化" class="headerlink" title="子进程开销监控与优化"></a>子进程开销监控与优化</h4><p>子进程负责AOF或者RDB文件的重写，它的运行过程主要涉及CPU、内存、硬盘三部分的消耗。</p><ul><li>CPU<ul><li>CPU开销分析。<br>子进程负责把进程内的数据分批写入文件，这个过程属于CPU密集操作，通常子进程对单核CPU利用率接近90%。</li><li>CPU消耗优化。<br>Redis是CPU密集型服务，不要做绑定单核CPU操作。由于子进程非常消耗CPU，会和父进程产生单核资源竞争。</li><li>不要和其他CPU密集型服务部署在一起，造成CPU过度竞争。</li><li>如果部署多个Redis实例，尽量保证同一时刻只有一个子进程执行重写工作，具体细节见5.4节“多实例部署”。</li></ul></li><li>内存<ul><li>内存消耗分析。<br>子进程通过fork操作产生，占用内存大小等同于父进程，理论上需要两倍的内存来完成持久化操作，但Linux有写时复制机制（copy-on-write）。<br>父子进程会共享相同的物理内存页，当父进程处理写请求时会把要修改的页创建副本，而子进程在fork操作过程中共享整个父进程内存快照。</li><li>内存消耗监控。<br>RDB重写时，Redis日志输出容如下：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">* Background saving started by pid 7692</span><br><span class="line">* DB saved on disk</span><br><span class="line">* RDB: 5 MB of memory used by copy-on-write</span><br><span class="line">* Background saving terminated with success</span><br></pre></td></tr></table></figure>如果重写过程中存在内存修改操作，父进程负责创建所修改内存页的副本，从日志中可以看出这部分内存消耗了5MB，可以等价认为RDB重写消耗了5MB的内存。<br>AOF重写时，Redis日志输出容如下：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">* Background append only file rewriting started by pid 8937</span><br><span class="line">* AOF rewrite child asks to stop sending diffs.</span><br><span class="line">* Parent agreed to stop sending diffs. Finalizing AOF...</span><br><span class="line">* Concatenating 0.00 MB of AOF diff received from parent.</span><br><span class="line">* SYNC append only file rewrite performed</span><br><span class="line">* AOF rewrite: 53 MB of memory used by copy-on-write</span><br><span class="line">* Background AOF rewrite terminated with success</span><br><span class="line">* Residual parent diff successfully flushed to the rewritten AOF (1.49 MB)</span><br><span class="line">* Background AOF rewrite finished successfully</span><br></pre></td></tr></table></figure>父进程维护页副本消耗同RDB重写过程类似，不同之处在于AOF重写需要AOF重写缓冲区，因此根据以上日志可以预估内存消耗为：53MB+1.49MB，也就是AOF重写时子进程消耗的内存量。<br><strong>注</strong>：编写shell脚本根据Redis日志可快速定位子进程重写期间内存过度消耗情况。</li><li>内存消耗优化：<ul><li>同CPU优化一样，如果部署多个Redis实例，尽量保证同一时刻只有一个子进程在工作。</li><li>避免在大量写入时做子进程重写操作，这样将导致父进程维护大量页副本，造成内存消耗。</li></ul></li><li>Linux kernel在2.6.38内核增加了Transparent Huge Pages（THP），支持huge page（2MB）的页分配，默认开启。当开启时可以降低fork创建子进程的速度，但执行fork之后，如果开启THP，复制页单位从原来4KB变为2MB，会大幅增加重写期间父进程内存消耗。建议设置<code>sudo echonever&gt;/sys/kernel/mm/transparent_hugepage/enabled</code>关闭THP。更多THP细节和配置见12.1节Linux配置优化</li></ul></li><li>硬盘<ul><li>硬盘开销分析。<br>子进程主要职责是把AOF或者RDB文件写入硬盘持久化。 势必造成硬盘写入压力。 根据Redis重写AOF/RDB的数据量， 结合系统工具如sar、 iostat、 iotop等， 可分析出重写期间硬盘负载情况。</li><li>硬盘开销优化。<ul><li>不要和其他高硬盘负载的服务部署在一起。如：存储服务、消息队列服务等。</li><li>AOF重写时会消耗大量硬盘IO，可以开启配置no-appendfsync-onrewrite，默认关闭。表示在AOF重写期间不做fsync操作。</li><li>当开启AOF功能的Redis用于高流量写入场景时，如果使用普通机械磁盘，写入吞吐一般在100MB/s左右，这时Redis实例的瓶颈主要在AOF同步硬盘上。</li><li>对于单机配置多个Redis实例的情况，可以配置不同实例分盘存储AOF文件，分摊硬盘写入压力。</li></ul></li><li><strong>注</strong>：配置no-appendfsync-on-rewrite=yes时，在极端情况下可能丢失整个AOF重写期间的数据，需要根据数据安全性决定是否配置。</li></ul></li></ul><h4 id="AOF追加阻塞"><a href="#AOF追加阻塞" class="headerlink" title="AOF追加阻塞"></a>AOF追加阻塞</h4><ul><li>当开启AOF持久化时，常用的同步硬盘的策略是everysec，用于平衡性能和数据安全性。对于这种方式，Redis使用另一条线程每秒执行fsync同步硬盘。当系统硬盘资源繁忙时，会造成Redis主线程阻塞，如图所示。<br><img src="https://s3.ax1x.com/2021/01/28/y9q2CR.png" alt="使用everysec做刷盘策略的流程"></li><li>阻塞流程分析：<ul><li>主线程负责写入AOF缓冲区。</li><li>AOF线程负责每秒执行一次同步磁盘操作，并记录最近一次同步时间。</li><li>主线程负责对比上次AOF同步时间：<ul><li>如果距上次同步成功时间在2秒内，主线程直接返回。</li><li>如果距上次同步成功时间超过2秒，主线程将会阻塞，直到同步操作完成。</li></ul></li></ul></li><li>通过对AOF阻塞流程可以发现两个问题：<ul><li>everysec配置最多可能丢失2秒数据，不是1秒。</li><li>如果系统fsync缓慢，将会导致Redis主线程阻塞影响效率。</li></ul></li><li>AOF阻塞问题定位：<ul><li>发生AOF阻塞时，Redis输出如下日志，用于记录AOF fsync阻塞导致拖慢Redis服务的行为：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Asynchronous AOF fsync is taking too long (disk is busy). Writing the AOF buffer</span><br><span class="line">without waiting for fsync to complete, this may slow down Redis</span><br></pre></td></tr></table></figure></li><li>每当发生AOF追加阻塞事件发生时，在info Persistence统计中，aof_delayed_fsync指标会累加，查看这个指标方便定位AOF阻塞问题。</li><li>AOF同步最多允许2秒的延迟，当延迟发生时说明硬盘存在高负载问题，可以通过监控工具如iotop，定位消耗硬盘IO资源的进程。优化AOF追加阻塞问题主要是优化系统硬盘负载，优化方式见上一节。</li></ul></li></ul><h3 id="多实例部署"><a href="#多实例部署" class="headerlink" title="多实例部署"></a>多实例部署</h3><p>Redis单线程架构导致无法充分利用CPU多核特性，通常的做法是在一台机器上部署多个Redis实例。当多个实例开启AOF重写后，彼此之间会产生对CPU和IO的竞争。本节主要介绍针对这种场景的分析和优化。</p><ul><li>对于单机多Redis部署，如果同一时刻运行多个子进程，对当前系统影响将非常明显，因此需要采用一种措施，把子进程工作进行隔离。Redis在info Persistence中为我们提供了监控子进程运行状况的度量指标，如表所示。<br><img src="https://s3.ax1x.com/2021/01/28/y9LGM6.png" alt="info Persistence片段度量指标"><br>我们基于以上指标，可以通过外部程序轮询控制AOF重写操作的执行，整个过程如图所示。<br><img src="https://s3.ax1x.com/2021/01/28/y9LYqO.png" alt="轮询控制AOF重写"></li><li>流程说明：<ul><li>外部程序定时轮询监控机器（machine）上所有Redis实例。</li><li>对于开启AOF的实例，查看（aof_current_sizeaof_base_size） / aof_base_size确认增长率。</li><li>当增长率超过特定阈值（如100%），执行bgrewriteaof命令手动触发当前实例的AOF重写。</li><li>运行期间循环检查aof_rewrite_in_progress和aof_current_rewrite_time_sec指标，直到AOF重写结束。</li><li>确认实例AOF重写完成后，再检查其他实例并重复第2步到第4步操作。从而保证机器内每个Redis实例AOF重写串行化执行。</li></ul></li></ul><h2 id="Ch-6-复制"><a href="#Ch-6-复制" class="headerlink" title="Ch 6 复制"></a>Ch 6 复制</h2><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><h3 id="拓扑"><a href="#拓扑" class="headerlink" title="拓扑"></a>拓扑</h3><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><h3 id="开发与运维中的问题"><a href="#开发与运维中的问题" class="headerlink" title="开发与运维中的问题"></a>开发与运维中的问题</h3><h2 id="Ch-7-阻塞"><a href="#Ch-7-阻塞" class="headerlink" title="Ch 7 阻塞"></a>Ch 7 阻塞</h2><h3 id="发现阻塞"><a href="#发现阻塞" class="headerlink" title="发现阻塞"></a>发现阻塞</h3><h3 id="内在原因"><a href="#内在原因" class="headerlink" title="内在原因"></a>内在原因</h3><h3 id="外在原因"><a href="#外在原因" class="headerlink" title="外在原因"></a>外在原因</h3><h2 id="Ch-8-理解内存"><a href="#Ch-8-理解内存" class="headerlink" title="Ch 8 理解内存"></a>Ch 8 理解内存</h2><h3 id="内存消耗"><a href="#内存消耗" class="headerlink" title="内存消耗"></a>内存消耗</h3><h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><h3 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h3><h2 id="Ch-11-缓存设计"><a href="#Ch-11-缓存设计" class="headerlink" title="Ch 11 缓存设计"></a>Ch 11 缓存设计</h2><h3 id="缓存的收益和成本"><a href="#缓存的收益和成本" class="headerlink" title="缓存的收益和成本"></a>缓存的收益和成本</h3><p><img src="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210209-104448-0194.png" alt="缓存层+存储层基本流程"><br>图左侧为客户端直接调用存储层的架构，右侧为比较典型的缓存层+存储层架构，下面分析一下缓存加入后带来的收益和成本。</p><p>收益如下：</p><ul><li>加速读写：因为缓存通常都是全内存的（例如Redis、 Memcache），而存储层通常读写性能不够强悍（例如MySQL），通过缓存的使用可以有效<br>地加速读写，优化用户体验。</li><li>降低后端负载：帮助后端减少访问量和复杂计算（例如很复杂的SQL语句），在很大程度降低了后端的负载。</li></ul><p>成本如下：</p><ul><li>数据不一致性：缓存层和存储层的数据存在着一定时间窗口的不一致性，时间窗口跟更新策略有关。</li><li>代码维护成本：加入缓存后，需要同时处理缓存层和存储层的逻辑，增大了开发者维护代码的成本。</li><li>运维成本：以Redis Cluster为例，加入后无形中增加了运维成本。</li></ul><p>缓存的使用场景基本包含如下两种：</p><ul><li>开销大的复杂计算：以MySQL为例子，一些复杂的操作或者计算（例如大量联表操作、一些分组计算），如果不加缓存，不但无法满足高并发<br>量，同时也会给MySQL带来巨大的负担。</li><li>加速请求响应：即使查询单条后端数据足够快（例如select*from table where id=?），那么依然可以使用缓存，以Redis为例子，每秒可以完成数万次读写，并且提供的批量操作可以优化整个IO链的响应时间。</li></ul><h3 id="缓存更新策略"><a href="#缓存更新策略" class="headerlink" title="缓存更新策略"></a>缓存更新策略</h3><p>缓存中的数据通常都是有生命周期的，需要在指定时间后被删除或更新，这样可以保证缓存空间在一个可控的范围。但是缓存中的数据会和数据源中的真实数据有一段时间窗口的不一致，需要利用某些策略进行更新。</p><p>下面将分别从使用场景、一致性、开发人员开发/维护成本三个方面介绍三种缓存的更新策略。</p><h4 id="LRU-LFU-FIFO算法剔除"><a href="#LRU-LFU-FIFO算法剔除" class="headerlink" title="LRU/LFU/FIFO算法剔除"></a>LRU/LFU/FIFO算法剔除</h4><ul><li>使用场景。剔除算法通常用于缓存使用量超过了预设的最大值时候，如何对现有的数据进行剔除。例如Redis使用maxmemory-policy这个配置作为内存最大值后对于数据的剔除策略。</li><li>一致性。要清理哪些数据是由具体算法决定，开发人员只能决定使用哪种算法，所以数据的一致性是最差的。</li><li>维护成本。 算法不需要开发人员自己来实现，通常只需要配置最大maxmemory和对应的策略即可。开发人员只需要知道每种算法的含义，选择适合自己的算法即可。</li></ul><h4 id="超时剔除"><a href="#超时剔除" class="headerlink" title="超时剔除"></a>超时剔除</h4><ul><li>使用场景。超时剔除通过给缓存数据设置过期时间，让其在过期时间后自动删除，例如Redis提供的expire命令。如果业务可以容忍一段时间内，缓存层数据和存储层数据不一致，那么可以为其设置过期时间。在数据过期后，再从真实数据源获取数据，重新放到缓存并设置过期间。例如一个视频的描述信息，可以容忍几分钟内数据不一致，但是涉及交易方面的业务，后果可想而知。</li><li>一致性。一段时间窗口内（取决于过期时间长短）存在一致性问题，即缓存数据和真实数据源的数据不一致。</li><li>维护成本。维护成本不是很高，只需设置expire过期时间即可，当然前提是应用方允许这段时间可能发生的数据不一致。</li></ul><h4 id="主动更新"><a href="#主动更新" class="headerlink" title="主动更新"></a>主动更新</h4><ul><li>使用场景。应用方对于数据的一致性要求高，需要在真实数据更新后，立即更新缓存数据。例如可以利用消息系统或者其他方式通知缓存更新。</li><li>一致性。一致性最高，但如果主动更新发生了问题，那么这条数据很可能很长时间不会更新，所以建议结合超时剔除一起使用效果会更好。</li><li>维护成本。维护成本会比较高，开发者需要自己来完成更新，并保证更新操作的正确性。</li></ul><h4 id="最佳实践-1"><a href="#最佳实践-1" class="headerlink" title="最佳实践"></a>最佳实践</h4><p>有两个建议：</p><ul><li>低一致性业务建议配置最大内存和淘汰策略的方式使用。</li><li>高一致性业务可以结合使用超时剔除和主动更新，这样即使主动更新出了问题，也能保证数据过期时间后删除脏数据。</li></ul><h3 id="缓存粒度控制"><a href="#缓存粒度控制" class="headerlink" title="缓存粒度控制"></a>缓存粒度控制</h3><p>缓存层选用Redis，存储层选用MySQL，是很多项目关于缓存比较常用的选型，如图：<br><img src="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210209-131625-0818.png" alt="Redis+MySQL架构"></p><p>例如现在需要将MySQL的用户信息使用Redis缓存，可以执行如下操作：<br>从MySQL获取用户信息：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">id</span>=&#123;<span class="keyword">id</span>&#125;</span><br></pre></td></tr></table></figure><p>将用户信息缓存到Redis中：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set user:&#123;id&#125; 'select * from user where id=&#123;id&#125;'</span><br></pre></td></tr></table></figure><p>假设用户表有100个列， 需要缓存到什么维度呢？</p><ul><li>缓存全部列：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set user:&#123;id&#125; 'select * from user where id=&#123;id&#125;'</span><br></pre></td></tr></table></figure></li><li>缓存部分重要列：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set user:&#123;id&#125; 'select &#123;importantColumn1&#125;, &#123;important Column2&#125; ... &#123;importantColumnN&#125;</span><br><span class="line">                from user where id=&#123;id&#125;'</span><br></pre></td></tr></table></figure></li></ul><p>上述这个问题就是缓存粒度问题，究竟是缓存全部属性还是只缓存部分重要属性呢？下面将从通用性、空间占用、代码维护三个角度进行说明。</p><ul><li>通用性。缓存全部数据比部分数据更加通用，但从实际经验看，很长时间内应用只需要几个重要的属性。</li><li>空间占用。缓存全部数据要比部分数据占用更多的空间，可能存在以下问题：<ul><li>全部数据会造成内存的浪费。</li><li>全部数据可能每次传输产生的网络流量会比较大，耗时相对较大，在极端情况下会阻塞网络。</li><li>全部数据的序列化和反序列化的CPU开销更大。</li></ul></li><li>代码维护。全部数据的优势更加明显，而部分数据一旦要加新字段需要修改业务代码，而且修改后通常还需要刷新缓存数据。</li></ul><p>缓存粒度问题是一个容易被忽视的问题，如果使用不当，可能会造成很多无用空间的浪费，网络带宽的浪费，代码通用性较差等情况，需要综合数据通用性、空间占用比、代码维护性三点进行取舍。</p><h3 id="穿透优化"><a href="#穿透优化" class="headerlink" title="穿透优化"></a>穿透优化</h3><p>缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中，通常出于容错的考虑，如果从存储层查不到数据则不写入缓存层，如图所示整个过程分为如下3步：<br><img src="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210209-135244-0463.png" alt="缓存穿透模型"></p><ul><li>缓存层不命中。</li><li>存储层不命中，不将空结果写回缓存。</li><li>返回空结果。</li></ul><p>缓存穿透将导致不存在的数据每次请求都要到存储层去查询，失去了缓存保护后端存储的意义。</p><p>缓存穿透问题可能会使后端存储负载加大，由于很多后端存储不具备高并发性，甚至可能造成后端存储宕掉。通常可以在程序中分别统计总调用数、缓存层命中数、存储层命中数，如果发现大量存储层空命中，可能就是出现了缓存穿透问题。</p><p>造成缓存穿透的基本原因有两个。第一，自身业务代码或者数据出现问题，第二，一些恶意攻击、爬虫等造成大量空命中。</p><p>下面我们来看一下如何解决缓存穿透问题。</p><ul><li><p>缓存空对象<br>如图所示，当第2步存储层不命中后，仍然将空对象保留到缓存层中，之后再访问这个数据将会从缓存中获取，这样就保护了后端数据源。<br><img src="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210209-132646-0973.png" alt="缓存空值应对穿透问题"><br>缓存空对象会有两个问题：</p><ul><li>第一，空值做了缓存，意味着缓存层中存了更多的键，需要更多的内存空间（如果是攻击，问题更严重），比较有效的方法是针对这类数据设置一个较短的过期时间，让其自动剔除。</li><li>第二，缓存层和存储层的数据会有一段时间窗口的不一致，可能会对业务有一定影响。例如过期时间设置为5分钟，如果此时存储层添加了这个数据，那此段时间就会出现缓存层和存储层数据的不一致，此时可以利用消息系统或者其他方式清除掉缓存层中的空对象。</li></ul><p>这种方法适用于数据命中不高、数据频繁变化、实时性高的应用场景，代码维护较为简单，但是缓存空间占用多，存在数据不一致问题。</p><p>下面给出了缓存空对象的实现代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">String <span class="title">get</span><span class="params">(String key)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 从缓存中获取数据</span></span><br><span class="line">  String cacheValue = cache.get(key);</span><br><span class="line">  <span class="comment">// 缓存为空</span></span><br><span class="line">  <span class="keyword">if</span> (StringUtils.isBlank(cacheValue)) &#123;</span><br><span class="line">    <span class="comment">// 从存储中获取</span></span><br><span class="line">    String storageValue = storage.get(key);</span><br><span class="line">    cache.set(key, storageValue);</span><br><span class="line">    <span class="comment">// 如果存储数据为空，需要设置一个过期时间(300秒)</span></span><br><span class="line">    <span class="keyword">if</span> (storageValue == <span class="keyword">null</span>) &#123;</span><br><span class="line">      cache.expire(key, <span class="number">60</span> * <span class="number">5</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> storageValue;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// 缓存非空</span></span><br><span class="line">  <span class="keyword">return</span> cacheValue;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>布隆过滤器拦截<br>如图所示，在访问缓存层和存储层之前，将存在的key用布隆过滤器提前保存起来，做第一层拦截。例如：一个推荐系统有4亿个用户id，每个小时算法工程师会根据每个用户之前历史行为计算出推荐数据放到存储层中，但是最新的用户由于没有历史行为，就会发生缓存穿透的行为，为此可以将所有推荐数据的用户做成布隆过滤器。如果布隆过滤器认为该用户id不存在，那么就不会访问存储层，在一定程度保护了存储层。<br><img src="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210209-132554-0440.png" alt="使用布隆过滤器应对穿透问题"><br>有关布隆过滤器的相关知识，可以参考：<a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Bloom_filter</a></p><p>可以利用Redis的Bitmaps实现布隆过滤器，GitHub上已经开源了类似的方案，参考：<br><a href="https://github.com/erikdubbelboer/redis-lua-scaling-bloom-filter" target="_blank" rel="noopener">https://github.com/erikdubbelboer/redis-lua-scaling-bloom-filter</a></p><p>这种方法适用于数据命中不高、数据相对固定、实时性低（通常是数据集较大）的应用场景，代码维护较为复杂，但是缓存空间占用少。</p></li></ul><h3 id="无底洞优化"><a href="#无底洞优化" class="headerlink" title="无底洞优化"></a>无底洞优化</h3><p>2010年， Facebook的Memcache节点已经达到了3000个， 承载着TB级别<br>的缓存数据。 但开发和运维人员发现了一个问题， 为了满足业务要求添加了<br>大量新Memcache节点， 但是发现性能不但没有好转反而下降了， 当时将这<br>种现象称为缓存的“无底洞”现象。<br>那么为什么会产生这种现象呢， 通常来说添加节点使得Memcache集群<br>性能应该更强了， 但事实并非如此。 键值数据库由于通常采用哈希函数将<br>key映射到各个节点上， 造成key的分布与业务无关， 但是由于数据量和访问<br>量的持续增长， 造成需要添加大量节点做水平扩容， 导致键值分布到更多的<br>节点上， 所以无论是Memcache还是Redis的分布式， 批量操作通常需要从不<br>同节点上获取， 相比于单机批量操作只涉及一次网络操作， 分布式批量操作<br>会涉及多次网络时间。<br>图11-6展示了在分布式条件下， 一次mget操作需要访问多个Redis节点，<br>需要多次网络时间。<br>而图11-7由于所有键值都集中在一个节点上， 所以一次批量操作只需要<br>一次网络时间。<br>无底洞问题分析：<br>·客户端一次批量操作会涉及多次网络操作， 也就意味着批量操作会随<br>着节点的增多， 耗时会不断增大。<br>·网络连接数变多， 对节点的性能也有一定影响。<br>710用一句通俗的话总结就是， 更多的节点不代表更高的性能， 所谓“无底<br>洞”就是说投入越多不一定产出越多。 但是分布式又是不可以避免的， 因为<br>访问量和数据量越来越大， 一个节点根本抗不住， 所以如何高效地在分布式<br>缓存中批量操作是一个难点。<br>下面介绍如何在分布式条件下优化批量操作。 在介绍具体的方法之前，<br>我们来看一下常见的IO优化思路：</p><h3 id="雪崩优化"><a href="#雪崩优化" class="headerlink" title="雪崩优化"></a>雪崩优化</h3><h3 id="热点key重建优化"><a href="#热点key重建优化" class="headerlink" title="热点key重建优化"></a>热点key重建优化</h3><h3 id="本章重点回顾"><a href="#本章重点回顾" class="headerlink" title="本章重点回顾"></a>本章重点回顾</h3>]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>回忆录</title>
    <url>/post/Life/memories/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>脑海中闪过的片段<a id="more"></a><br>时常有一些回忆片段涌现出脑海，借此记录下来。</p>]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title>Python进阶</title>
    <url>/post/Python/fluent-python-notes/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>《流畅的Python》学习笔记<a id="more"></a><br>学习Python之前，最好熟悉一下Python的语言和风格规范<br><a href="https://google-styleguide.readthedocs.io/zh_CN/latest/google-python-styleguide/contents.html" target="_blank" rel="noopener">https://google-styleguide.readthedocs.io/zh_CN/latest/google-python-styleguide/contents.html</a><br>原项目：<a href="https://github.com/google/styleguide" target="_blank" rel="noopener">https://github.com/google/styleguide</a></p><h2 id="Ch-0-Python的语言和风格规范"><a href="#Ch-0-Python的语言和风格规范" class="headerlink" title="Ch 0 Python的语言和风格规范"></a>Ch 0 Python的语言和风格规范</h2><h3 id="语言规范"><a href="#语言规范" class="headerlink" title="语言规范"></a>语言规范</h3><h4 id="Lint"><a href="#Lint" class="headerlink" title="Lint"></a>Lint</h4><ul><li>定义<br>pylint是一个在Python源代码中查找bug的工具，对于C和C++这样的不那么动态的(原文是less dynamic)语言，这些bug通常由编译器来捕获，由于Python的动态特性，有些警告可能不对，不过伪警告应该很少。</li><li>优点<br>可以捕获容易忽视的错误，例如输入错误，使用未赋值的变量等。</li><li>缺点<br><code>pylint isn&#39;t perfect. To take advantage of it, sometimes we&#39;ll need to write around it, suppress its warnings or fix it.</code></li><li>结论<br>确保对你的代码运行pylint，抑制不准确的警告，以便能够将其他警告暴露出来。可以通过设置一个行注释来抑制警告，例如:<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">dict = <span class="string">'something awful'</span>  <span class="comment"># Bad Idea... pylint: disable=redefined-builtin</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="导入包、模块"><a href="#导入包、模块" class="headerlink" title="导入包、模块"></a>导入包、模块</h4><ul><li>定义<br>模块间共享代码的重用机制。</li><li>优点<br>命名空间管理约定十分简单，每个标识符的源都用一种一致的方式指示，x.Obj表示Obj对象定义在模块x中。</li><li>缺点<br>模块名仍可能冲突，有些模块名太长，不太方便。</li><li>结论<br>导入时不要使用相对名称，即使模块在同一个包中，也要使用完整包名，这能帮助你避免无意间导入一个包两次。<br>所有的新代码都应该用完整包名来导入每个模块。</li></ul><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><ul><li>结论<br>异常必须遵守特定条件：<ul><li>优先合理的使用内置异常类。比如 ValueError 指示了一个程序错误，比如在方法需要正数的情况下传递了一个负数错误。不要使用 assert 语句来验证公共API的参数值，assert 是用来保证内部正确性的，而不是用来强制纠正参数使用，若需要使用异常来指示某些意外情况,不要用 assert，用 raise 语句，例如：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect_to_next_port</span><span class="params">(self, minimum)</span>:</span></span><br><span class="line">  <span class="string">"""Connects to the next available port.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">      minimum: A port value greater or equal to 1024.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">      The new minimum port.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">      ConnectionError: If no available port is found.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">if</span> minimum &lt; <span class="number">1024</span>:</span><br><span class="line">      <span class="comment"># Note that this raising of ValueError is not mentioned in the doc</span></span><br><span class="line">      <span class="comment"># string's "Raises:" section because it is not appropriate to</span></span><br><span class="line">      <span class="comment"># guarantee this specific behavioral reaction to API misuse.</span></span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">f'Min. port must be at least 1024, not <span class="subst">&#123;minimum&#125;</span>.'</span>)</span><br><span class="line">  port = self._find_next_open_port(minimum)</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> port:</span><br><span class="line">      <span class="keyword">raise</span> ConnectionError(</span><br><span class="line">          <span class="string">f'Could not connect to service on port <span class="subst">&#123;minimum&#125;</span> or higher.'</span>)</span><br><span class="line">  <span class="keyword">assert</span> port &gt;= minimum, (</span><br><span class="line">      <span class="string">f'Unexpected port <span class="subst">&#123;port&#125;</span> when minimum was <span class="subst">&#123;minimum&#125;</span>.'</span>)</span><br><span class="line">  <span class="keyword">return</span> port</span><br></pre></td></tr></table></figure></li><li>模块或包应该定义自己的特定域的异常基类，这个基类应该从内建的Exception类继承，模块的异常基类后缀应该叫做 <code>Error</code>。</li><li>永远不要使用 <code>except:</code> 语句来捕获所有异常，也不要捕获 <code>Exception</code> 或者 <code>StandardError</code>，除非你打算重新触发该异常，或者你已经在当前线程的最外层(记得还是要打印一条错误消息)，在异常这方面，Python非常宽容，<code>except:</code> 真的会捕获包括Python语法错误在内的任何错误，使用 <code>except:</code> 很容易隐藏真正的bug。</li><li>尽量减少try/except块中的代码量. try块的体积越大，期望之外的异常就越容易被触发，这种情况下，try/except块将隐藏真正的错误。</li><li>使用finally子句来执行那些无论try块中有没有异常都应该被执行的代码，这对于清理资源常常很有用，例如关闭文件。</li></ul></li></ul><h4 id="全局变量"><a href="#全局变量" class="headerlink" title="全局变量"></a>全局变量</h4><ul><li>结论<br>避免使用全局变量，鼓励使用模块级的常量。因为导入时会对模块级变量赋值，可能改变模块行为。例如 <code>MAX_HOLY_HANDGRENADE_COUNT = 3</code>，注意常量命名必须全部大写，用<code>_</code>分隔。若必须要使用全局变量，应在模块内声明全局变量，并在名称前<code>_</code>使之成为模块内部变量，外部访问必须通过模块级的公共函数。</li></ul><h4 id="嵌套-局部-内部类或函数"><a href="#嵌套-局部-内部类或函数" class="headerlink" title="嵌套/局部/内部类或函数"></a>嵌套/局部/内部类或函数</h4><ul><li>定义<br>类可以定义在方法，函数或者类中，函数可以定义在方法或函数中，封闭区间中定义的变量对嵌套函数是只读的，(译者注:即内嵌函数可以读外部函数中定义的变量，但是无法改写，除非使用 <code>nonlocal</code>)</li><li>优点<br>允许定义仅用于有效范围的工具类和函数，在装饰器中比较常用。</li><li>缺点<br>嵌套类或局部类的实例不能序列化(pickled)，内嵌的函数和类无法直接测试，同时内嵌函数和类会使外部函数的可读性变差。</li><li>结论<br>使用内部类或者内嵌函数可以忽视一些警告.但是应该避免使用内嵌函数或类,除非是想覆盖某些值.若想对模块的用户隐藏某个函数,不要采用嵌套它来隐藏,应该在需要被隐藏的方法的模块级名称加 _ 前缀,这样它依然是可以被测试的.</li></ul><h4><a href="#" class="headerlink"></a></h4><ul><li>定义<br>pylint是一个在Python源代码中查找bug的工具，对于C和C++这样的不那么动态的(原文是less dynamic)语言，这些bug通常由编译器来捕获，由于Python的动态特性，有些警告可能不对，不过伪警告应该很少。</li><li>优点<br>可以捕获容易忽视的错误，例如输入错误，使用未赋值的变量等。</li><li>缺点<br><code>pylint isn&#39;t perfect. To take advantage of it, sometimes we&#39;ll need to write around it, suppress its warnings or fix it.</code></li><li>结论<br>确保对你的代码运行pylint，抑制不准确的警告，以便能够将其他警告暴露出来。可以通过设置一个行注释来抑制警告，例如:<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">dict = <span class="string">'something awful'</span>  <span class="comment"># Bad Idea... pylint: disable=redefined-builtin</span></span><br></pre></td></tr></table></figure><h4 id="-1"><a href="#-1" class="headerlink"></a></h4></li><li>定义<br>pylint是一个在Python源代码中查找bug的工具，对于C和C++这样的不那么动态的(原文是less dynamic)语言，这些bug通常由编译器来捕获，由于Python的动态特性，有些警告可能不对，不过伪警告应该很少。</li><li>优点<br>可以捕获容易忽视的错误，例如输入错误，使用未赋值的变量等。</li><li>缺点<br><code>pylint isn&#39;t perfect. To take advantage of it, sometimes we&#39;ll need to write around it, suppress its warnings or fix it.</code></li><li>结论<br>确保对你的代码运行pylint，抑制不准确的警告，以便能够将其他警告暴露出来。可以通过设置一个行注释来抑制警告，例如:<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">dict = <span class="string">'something awful'</span>  <span class="comment"># Bad Idea... pylint: disable=redefined-builtin</span></span><br></pre></td></tr></table></figure><h4 id="-2"><a href="#-2" class="headerlink"></a></h4></li><li>定义<br>pylint是一个在Python源代码中查找bug的工具，对于C和C++这样的不那么动态的(原文是less dynamic)语言，这些bug通常由编译器来捕获，由于Python的动态特性，有些警告可能不对，不过伪警告应该很少。</li><li>优点<br>可以捕获容易忽视的错误，例如输入错误，使用未赋值的变量等。</li><li>缺点<br><code>pylint isn&#39;t perfect. To take advantage of it, sometimes we&#39;ll need to write around it, suppress its warnings or fix it.</code></li><li>结论<br>确保对你的代码运行pylint，抑制不准确的警告，以便能够将其他警告暴露出来。可以通过设置一个行注释来抑制警告，例如:<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">dict = <span class="string">'something awful'</span>  <span class="comment"># Bad Idea... pylint: disable=redefined-builtin</span></span><br></pre></td></tr></table></figure><h4 id="-3"><a href="#-3" class="headerlink"></a></h4></li><li>定义<br>pylint是一个在Python源代码中查找bug的工具，对于C和C++这样的不那么动态的(原文是less dynamic)语言，这些bug通常由编译器来捕获，由于Python的动态特性，有些警告可能不对，不过伪警告应该很少。</li><li>优点<br>可以捕获容易忽视的错误，例如输入错误，使用未赋值的变量等。</li><li>缺点<br><code>pylint isn&#39;t perfect. To take advantage of it, sometimes we&#39;ll need to write around it, suppress its warnings or fix it.</code></li><li>结论<br>确保对你的代码运行pylint，抑制不准确的警告，以便能够将其他警告暴露出来。可以通过设置一个行注释来抑制警告，例如:<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">dict = <span class="string">'something awful'</span>  <span class="comment"># Bad Idea... pylint: disable=redefined-builtin</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="风格规范"><a href="#风格规范" class="headerlink" title="风格规范"></a>风格规范</h3><h4 id="-4"><a href="#-4" class="headerlink"></a></h4><h4 id="-5"><a href="#-5" class="headerlink"></a></h4><h4 id="-6"><a href="#-6" class="headerlink"></a></h4><h4 id="-7"><a href="#-7" class="headerlink"></a></h4><h4 id="-8"><a href="#-8" class="headerlink"></a></h4><h4 id="-9"><a href="#-9" class="headerlink"></a></h4><h4 id="-10"><a href="#-10" class="headerlink"></a></h4><h4 id="-11"><a href="#-11" class="headerlink"></a></h4><center><span style="font-weight:700;font-size:24px">序幕</span></center><h2 id="Ch-1-Python数据模型"><a href="#Ch-1-Python数据模型" class="headerlink" title="Ch 1 Python数据模型"></a>Ch 1 Python数据模型</h2><p>数据（对象）模型是对Python框架的描述，规范了这门语言自身构建模块的接口，这些模块包括序列、迭代器、函数、类和上下文管理器等。<br>而无论在哪种框架下编程，都会大量实现会被框架本身调用的方法，Python里会使用特殊方法去激活一些基本的对象操作，这类方法称为魔术方法（magic method）或双下方法（dunder method），形如<code>__getitem__</code>。</p><h3 id="一摞Python风格的纸牌"><a href="#一摞Python风格的纸牌" class="headerlink" title="一摞Python风格的纸牌"></a>一摞Python风格的纸牌</h3><p>如下代码段：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> choice</span><br><span class="line"></span><br><span class="line"><span class="comment">#collections.namedtuple用于构建一个简单的类，</span></span><br><span class="line"><span class="comment">#该类用来构建只有少数属性但没有方法的对象，如数据库条目。</span></span><br><span class="line">Card = collections.namedtuple(<span class="string">'Card'</span>, [<span class="string">'rank'</span>, <span class="string">'suit'</span>])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FrenchDeck</span>:</span></span><br><span class="line">    ranks = [str(n) <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">11</span>)] + list(<span class="string">'JQKA'</span>)</span><br><span class="line">    suits = <span class="string">'方块 梅花 红桃 黑桃'</span>.split()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._cards = [Card(rank, suit) <span class="keyword">for</span> suit <span class="keyword">in</span> self.suits</span><br><span class="line">                                        <span class="keyword">for</span> rank <span class="keyword">in</span> self.ranks]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._cards)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, position)</span>:</span></span><br><span class="line">        <span class="string">"""提供了元素定位deck[0]</span></span><br><span class="line"><span class="string">        将[]操作交给self._cards列表，所以deck类自动支持切片操作且可迭代（支持反向迭代reversed(deck)）</span></span><br><span class="line"><span class="string">        迭代通常是隐式的，若一个集合类型未实现__contains__方法，那么in运算就会按顺序做一次迭代搜索</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self._cards[position]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    card1 = Cards(<span class="number">3</span>, <span class="string">'红桃'</span>)</span><br><span class="line">    print(card1)</span><br><span class="line">    deck = FrenchDeck()</span><br><span class="line">    print(len(deck))</span><br><span class="line">    print(deck[<span class="number">-1</span>])</span><br><span class="line">    print(choice(deck))</span><br><span class="line">    print(Card(<span class="string">'7'</span>, <span class="string">'红桃'</span>))</span><br><span class="line">    <span class="comment"># TODO(xyr): 排序和洗牌</span></span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">    Card(rank=<span class="number">3</span>, suit=<span class="string">'红桃'</span>)</span><br><span class="line">    <span class="number">52</span></span><br><span class="line">    Card(rank=<span class="string">'A'</span>, suit=<span class="string">'黑桃'</span>)</span><br><span class="line">    Card(rank=<span class="string">'3'</span>, suit=<span class="string">'黑桃'</span>)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><h3 id="如何使用特殊方法"><a href="#如何使用特殊方法" class="headerlink" title="如何使用特殊方法"></a>如何使用特殊方法</h3><ul><li>首先，特殊方法是为了被Python解释器调用，而非自己调用，除非有大量元编程存在，或者在自己的子类的<code>__init__</code>方法中调用超累的构造器。</li><li>特殊方法的调用一般是隐式的，比如<code>for i in x:</code>调用的其实是iter(x)，而这个函数背后则是<code>x.__iter__()</code>方法（前提是这个方法已经在x中被实现）。</li><li>通过内置的函数（len、str等）使用特殊方法是最好的选择。</li><li>如果是Python内置的类型（list、str等），CPython的<code>__len__</code>实际上会直接返回PyVarObject（表示内存中长度可变的内置对象的C语言结构体）里的ob_size属性。</li><li>不要随意添加特殊方法，以防在后面的更新中被Python内部使用而产生冲突。</li></ul><h4 id="模拟数值类型"><a href="#模拟数值类型" class="headerlink" title="模拟数值类型"></a>模拟数值类型</h4><p>利用特殊方法，可以让自定义对象通过“+”（或别的运算符）进行运算，例如下面的一个自定义的二维向量类，其中的+、*和abs（取绝对值或取模）运算即是由这些特殊方法实现。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> hypot</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vector</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x=<span class="number">0</span>, y=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Vector(%r, %r)'</span> % (self.x, self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__abs__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> hypot(self.x, self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__bool__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> bool(abs(self))</span><br><span class="line">        <span class="comment"># return bool(self.x or self.y)  #更加高效</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        x = self.x + other.x</span><br><span class="line">        y = self.y + other.y</span><br><span class="line">        <span class="keyword">return</span> Vector(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__mul__</span><span class="params">(self, scalar)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Vector(self.x * scalar, self.y * scalar)  <span class="comment">#未考虑交换律</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    v1 = Vector(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    v2 = Vector(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    print(v1+v2)</span><br><span class="line">    print(abs(v1))</span><br><span class="line">    print(v1*<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">    Vector(<span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line">    <span class="number">5.0</span></span><br><span class="line">    Vector(<span class="number">9</span>, <span class="number">12</span>)</span><br></pre></td></tr></table></figure><h4 id="字符串表示形式"><a href="#字符串表示形式" class="headerlink" title="字符串表示形式"></a>字符串表示形式</h4><ul><li>Python通过内置函数repr将一个对象用字符串的形式表达出来，repr就是通过<code>__repr__</code>这个特殊方法得到一个对象的字符串的形式，若未实现<code>__repr__</code>，打印一个向量实例时得到的会是<code>&lt;Vector object at 0x10e100070&gt;</code></li><li>格式化字符串的两种方法，使用<code>%</code>和使用<code>str.format</code>均利用了repr。</li><li><code>__repr__</code>中使用<code>%r</code>来获取对象各个属性的标准字符串表示形式。</li><li><code>__repr__</code>所返回的字符串应准确无歧义，且尽可能表达出如何用代码创建出这个被打印的对象。</li><li><code>__repr__</code>和<code>__str__</code>的区别在于前者方便记录和调试日志，后者在<code>str()</code>函数被使用，或<code>print()</code>打印对象时被调用，且返回的字符串对终端用户更友好。两者优先实现<code>__repr__</code>，因为一个函数没有<code>__str__</code>而去调用它会用<code>__repr__</code>来代替。（详见<a href="http://stackoverflow.com/questions/1436703/differencebetween-str-and-repr-in-python）" target="_blank" rel="noopener">http://stackoverflow.com/questions/1436703/differencebetween-str-and-repr-in-python）</a></li></ul><h4 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h4><p>通过<code>__add__</code>和<code>__mul__</code>实现了+和*，这里不改变操作对象，而是返回一个新创建的向量对象。</p><h4 id="自定义的布尔值"><a href="#自定义的布尔值" class="headerlink" title="自定义的布尔值"></a>自定义的布尔值</h4><ul><li><code>bool(x)</code>背后调用<code>x.__bool__()</code>，若未实现<code>__bool__</code>方法，那么<code>bool(x)</code>会尝试调用<code>x.__len__()</code>，0为False，否则返回True。</li><li>默认情况下自定义类的实例总为真，除非这个类对<code>__bool__</code>或者<code>__len__</code>函数有自己的实现。</li><li>更高效的<code>__bool__</code>方法，<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__bool__</span><span class="params">(self)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> bool(self.x <span class="keyword">or</span> self.y)</span><br></pre></td></tr></table></figure></li></ul><h3 id="特殊方法一览"><a href="#特殊方法一览" class="headerlink" title="特殊方法一览"></a>特殊方法一览</h3><p>Python语言参考手册中的“DataModel”，<a href="https://docs.python.org/3/reference/datamodel.html" target="_blank" rel="noopener">（https://docs.python.org/3/reference/datamodel.html）</a>一章列出了83个特殊方法的名字，其中47个用于实现算术运算、位运算和比较操作。<br><img src="https://s3.ax1x.com/2021/01/04/sP0shD.jpg" alt=""></p><h3 id="为什么len不是普通方法"><a href="#为什么len不是普通方法" class="headerlink" title="为什么len不是普通方法"></a>为什么len不是普通方法</h3><p>“实用胜于纯粹，不能让特例特殊到破坏既定规则。”<br>len之所以不是一个普通方法，是为了让Python自带的数据结构可以走后门，abs也是同理。但是多亏了它是特殊方法，我们也可以把len用于自定义数据类型。这种处理方式在保持内置类型的效率和保证语言的一致性之间找到了一个平衡点。</p><center><span style="font-weight:700;font-size:24px">数据结构</span></center><h2 id="Ch2-序列构成的数组"><a href="#Ch2-序列构成的数组" class="headerlink" title="Ch2 序列构成的数组"></a>Ch2 序列构成的数组</h2><p>无论那种数据结构，都共用一套操作：<strong>迭代</strong>、<strong>切片</strong>、<strong>排序</strong>和<strong>拼接</strong>。</p><h3 id="内置序列类型概览"><a href="#内置序列类型概览" class="headerlink" title="内置序列类型概览"></a>内置序列类型概览</h3><h3 id="列表推导和生成器表达式"><a href="#列表推导和生成器表达式" class="headerlink" title="列表推导和生成器表达式"></a>列表推导和生成器表达式</h3><h3 id="元组不仅仅是不可变的列表"><a href="#元组不仅仅是不可变的列表" class="headerlink" title="元组不仅仅是不可变的列表"></a>元组不仅仅是不可变的列表</h3><h3 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h3><h3 id="对序列使用-和"><a href="#对序列使用-和" class="headerlink" title="对序列使用+和*"></a>对序列使用+和*</h3><h3 id="序列的增量赋值"><a href="#序列的增量赋值" class="headerlink" title="序列的增量赋值"></a>序列的增量赋值</h3><h3 id="list-sort方法和内置函数sorted"><a href="#list-sort方法和内置函数sorted" class="headerlink" title="list.sort方法和内置函数sorted"></a>list.sort方法和内置函数sorted</h3><h3 id="用bisect来管理已排序的序列"><a href="#用bisect来管理已排序的序列" class="headerlink" title="用bisect来管理已排序的序列"></a>用bisect来管理已排序的序列</h3><h3 id="当列表不是首选时"><a href="#当列表不是首选时" class="headerlink" title="当列表不是首选时"></a>当列表不是首选时</h3>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>WireShark网络分析</title>
    <url>/post/Networks/wireshark/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>WireShark网络分析</p><a id="more"></a><p>官网：<a href="https://www.wireshark.org/" target="_blank" rel="noopener">https://www.wireshark.org/</a></p><h2 id="《Wireshark网络分析的艺术》笔记"><a href="#《Wireshark网络分析的艺术》笔记" class="headerlink" title="《Wireshark网络分析的艺术》笔记"></a>《Wireshark网络分析的艺术》笔记</h2><h3 id="关于TCP的延迟确认机制可能带来的性能问题，"><a href="#关于TCP的延迟确认机制可能带来的性能问题，" class="headerlink" title="关于TCP的延迟确认机制可能带来的性能问题，"></a>关于TCP的延迟确认机制可能带来的性能问题，</h3><ul><li><strong>延迟确认的优点</strong>：在延迟的时间内，客户端有数据要发，就可以在发数据时捎带确认信息，省去一个纯粹的确认包（实际为帧，后面全称为包），进而节省带宽。</li><li><strong>可能会导致的问题</strong>：网络拥塞时大量重传影响性能，或因延迟太久导致超时重传，或TCP窗口极小。</li><li><strong>解决</strong>：启用SACK会在ACK中附带已收到的包号（或者关闭延迟确认），从而解决延迟确认带来的性能问题。<br>在Wireshark抓包中使用Filter：<code>tcp.analysis.ack_rtt&gt;0.2 and tcp.len==0</code>将超过200ms的确认包筛选出来</li></ul><h3 id="关于TCP的三次握手失败分析"><a href="#关于TCP的三次握手失败分析" class="headerlink" title="关于TCP的三次握手失败分析"></a>关于TCP的三次握手失败分析</h3><ul><li><strong>主要原因</strong>：</li></ul>]]></content>
      <categories>
        <category>Networks</category>
      </categories>
      <tags>
        <tag>WireShark</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫基础</title>
    <url>/post/Python/python-spider/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>《Python3网络爬虫开发实战》笔记<a id="more"></a></p><h2 id="Ch-1-爬虫基础"><a href="#Ch-1-爬虫基础" class="headerlink" title="Ch 1 爬虫基础"></a>Ch 1 爬虫基础</h2><h3 id="HTTP基本原理"><a href="#HTTP基本原理" class="headerlink" title="HTTP基本原理"></a>HTTP基本原理</h3><h4 id="URI和URL"><a href="#URI和URL" class="headerlink" title="URI和URL"></a>URI和URL</h4><p>URI的全称为 Uniform Resource Identifier，即统一资源标志符。<br>URL的全称为 Universal Resource Locator，即统一资源定位符。<br>URN的全称为 Universal Resource Name，即统一资源名称。<br>三者关系为：<br><img src="https://s3.ax1x.com/2021/01/05/skJuQI.png" alt=""></p><h4 id="超文本"><a href="#超文本" class="headerlink" title="超文本"></a>超文本</h4><p>HypeText</p><h4 id="HTTP和HTTPS"><a href="#HTTP和HTTPS" class="headerlink" title="HTTP和HTTPS"></a>HTTP和HTTPS</h4><p>HTTP的全称是Hyper Text Transfer Protocol，超文本传输协议。<br>HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的HTTP通道，简单讲是 HTTP 的安全版，即HTTP下加入 SSL层，简称为HTTPS。</p><h4 id="HTTP请求过程"><a href="#HTTP请求过程" class="headerlink" title="HTTP请求过程"></a>HTTP请求过程</h4><h4 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h4><ul><li><p>请求方法<br><img src="https://s3.ax1x.com/2021/01/05/skt3GQ.png" alt=""></p></li><li><p>请求网址<br>URL</p></li><li><p>请求头<br>用来说明服务器要使用的附加信息。<br><img src="https://s3.ax1x.com/2021/01/05/sktHsI.png" alt=""></p><p>因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。</p></li><li><p>请求体<br>一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。<br><img src="https://s3.ax1x.com/2021/01/05/skNbp4.png" alt=""></p><p>在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type， 不然可能会导致POST提交后无法正常响应。</p></li></ul><h4 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h4><ul><li><p>响应状态码<br>响应状态码表示服务器的响应状态。<br><img src="https://s3.ax1x.com/2021/01/05/skUWCD.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skaVxJ.png" alt=""></p></li><li><p>响应头<br>响应头包含了服务器对请求的应答信息。<br><img src="https://s3.ax1x.com/2021/01/05/skavFK.png" alt=""></p></li><li><p>响应体<br>响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。</p></li></ul><h3 id="网页基础"><a href="#网页基础" class="headerlink" title="网页基础"></a>网页基础</h3><h4 id="网页的组成"><a href="#网页的组成" class="headerlink" title="网页的组成"></a>网页的组成</h4><p>网页可以分为三大部分——HTML,CSS和JavaScript。如果把网页比作一个人的话， HTML相于骨架，JavaScript相当于肌肉，CSS相当于皮肤。</p><ul><li>HTML<br>HTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。</li><li>CSS<br>CSS，全称叫作Cascading Style Sheets，即层叠样式表。</li><li>JavaScript<br>JavaScript，简称JS，是一种脚本语言，实现了一种实时、动态、交互的页面功能。</li></ul><h4 id="网页的结构"><a href="#网页的结构" class="headerlink" title="网页的结构"></a>网页的结构</h4><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="节点树及节点间关系"><a href="#节点树及节点间关系" class="headerlink" title="节点树及节点间关系"></a>节点树及节点间关系</h4><p><img src="https://s3.ax1x.com/2021/01/05/skyP9x.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skykjO.png" alt=""></p><h4 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h4><p>常用三种方法：根据id（#）、根据class（.）以及标签名（h1）进行筛选。<br>嵌套选择：</p><ul><li>各选择器之间加空格代表嵌套关系，如div #container为先选择一个div节点，在选择其内部id为container的节点。</li><li>不加空格代表并列关系，如div#container为选择id为container的div节点。</li></ul><p>css选择器还有一些其他语法规则，具体如表2-4所示。<br><img src="https://s3.ax1x.com/2021/01/05/skh0nx.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skhWjI.png" alt=""></p><p><img src="https://s3.ax1x.com/2021/01/05/skhhut.png" alt=""></p><h3 id="爬虫的基本原理"><a href="#爬虫的基本原理" class="headerlink" title="爬虫的基本原理"></a>爬虫的基本原理</h3><h4 id="爬虫概述"><a href="#爬虫概述" class="headerlink" title="爬虫概述"></a>爬虫概述</h4><ul><li>获取网页</li><li>提取信息</li><li>保存数据</li><li>自动化程序</li></ul><h4 id="能抓怎样的数据"><a href="#能抓怎样的数据" class="headerlink" title="能抓怎样的数据"></a>能抓怎样的数据</h4><p>HTML代码、JSON文件、二进制数据等</p><h4 id="JavaScript渲染页面"><a href="#JavaScript渲染页面" class="headerlink" title="JavaScript渲染页面"></a>JavaScript渲染页面</h4><p>通过分析其后台Ajax接口，或使用Selenium、Splash库来模拟JavaScript渲染。</p><h3 id="会话和Cookies"><a href="#会话和Cookies" class="headerlink" title="会话和Cookies"></a>会话和Cookies</h3><h4 id="静态网页和动态网页"><a href="#静态网页和动态网页" class="headerlink" title="静态网页和动态网页"></a>静态网页和动态网页</h4><h4 id="无状态HTTP"><a href="#无状态HTTP" class="headerlink" title="无状态HTTP"></a>无状态HTTP</h4><p>HTTP连接本身是无状态的。</p><ul><li><p>会话<br>Web中，会话对象用来存储特定用户会话所需的属性及配置信息。</p></li><li><p>Cookies<br>Cookies指某些网站为了辨别用户身份、进行会话跟踪而存储在用户本地终端上的数据。</p></li><li><p>会话维持<br>当客户端第一次请求服务器时，服务器会返回一个请求头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把Cookie保 存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies放到请求头一起提交给服务器，“Cookies携带了会话ID信息，服务器检查该Cookies即可找到对应的会话是什么，然后再判断会话来以此来辨认用户状态。</p></li><li><p>属性结构</p><ul><li>Name：该Cookie的名称。一旦创建，该名称便不可更改。</li><li>Value：该Cookie的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。</li><li>Domain：可以访问该 Cookie 的域名 。 例如，如果设置为 . zhihu.com ，则所有以 zh ihu .com 结尾的域名都可以访问该 Cookie。</li><li>Max Age：该Cookie失效的时间，单位为秒，也常和Expires一起使用，通过它可以计算出其有效时间。Max Age如果为正数，则该Cookie在Max Age 秒之后失效。如果为负数，则关闭浏览器时Cookie即失效，浏览器也不会以任何形式保存该Cookie。</li><li>Path：该Cookie的使用路径。如果设置为／path/，则只有路径为／path/的页面可以访问该Cookie；如果设置为/，则本域名下的所有页面都可以访问该Cookie。</li><li>Size字段：此Cookie的大小。</li><li>HTTP字段：Cookie的httponly属性。若此属性为true，则只有在HTTP头中会带有此Cookie的信息，而不能通过document.cookie来访问此Cookie。</li><li>Secure：该Cookie是否仅被使用安全协议传输。安全协议有HTTPS和SSL等，在网络上传输数据之前先将数据加密。默认为false。</li></ul></li><li><p>会话Cookie和持久Cookie<br>表面意思为会话Cookie存在浏览器内存里，浏览器关闭则Cookie失效；持久Cookie保存在硬盘里，下次可再次使用。<br>实际为设置Cookie的Max Age或Expires字段。</p></li></ul><h4 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a>常见误区</h4><p>“浏览器关闭，会话就消失了”。是不准确的。</p><h3 id="代理的基本原理"><a href="#代理的基本原理" class="headerlink" title="代理的基本原理"></a>代理的基本原理</h3><h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h4><p>本机的网络请求通过代理服务器访问Web服务器。</p><h4 id="代理的作用"><a href="#代理的作用" class="headerlink" title="代理的作用"></a>代理的作用</h4><ul><li>突破自身IP访问限制。</li><li>访问一些单位或团体内部资源。</li><li>提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将·其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。</li><li>隐藏真实IP：免受攻击或防止IP被封锁。</li></ul><h4 id="代理分类"><a href="#代理分类" class="headerlink" title="代理分类"></a>代理分类</h4><ul><li>根据协议区分<ul><li>FTP代理服务器：主要用于访问FTP服务器，一般有上传、下载以及缓存功能，端口一般为21、2121等。</li><li>HTTP代理服务器：主要用于访问网页，一般有内容过滤和缓存功能，端口一般为80、8080、3128等。</li><li>SSl/TLS代理：主要用于访问加密网站，一般有SSL或TLS加密功能（最高支持128位加密强度），端口一般为443。</li><li>RTSP代理：主要用于访问Real流媒体服务器，一般有缓存功能，端口一般为554。</li><li>Telnet代理：主要用于telnet远程控制（黑客人侵计算机时常用于隐藏身份），端口一般为23。</li><li>POP3/SMTP代理：主要用于POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25。</li><li>SOCKS代理：只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080。SOCKS代理协议又分为SOCKS4和SOCKS5，前者只支持TCP，而后者支持TCP和UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCKS4能做到的SOCKS5都可以做到，但 SOCKS5能做到的SOCKS4不一定能做到。</li></ul></li><li>根据匿名程度区分<ul><li>高度匿名代理：会将数据包原封不动地转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的IP是代理服务器的IP。</li><li>普通匿名代理：会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实IP。代理服务器通常会加入的HTTP头有HTTP_VIA和 HTTP_X_FORWARDED_FOR。</li><li>透明代理：不但改动了数据包 还会告诉服务器客户端的真实IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网巾的硬件防火墙。</li><li>间谍代理：指组织或个人创建的用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。</li></ul></li></ul><h4 id="常见代理设置"><a href="#常见代理设置" class="headerlink" title="常见代理设置"></a>常见代理设置</h4><ul><li>网上的免费代理</li><li>付费代理服务</li><li>ADSL拨号：拨一次号换一次IP，稳定性高。</li></ul><h2 id="Ch-2-基本库的使用"><a href="#Ch-2-基本库的使用" class="headerlink" title="Ch 2 基本库的使用"></a>Ch 2 基本库的使用</h2><h3 id="使用urllib"><a href="#使用urllib" class="headerlink" title="使用urllib"></a>使用urllib</h3><p>urllib为Python内置的HTTP请求库，包含以下4个模块：</p><ul><li>request：它是最基本的HTTP请求模块，可以用来模拟发送请求。</li><li>error：异常处理模块。</li><li>parse：一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。</li><li>robot parser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，用得比较少。</li></ul><h4 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h4><ul><li>urlopen()<br>urlopen()返回一个HTTPResponse类型的对象，主要包含read()、readinto()、getheader(name)、getheaders()、fileno()等方法，以及msg、version、status、reason、debuglevel、closed等属性。<br>urlopen()函数的API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, timeout=<span class="number">1</span>, cafile=<span class="literal">None</span>, </span><br><span class="line">                            capath=<span class="literal">None</span>, cadefault=<span class="literal">False</span>, context=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">data：需要是字节流编码格式，即bytes类型，请求方法变为POST</span><br><span class="line">timeout：单位为秒</span><br><span class="line">cafile和capath：指定CA证书及其路径</span><br><span class="line">cadefault：已弃用</span><br><span class="line">context：用来指定SSL设置，必须是ssl.SSLContext类型</span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'word'</span>:<span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">response= urllib.request.urlopen(<span class="string">'http://httpbin.org/post'</span>, data=data)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure></li><li>Request类<br>API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(ur1, data=None, headers=&#123;&#125;,</span></span></span><br><span class="line"><span class="class"><span class="params">                                  origin_req_host=None, unverifiable=False, method=None)</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">headers</span>：为一个字典，用来构造请求头，也可以后面用<span class="title">add_header</span><span class="params">()</span>添加，常用来修改<span class="title">User</span>-<span class="title">Agent</span></span></span><br><span class="line"><span class="class"><span class="title">origin_req_host</span>：请求方的<span class="title">host</span>名称或<span class="title">IP</span>地址</span></span><br><span class="line"><span class="class"><span class="title">unverifiable</span>：请求权限问题</span></span><br><span class="line"><span class="class"><span class="title">method</span>：请求方法</span></span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (Compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Germey'</span></span><br><span class="line">&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict),encoding=<span class="string">'utf8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure></li><li>用Opener构建Handler<br>官方文档：<a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler</a><ul><li>验证<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">'username'</span></span><br><span class="line">password = <span class="string">'password'</span></span><br><span class="line">url = <span class="string">'http://localhost:5000/'</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>代理<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:9743'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>Cookies<ul><li>获取Cookies<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">"="</span>+item.value)</span><br></pre></td></tr></table></figure></li><li>Cookie保存至文件<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'cookies.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line"><span class="comment">#cookie = http.cookiejar.LWPCookieJar(filename)  另一种格式</span></span><br><span class="line"><span class="comment">#cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)  读取</span></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="处理异常"><a href="#处理异常" class="headerlink" title="处理异常"></a>处理异常</h4><ul><li>URLError<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li><li>HTTPError<br>URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre></td></tr></table></figure></li></ul><h4 id="解析链接"><a href="#解析链接" class="headerlink" title="解析链接"></a>解析链接</h4><p>url.parse模块，定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。<br>支持如下协议的URL处理：file、ftp、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。<br>常用方法如下：</p><ul><li><p>urlparse()<br>实现URL的识别与分段。<br>API：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">''</span>, allow_fragments=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">scheme：url中没有协议时，作为默认的协议。</span><br><span class="line">allow_fragments：是否忽略fragment。如果设置为<span class="literal">False</span>，fragment部分就会被忽略，</span><br><span class="line">                 它会被依次解析为query、parameters或者path的一部分，而fragment部分为空。</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html#comment'</span>,</span><br><span class="line">                   scheme=<span class="string">'https'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)  <span class="comment">#返回的result为ParseResult类型，实际上是一个元组，支持result[0]和result.scheme</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">ParseResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, </span><br><span class="line">            path=<span class="string">'/index.html#comment'</span>, params=<span class="string">''</span>, query=<span class="string">''</span>, fragment=<span class="string">''</span>)</span><br></pre></td></tr></table></figure></li><li><p>urlunparse()<br>实现URL的构造。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"></span><br><span class="line"><span class="comment">#data可以用其他类型，但长度必须是6</span></span><br><span class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunparse(data))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">http://www.baidu.com/index.html;user?a=6#comment</span><br></pre></td></tr></table></figure></li><li><p>urlsplit()<br>类似urlparse()，但只返回5个结果，params合并到path里。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"></span><br><span class="line">result = urlsplit(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">SplitResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, </span><br><span class="line">            path=<span class="string">'/index.html;user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span><br></pre></td></tr></table></figure></li><li><p>urlunsplit()<br>类似urlunparse()，但只传入5个参数。</p></li><li><p>urljoin()</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">urljoin(base_url, target_url)</span><br><span class="line"></span><br><span class="line">分析base_url的scheme、netloc和path三个内容并对target_url进行补充</span><br></pre></td></tr></table></figure></li><li><p>urlencode()<br>将字典序列化为GET请求的参数。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">'http://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">http://www.baidu.com?name=germey&amp;age=22</span><br></pre></td></tr></table></figure></li><li><p>parse_qs()<br>将URL反序列化为字典。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">'name=germey&amp;age=22'</span></span><br><span class="line">print(parse_qs(query))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&#123;<span class="string">'name'</span>: [<span class="string">'germey'</span>], <span class="string">'age'</span>: [<span class="string">'22'</span>]&#125;</span><br></pre></td></tr></table></figure></li><li><p>parse_qsl()<br>将URL转化为元组组成的列表。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[(<span class="string">'name'</span>, <span class="string">'germey'</span>), (<span class="string">'age'</span>, <span class="string">'22'</span>)]</span><br></pre></td></tr></table></figure></li><li><p>quote()<br>将内容（中文字符）转化为URL编码的格式。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">keyword = <span class="string">'壁纸'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8</span><br></pre></td></tr></table></figure></li><li><p>unquote()<br>进行URL解码</p></li></ul><h4 id="分析Robots协议"><a href="#分析Robots协议" class="headerlink" title="分析Robots协议"></a>分析Robots协议</h4><ul><li><p>Robots协议<br>Robots协议也称作爬虫协议、机器人协议，全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robot.txt的文本文件，一般放在网站的根目录下。</p></li><li><p>爬虫名称<br>常见的搜索爬虫的名称及对应的网站：<br>BaiduSpider 百度 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a><br>Googlebot 谷歌 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>360Spider 360 搜索 <a href="http://www.so.com" target="_blank" rel="noopener">www.so.com</a><br>YodaoBot 有道 <a href="http://www.youdao.com" target="_blank" rel="noopener">www.youdao.com</a><br>ia_archiver Alexa <a href="http://www.alexa.cn" target="_blank" rel="noopener">www.alexa.cn</a><br>Scooter altavista <a href="http://www.altavista.com" target="_blank" rel="noopener">www.altavista.com</a></p></li><li><p>robotparser<br>urllib.robotparser模块提供了一个RobotFileParser类，该类的一些方法如下：</p><ul><li>set_url()：用来设置robots.txt文件的链接。也可在创建RobotFileParser对象时传入链接。</li><li>read()：读取robots.txt文件并进行分析。</li><li>parse()：用来解析robots.txt文件，传人的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。</li><li>can_fetch()：该方法传人两个参数，第一个是 User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，结果为True 或False。</li><li>mtime()：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是有必要的，可能需要定期检查来抓取最新的robots.txt。</li><li>modified()：同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间。</li></ul><p>示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line"><span class="comment">#上面两行可以用parse方法来执行读取和分析</span></span><br><span class="line"><span class="comment">#rp.parse(urlopen('http://www.jianshu.com/robots.txt').read().decode('utf-8').split('\n'))</span></span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="使用requests"><a href="#使用requests" class="headerlink" title="使用requests"></a>使用requests</h3><p>解决urllib中Cookies、登录验证、代理设置不方便的问题。<br>安装<code>pip install requests</code><br>requests的官方文档：<a href="http://docs.python-requests.org/" target="_blank" rel="noopener">http://docs.python-requests.org/</a></p><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>requests库包含get()、post()、put()、delete()、head()和options()等方法，分别对应各种方式请求网页。</p><ul><li><p>GET请求</p><ul><li>基本实例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line"><span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=data)</span><br><span class="line">print(type(r))</span><br><span class="line">print(r.status)</span><br><span class="line"><span class="comment">#网页的返回类型实际是JSON格式的str类型，调用json()可将其转化为字典</span></span><br><span class="line">print(type(r.json()))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">requests</span>.<span class="title">models</span>.<span class="title">Response</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">dict</span>'&gt;</span></span><br></pre></td></tr></table></figure></li><li>抓取网页<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) '</span></span><br><span class="line">                    <span class="string">'AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                     <span class="string">'Chrome/52.0.2743.116 Safari/537.36'</span>)</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>, headers=headers)</span><br><span class="line">pattern = re.compile(<span class="string">'explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;'</span>, re.S)</span><br><span class="line">titles = re.findall(pattern, r.text)</span><br><span class="line">print(titles)</span><br></pre></td></tr></table></figure></li><li>抓取二进制数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">"https://github.com/favicon.ico"</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'favicon.ico'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(r.content)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>POST请求</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="string">'22'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=data)</span><br></pre></td></tr></table></figure></li><li><p>响应</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://www.xxyr.cc'</span>)</span><br><span class="line">print(type(r.status_code))</span><br><span class="line">print(type(r.headers))</span><br><span class="line">print(type(r.cookies))</span><br><span class="line">print(type(r.url))</span><br><span class="line">print(type(r.history))</span><br><span class="line">print(type(r.text))  <span class="comment">#返回内容的字符串形式</span></span><br><span class="line">print(type(r.content)) <span class="comment">#返回内容的二进制形式</span></span><br><span class="line">print(requests.codes.ok)  <span class="comment">#内置的返回码</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">int</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">requests</span>.<span class="title">structures</span>.<span class="title">CaseInsensitiveDict</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">requests</span>.<span class="title">cookies</span>.<span class="title">RequestsCookieJar</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">list</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">bytes</span>'&gt;</span></span><br><span class="line"><span class="class">200</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h4><ul><li><p>文件上传</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">files = &#123;<span class="string">'file'</span>: open(<span class="string">'favicon.ico'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>, files=files)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li><li><p>Cookies</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">print(r.cookies)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> r.cookies.items():</span><br><span class="line">    print(key + <span class="string">'='</span> + value)</span><br></pre></td></tr></table></figure><p>可将Cookie字段添加到headers里实现登录：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>,</span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://www.zhihu.com'</span>, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>或将其作为cookie参数添加到get()方法里：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">cookies = <span class="string">'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0="AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t="2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0'</span></span><br><span class="line">jar = requests.cookies.RequestsCookieJar()</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> cookie <span class="keyword">in</span> cookies.split(<span class="string">';'</span>):</span><br><span class="line">    key, value = cookie.split(<span class="string">'='</span>, <span class="number">1</span>)</span><br><span class="line">    jar.set(key, value)</span><br><span class="line">r = requests.get(<span class="string">'http://www.zhihu.com'</span>, cookies=jar, headers=headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li><li><p>会话维持<br>当访问登录网站后的页面，或同一站点的不同页面时，就需要进行会话维持。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>)</span><br><span class="line">r = s.get(<span class="string">'http://httpbin.org/cookies'</span>)</span><br><span class="line">print(r.text)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"cookies"</span>: &#123;</span><br><span class="line">    <span class="string">"number"</span>: <span class="string">"123456789"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>SSL证书验证</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)  <span class="comment">#默认为True，自动验证证书</span></span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure><p><code>verify=False</code>会忽略证书的验证，但会报一个警告，解决方法如下：</p><ul><li>设置忽略警告<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line"></span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li><li>捕获警告到日志<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">logging.captureWarnings(<span class="literal">True</span>)</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li><li>指定一个本地证书用作客户端证书<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>代理设置</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line"><span class="string">'http'</span>: <span class="string">'http://10.10.1.10:3128'</span>,</span><br><span class="line"><span class="string">'https'</span>: <span class="string">'http://10.10.1.10:1080'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><p>若要使用HTTP Basic Auth，可以使用如下代理形式：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'http://user:password@10.10.1.10:3128/'</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><p>requests还支持SOCKS代理：<br>安装：<code>pip3 install &#39;requests[socks]&#39;</code></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://user:password@host:port'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://user:password@host:port'</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">'https://www.taobao.com'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure></li><li><p>超时设置<br>为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应就报错。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>, timeout=<span class="number">1</span>)</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure><p>timeout=1表示超时时间为1秒，默认为None，即永远等待。<br>实际上，请求分为两个阶段，即连接（connect）和读取（read）。上面设置的timeout将用作连接和读取这二者的timeout总和。<br>如果要分别指定，就可以传入一个元组：<br><code>r = requests.get(&#39;https://www.taobao.com&#39;, timeout=(5,11, 30))</code></p></li><li><p>身份认证</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment">#auth=('username', 'password')即auth=HTTPBasicAuth('username', 'password')</span></span><br><span class="line">r = requests.get(<span class="string">'http://localhost:5000'</span>, auth=(<span class="string">'username'</span>, <span class="string">'password'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure><p>此外requests还提供了其他认证方式，如OAuth认证等。</p></li><li><p>Prepared Request<br>用于将请求表示为数据结构，其中各个参数通过一个Request对象来表示。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 '</span>)</span><br><span class="line">                    (<span class="string">'(KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'</span>)</span><br><span class="line">&#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(<span class="string">'POST'</span>, url, data=data, headers=headers)</span><br><span class="line"><span class="comment">#Session的prepare_request()方法将其转化为一个Prepared Request对象</span></span><br><span class="line">prepped = s.prepare_request(req)  </span><br><span class="line">r = s.send(prepped)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li></ul><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>详见： <a href="https://xxyr.cc/post/tech/regex-review/">正则表达式复习</a></p><h3 id="抓取猫眼电影排行"><a href="#抓取猫眼电影排行" class="headerlink" title="抓取猫眼电影排行"></a>抓取猫眼电影排行</h3><p>详见<a href="https://github.com/Python3WebSpider/MaoYan" target="_blank" rel="noopener">https://github.com/Python3WebSpider/MaoYan</a></p><h2 id="Ch-3-解析库的使用"><a href="#Ch-3-解析库的使用" class="headerlink" title="Ch 3 解析库的使用"></a>Ch 3 解析库的使用</h2><h3 id="使用XPath"><a href="#使用XPath" class="headerlink" title="使用XPath"></a>使用XPath</h3><p>lxml库安装：<code>pip install lxml</code><br>如果想查询更多XPath的用法，可以查看：<a href="http://www.w3school.com.cn/xpath/index.asp" target="_blank" rel="noopener">http://www.w3school.eom.cn/xpath/index.asp</a><br>如果想查询更多lxml库的用法，可以查看：<a href="http://lxml.de/" target="_blank" rel="noopener">http://lxml.de／</a></p><h4 id="XPath概览"><a href="#XPath概览" class="headerlink" title="XPath概览"></a>XPath概览</h4><ul><li>XPath，全称XML Path Language，即XML路径语言，是一门在XML文档中查找信息的语言，最初是用来搜寻XML文档的，但同样适用于HTML文档的搜索。</li><li>XPath于1999年ll月16日成为W3C标准，它被设计为供XSLT、XPointer以及其他XML解析软件使用，更多的文档可以访问其官方网站：<a href="https://www.w3.org/TR/xpath" target="_blank" rel="noopener">https://www.w3.org/TR/xpath</a>。</li></ul><h4 id="Xpath常用规则"><a href="#Xpath常用规则" class="headerlink" title="Xpath常用规则"></a>Xpath常用规则</h4><p><img src="https://s3.ax1x.com/2021/01/11/s89S0K.png" alt=""></p><h4 id="实例引入"><a href="#实例引入" class="headerlink" title="实例引入"></a>实例引入</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line"><span class="comment"># html = etree.parse('./test.html', etree.HTMLParser())  #从文件读取</span></span><br><span class="line">result = etree.tostring(html) <span class="comment">#修正和补全HTML代码，但返回bytes类型</span></span><br><span class="line">print(result.decode(<span class="string">'utf-8'</span>)) <span class="comment">#转成str类型</span></span><br></pre></td></tr></table></figure><h4 id="节点选择"><a href="#节点选择" class="headerlink" title="节点选择"></a>节点选择</h4><ul><li>所有节点<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//*'</span>)   <span class="comment">#选择所有节点</span></span><br><span class="line"><span class="comment">#result = html.xpath('//li') #选择所有li节点</span></span><br><span class="line">print(result)</span><br><span class="line">print(result[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回形式是一个列表，每个元素是Element类型</span></span><br><span class="line">Output：</span><br><span class="line">[&lt;Element li at <span class="number">0x2257c34aa48</span>&gt;, &lt;Element li at <span class="number">0x2257c34aa88</span>&gt;]</span><br><span class="line">&lt;Element li at <span class="number">0x2257c34aa48</span>&gt;</span><br></pre></td></tr></table></figure></li><li>子节点<br>/是直接节点，//是所有节点。<br>如选择li节点的所有直接a子节点，可以用<code>//li/a</code>，<br>选择ul节点下的所有a子节点，可以用<code>//ul//a</code></li><li>父节点<br>首先选中href属性为link4.html的a节点，然后再获取其父节点，然后再获取其class属性。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/../@class'</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">result = html.xpath(<span class="string">'//a[@href="link4.html"]/parent::*/@class'</span>)</span><br></pre></td></tr></table></figure></li><li>按序选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/a/text()'</span>)           <span class="comment">#序号从1开始</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[last()]/a/text()'</span>)      <span class="comment">#选取最后一个</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[position()&lt;3]/a/text()'</span>)<span class="comment">#选取第1、2个</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[last()-2]/a/text()'</span>)    <span class="comment">#选取倒数第三个</span></span><br></pre></td></tr></table></figure>具体参考：<a href="http://www.w3school.com.cn/xpath/xpath_functions.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_functions.asp</a></li><li>节点轴选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::*'</span>)          <span class="comment">#选取第一个li节点的所有祖先节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/ancestor::div'</span>)        <span class="comment">#选取第一个li节点的祖先div节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/attribute::*'</span>)         <span class="comment">#获取第一个li节点的所有属性值</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/child::a'</span>)             <span class="comment">#child::直接子节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/descendant::span'</span>)     <span class="comment">#descendant::子孙节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following::*[2]'</span>)      <span class="comment">#following::当前结点之后的所有节点</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[1]/following-sibling::*'</span>) <span class="comment">#following-sibling::</span></span><br><span class="line">                                                    <span class="comment">#当前结点之后的所有同级节点</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="文本获取"><a href="#文本获取" class="headerlink" title="文本获取"></a>文本获取</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]/a/text()'</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]//text()'</span>)</span><br><span class="line"><span class="comment">#前者准确获取li&gt;a内的文本，</span></span><br><span class="line"><span class="comment">#而后者获取li内的所有文本，可能会获取到换行符之类的信息</span></span><br></pre></td></tr></table></figure><h4 id="属性操作"><a href="#属性操作" class="headerlink" title="属性操作"></a>属性操作</h4><ul><li>属性获取<br>获取所有li节点下所有a节点的href属性：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a/@href'</span>)</span><br></pre></td></tr></table></figure></li><li>属性匹配<br>通过属性筛选节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">'./test.html'</span>, etree.HTMLParser())</span><br><span class="line">result = html.xpath(<span class="string">'//li/a[@href="link1.html"]'</span>)</span><br></pre></td></tr></table></figure></li><li>属性多值匹配<br>用contains()函数：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li")]/a/text()'</span>)</span><br></pre></td></tr></table></figure></li><li>多属性匹配<br>用运算符and来连接：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;li class="li li-first" name="item"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li") and @name="item"]/a/text()'</span>)</span><br></pre></td></tr></table></figure>XPath中的运算符：<br><img src="https://s3.ax1x.com/2021/01/12/sJ0Uaj.png" alt="运算符及其介绍"></li></ul><h3 id="使用Beautiful-Soup"><a href="#使用Beautiful-Soup" class="headerlink" title="使用Beautiful Soup"></a>使用Beautiful Soup</h3><p>安装：<code>pip install beautifulsoup4</code></p><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>Beautiful Soup是Python的一个HTML或XML的解析库，可以用它来方便地从网页中提取数据.<br>官方解释如下：</p><ul><li>Beautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。</li><li>Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。不需要考虑编码方式，除非文档没有指定一个编码方式，这时仅需说明一下原始编码方式就可以了。</li><li>Beautiful Soup已成为和lxml、html6lib 一样出色的Python解释器，为用户灵活地提供不同的解析策略或强劲的速度。</li></ul><h4 id="解析器"><a href="#解析器" class="headerlink" title="解析器"></a>解析器</h4><p>Beautiful Soup在解析时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器。（推荐lxml）<br><img src="https://s3.ax1x.com/2021/01/12/sJ26Lq.png" alt="Beautiful Soup支持的解析器"></p><h4 id="基本用法-1"><a href="#基本用法-1" class="headerlink" title="基本用法"></a>基本用法</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)  <span class="comment">#会自动补全HTML代码</span></span><br><span class="line">print(soup.prettify())              <span class="comment">#将要解析的字符串以标准缩进格式输出</span></span><br><span class="line">print(soup.title.string)            <span class="comment">#获取title节点的文本</span></span><br></pre></td></tr></table></figure><h4 id="节点选择器"><a href="#节点选择器" class="headerlink" title="节点选择器"></a>节点选择器</h4><ul><li>选择元素<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.title)</span><br><span class="line">print(type(soup.title))</span><br><span class="line">print(soup.title.string)</span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.p)   <span class="comment">#选择第一个匹配的p节点</span></span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;</span></span><br><span class="line"><span class="string">&lt;class '</span>bs4.element.Tag<span class="string">'&gt;</span></span><br><span class="line"><span class="string">The Dormouse'</span>s story</span><br><span class="line">&lt;head&gt;&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line">&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure></li><li>提取信息<ul><li>获取名称<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.title.name)</span><br><span class="line">title</span><br></pre></td></tr></table></figure></li><li>获取属性<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.p.attrs)</span><br><span class="line">print(soup.p.attrs[<span class="string">'name'</span>])</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'class'</span>: [<span class="string">'title'</span>], <span class="string">'name'</span>: <span class="string">'dromouse'</span>&#125;</span><br><span class="line">dromouse</span><br></pre></td></tr></table></figure>或：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.p[<span class="string">'class'</span>])</span><br><span class="line">print(soup.p[<span class="string">'name'</span>])</span><br><span class="line"></span><br><span class="line">[<span class="string">'title'</span>]   <span class="comment">#class属性值可能有多个，所以返回列表</span></span><br><span class="line">dromouse</span><br></pre></td></tr></table></figure></li><li>获取内容<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.title.string)  <span class="comment">#选择第一个匹配的</span></span><br><span class="line">The Dormouse<span class="string">'s story</span></span><br></pre></td></tr></table></figure></li></ul></li><li>嵌套选择<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.head.title.string)   <span class="comment">#选择head内的title节点</span></span><br><span class="line">The Dormouse<span class="string">'s story</span></span><br></pre></td></tr></table></figure></li><li>关联选择<ul><li>子节点和子孙节点<br>直接子节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">soup.p.contents   <span class="comment">#返回包含各直接子节点的列表</span></span><br></pre></td></tr></table></figure>或<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, child <span class="keyword">in</span> enumerate(soup.p.children):   <span class="comment">#返回生成器类型</span></span><br><span class="line">  print(i, child)</span><br></pre></td></tr></table></figure>子孙节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, child <span class="keyword">in</span> enumerate(soup.p.descendants):</span><br><span class="line">  print(i, child)</span><br></pre></td></tr></table></figure></li><li>父节点和祖先节点<br>直接父节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.a.parent)</span><br></pre></td></tr></table></figure>祖先节点：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, parent <span class="keyword">in</span> enumerate(soup.p.parents):</span><br><span class="line">  print(i, parent)</span><br></pre></td></tr></table></figure></li><li>兄弟节点<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Next Sibling'</span>, soup.a.next_sibling)</span><br><span class="line">print(<span class="string">'Prev Sibling'</span>, soup.a.previous_sibling)</span><br><span class="line">print(<span class="string">'Next Siblings'</span>, list(enumerate(soup.a.next_siblings)))</span><br><span class="line">print(<span class="string">'Prev Siblings'</span>, list(enumerate(soup.a.previous_siblings)))</span><br></pre></td></tr></table></figure></li><li>提取信息<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">print(soup.a.next_sibling.string)</span><br><span class="line">print(list(soup.a.parents)[<span class="number">0</span>].attrs[<span class="string">'class'</span>])</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="方法选择器"><a href="#方法选择器" class="headerlink" title="方法选择器"></a>方法选择器</h4><ul><li>findall()<br>API：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">find_all(name, attrs, recursive, text, **kwargs)</span><br></pre></td></tr></table></figure>示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">print(soup.find_all(name=<span class="string">'ul'</span>))</span><br><span class="line">print(type(soup.find_all(name=<span class="string">'ul'</span>)[<span class="number">0</span>]))</span><br><span class="line"><span class="comment">#返回结果为列表类型，每个元素为bs4.element.Tag类型</span></span><br><span class="line"></span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">'id'</span>: <span class="string">'list-1'</span>&#125;))</span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">'name'</span>: <span class="string">'elements'</span>&#125;))</span><br><span class="line">或</span><br><span class="line">print(soup.find_all(id=<span class="string">'list-1'</span>))</span><br><span class="line">print(soup.find_all(class_=<span class="string">'element'</span>))    <span class="comment">#class_因为是关键字</span></span><br><span class="line"></span><br><span class="line">print(soup.find_all(text=re.compile(<span class="string">'link'</span>)))</span><br><span class="line"><span class="comment">#参数可以是字符串，也可以是正则表达式对象</span></span><br></pre></td></tr></table></figure></li><li>find()<br>返回第一个匹配的元素。</li><li>find_parents()和find_parent()：前者返回所有祖先节点，后者返回直接父节点。</li><li>find_next_siblings()和find_next_sibling()：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。</li><li>find_previous_siblings()和find_previous_sibling()：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点 。</li><li>find_all_next()和find_next()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。</li><li>find_all_previous()和find_previous()：前者返回节点前所有符合条件的节点，后者返回第一个符合条件的节点。</li></ul><h4 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><p>参考：<a href="http://www.w3school.com.cn/cssref/css_selectors.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/cssref/css_selectors.asp</a></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">print(soup.select(<span class="string">'.panel .panel-heading'</span>))   <span class="comment">#返回列表类型</span></span><br><span class="line">print(soup.select(<span class="string">'ul li'</span>))</span><br><span class="line">print(soup.select(<span class="string">'#list-2 .element'</span>))</span><br><span class="line">print(type(soup.select(<span class="string">'ul'</span>)[<span class="number">0</span>]))             <span class="comment">#返回bs4.element.Tag类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#嵌套选择</span></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul.select(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取属性</span></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul[<span class="string">'id'</span>])</span><br><span class="line">    print(ul.attrs[<span class="string">'id'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取文本</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> soup.select(<span class="string">'li'</span>):</span><br><span class="line">    print(<span class="string">'Get Text:'</span>, li.get_text())</span><br><span class="line">    print(<span class="string">'String:'</span>, li.string)</span><br></pre></td></tr></table></figure><h3 id="使用pyquery"><a href="#使用pyquery" class="headerlink" title="使用pyquery"></a>使用pyquery</h3><p>安装<code>pip install pyquery</code><br>pyquery的官方文档：<a href="http://pyquery.readthedocs.io" target="_blank" rel="noopener">http://pyquery.readthedocs.io</a></p><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">html = requests.get(url, headers=headers).text()</span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">print(doc(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#URL初始化</span></span><br><span class="line">doc = PyQuery(url=<span class="string">'http://xxyr.cc'</span>)</span><br><span class="line">print(doc(<span class="string">'title'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件初始化</span></span><br><span class="line">doc = PyQuery(filename=<span class="string">'demo.html'</span>)</span><br><span class="line">print(doc(<span class="string">'li'</span>))</span><br></pre></td></tr></table></figure><h4 id="基本CSS选择器"><a href="#基本CSS选择器" class="headerlink" title="基本CSS选择器"></a>基本CSS选择器</h4><p>关于CSS选择器的更多用法，可以参考：<a href="https://www.w3school.com.cn/css/css_selector_type.asp" target="_blank" rel="noopener">https://www.w3school.com.cn/css/css_selector_type.asp</a></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">print(doc(<span class="string">'#container .list li'</span>))         <span class="comment"># #id  .class</span></span><br><span class="line">print(type(doc(<span class="string">'#container .list li'</span>)))   </span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br></pre></td></tr></table></figure><h4 id="查找结点"><a href="#查找结点" class="headerlink" title="查找结点"></a>查找结点</h4><p>下面介绍一些常用的查询函数，这些函数和jQuery中函数的用法完全相同。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">items = doc(<span class="string">'.list'</span>)</span><br><span class="line">li = doc(<span class="string">'.list .item-0.active'</span>)</span><br><span class="line"></span><br><span class="line">lis1 = items.find(<span class="string">'li'</span>)          <span class="comment">#所有子孙节点</span></span><br><span class="line">lis2 = items.children(<span class="string">'.active'</span>) <span class="comment">#子节点</span></span><br><span class="line"></span><br><span class="line">container = items.parent()       <span class="comment">#获取直接父节点</span></span><br><span class="line">parents = items.parents()        <span class="comment">#获取所有祖先节点，可添加CSS选择器获取特定祖先节点</span></span><br><span class="line"></span><br><span class="line">bro = li.siblings()              <span class="comment">#获取所有兄弟节点，可添加CSS选择器获取特定兄弟节点</span></span><br></pre></td></tr></table></figure><h4 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">lis = doc(<span class="string">'li'</span>).items()</span><br><span class="line">print(type(lis))                <span class="comment">#&lt;class 'generator'&gt;</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> lis:</span><br><span class="line">    print(li, type(li))         <span class="comment">#&lt;class 'pyquery.pyquery.PyQuery'&gt;</span></span><br></pre></td></tr></table></figure><h4 id="获取信息"><a href="#获取信息" class="headerlink" title="获取信息"></a>获取信息</h4><ul><li>获取属性<br>提取到某个PyQuery类型的节点后，就可以调用attr()方法来获取属性：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">a = doc(<span class="string">'.item-0.active a'</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(type(a))</span><br><span class="line">print(a.attr(<span class="string">'href'</span>))   <span class="comment">#若a有多个，只返回第一个a的属性值</span></span><br><span class="line">print(a.attr.href)</span><br><span class="line"></span><br><span class="line">&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">link3</span>.<span class="title">html</span></span></span><br><span class="line"><span class="class"><span class="title">link3</span>.<span class="title">html</span></span></span><br></pre></td></tr></table></figure></li><li>获取文本<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'li'</span>)</span><br><span class="line">print(li.html())    <span class="comment">#返回第一个节点的内部HTML文本</span></span><br><span class="line">print(type(li.html()))</span><br><span class="line">print(li.text())    <span class="comment">#返回所有节点内部的纯文本，中间用一个空格分隔开</span></span><br><span class="line">print(type(li.text()))</span><br><span class="line"></span><br><span class="line">&lt;a href="link2.html"&gt;second item&lt;/a&gt;</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">second</span> <span class="title">item</span> <span class="title">third</span> <span class="title">item</span> <span class="title">fourth</span> <span class="title">item</span> <span class="title">fifth</span> <span class="title">item</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">str</span>'&gt;</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="节点操作"><a href="#节点操作" class="headerlink" title="节点操作"></a>节点操作</h4><p>pyquery提供了一系列方法来对节点进行动态修改，比如为某个节点添加一个 class，移除某个节点等，如append()、empty()和prepend()等方法，它们和jQuery的用法完全一致，详细的用法可以参考官方文档：<a href="http://pyquery.readthedocs.io/en/latest/api.html" target="_blank" rel="noopener">http://pyquery.readthedocs.io/en/latest/api.html</a></p><ul><li>addClass和removeClass<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'.item-0.active'</span>)</span><br><span class="line">li.removeClass(<span class="string">'active'</span>)</span><br><span class="line">li.addClass(<span class="string">'active'</span>)</span><br></pre></td></tr></table></figure></li><li>attr、text和html<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">li.attr(<span class="string">'name'</span>, <span class="string">'link'</span>)             <span class="comment">#修改或增加name属性值为link</span></span><br><span class="line">li.text(<span class="string">'changed item'</span>)             <span class="comment">#将li节点内部的文本替换为纯文本</span></span><br><span class="line">li.html(<span class="string">'&lt;span&gt;changed item&lt;/span&gt;'</span>)<span class="comment">#将li节点内部的文本替换为HTML文本</span></span><br></pre></td></tr></table></figure></li><li>remove()<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">wrap = doc(<span class="string">'.wrap'</span>)</span><br><span class="line">wrap.find(<span class="string">'p'</span>).remove()</span><br><span class="line">type(wrap.find(<span class="string">'p'</span>))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pyquery</span>.<span class="title">pyquery</span>.<span class="title">PyQuery</span>'&gt;</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="伪类选择器"><a href="#伪类选择器" class="headerlink" title="伪类选择器"></a>伪类选择器</h4><p>css选择器之所以强大，还有一个很重要的原因，就是支持多种多样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等。示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"></span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">doc = PyQuery(html)</span><br><span class="line">li = doc(<span class="string">'li:first-child'</span>)</span><br><span class="line">li = doc(<span class="string">'li:last-child'</span>)</span><br><span class="line">li = doc(<span class="string">'li:nth-child(2)'</span>)     <span class="comment">#第二个节点</span></span><br><span class="line">li = doc(<span class="string">'li:gt(2)'</span>)            <span class="comment">#第三个li之后的li节点</span></span><br><span class="line">li = doc(<span class="string">'li:nth-child(2n)'</span>)    <span class="comment">#偶数位置的节点</span></span><br><span class="line">li = doc(<span class="string">'li:contains(second)'</span>) <span class="comment">#包含second文本的li节点</span></span><br></pre></td></tr></table></figure><h2 id="Ch-4-数据存储"><a href="#Ch-4-数据存储" class="headerlink" title="Ch 4 数据存储"></a>Ch 4 数据存储</h2><h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><h4 id="TXT文本存储"><a href="#TXT文本存储" class="headerlink" title="TXT文本存储"></a>TXT文本存储</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'test.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">  file.write(<span class="string">'\n'</span>.join([question, author, answer]))</span><br><span class="line">  file.write(<span class="string">'\n'</span>+<span class="string">'='</span>*<span class="number">50</span>+<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><p>打开方式：</p><ul><li>r：以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。</li><li>rb：以二进制只读方式打开一个文件。文件指针将会放在文件的开头。</li><li>r+：以读写方式打开一个文件。文件指针将会放在文件的开头。</li><li>rb+：以二进制读写方式打开一个文件。文件指针将会放在文件的开头。</li><li>w：以写入方式打开一个文件。如果该文件已存在，则将其瞿盖。如果该文件不存在，则创建新文件。</li><li>wb：以二进制写入方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li><li>w+：以读写方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li><li>wb+：以二进制读写格式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件。</li><li>a：以追加方式打开一个文件。如果该文件已存在，文件指针将会放在文件结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，则创建新文件来写入。</li><li>ab：以二进制追加方式打开一个文件。如果该文件已存在，则文件指针将会放在文件结尾。如果该文件不存在，则创建新文件来写入。</li><li>a+：以读写方式打开一个文件。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果眩文件不存在，则创建新文件来读写。</li><li>ab+：以二进制追加方式打开一个文件。如果该文件已存在，则文件指针将会放在文件结尾。如果该文件不存在，则创建新文件用于读写。</li></ul><h4 id="JSON文件存储"><a href="#JSON文件存储" class="headerlink" title="JSON文件存储"></a>JSON文件存储</h4><p>JSON，全称为JavaScript Object Notation，也就是JavaScript对象标记，通过对象和数组的组合来表示数据，构造简洁但是结构化程度非常高，是一种轻量级的数据交换格式。<br>对象和数组：</p><ul><li>对象：在JavaScript中使用花括号｛｝包裹起来的内容，数据结构为｛ keyl : valuel, key2 : value2, … ｝的键值对结构。 在面向对象的语言中，key为对象的属性，value为对应的值。键名可以使用整数和字符串来表示。值的类型可以是任意类型。</li><li>数组：数组在JavaScript中是方括号［］包裹起来的内容，数据结构为［ “java”， “javascript”］的索引结构。在JavaScript中，数组是一种比较特殊的数据类型，它也可以像对象那样使用键值对，但还是索引用得多。同样，值的类型可以是任意类型。</li></ul><p>示例：loads()和dumps()</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'package.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    str = file.read()</span><br><span class="line">    data = json.loads(str)</span><br><span class="line">    print(type(data))</span><br><span class="line">    <span class="comment">#获取key为scripts里的key为build的value，若没有则返回test</span></span><br><span class="line">    print(data[<span class="string">'scripts'</span>].get(<span class="string">'build'</span>, <span class="string">'test'</span>))</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">dict</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">hexo</span> <span class="title">generate</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">with</span> <span class="title">open</span><span class="params">(<span class="string">'package.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span> <span class="title">as</span> <span class="title">file</span>:</span></span><br><span class="line">    <span class="comment">#将json对象转化为字符串，且缩进为2，指定编码为了显示中文</span></span><br><span class="line">    file.write(json.dumps(data, indent=<span class="number">2</span>, ensure_ascii=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure><p>若json文件包含多条记录，可以：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#逐行读取</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'moviebt.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file.readlines():</span><br><span class="line">        data = json.loads(line)                 <span class="comment">#data为字典</span></span><br><span class="line">        print(data.get(<span class="string">'result'</span>).get(<span class="string">'title'</span>))</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">    data = [json.loads(line) <span class="keyword">for</span> line <span class="keyword">in</span> file]  <span class="comment">#data为列表</span></span><br><span class="line">    print(data[<span class="number">0</span>].get(<span class="string">'result'</span>).get(<span class="string">'title'</span>))</span><br></pre></td></tr></table></figure><p>参见：<br><a href="https://stackoverflow.com/questions/48140858/json-decoder-jsondecodeerror-extra-data-line-2-column-1-char-190" target="_blank" rel="noopener">json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 190) [duplicate]</a><br><a href="https://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data" target="_blank" rel="noopener">Python json.loads shows ValueError: Extra data</a></p><h4 id="CSV文件存储"><a href="#CSV文件存储" class="headerlink" title="CSV文件存储"></a>CSV文件存储</h4><p>CSV，全称为Comma-Separated Values，中文可以叫作逗号分隔值或字符分隔值，其文件以纯文本形式存储表格数据。</p><ul><li>写入<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open (<span class="string">'data.csv'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">  <span class="comment">#delimiter指定分隔符 lineterminator指定行终止符，默认\n</span></span><br><span class="line">  writer = csv.writer(csvfile, delimiter=<span class="string">' '</span>, lineterminator=<span class="string">'\n\n'</span>)</span><br><span class="line">  writer.writerow([<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'age'</span>])</span><br><span class="line">  writer.writerow([<span class="string">'10001'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>])</span><br><span class="line">  writer.writerow([<span class="string">'10002'</span>, <span class="string">'Bob'</span>, <span class="number">22</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">#写入多行</span></span><br><span class="line">  writer.writerows([<span class="string">'10001'</span>, <span class="string">'Mike'</span>, <span class="number">20</span>], [<span class="string">'10002'</span>, <span class="string">'Bob'</span>, <span class="number">22</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">#字典写入</span></span><br><span class="line">  fieldnames = [<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'age'</span>)</span><br><span class="line">  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)</span><br><span class="line">  writer. writeheader()</span><br><span class="line">  writer.writerow(&#123;<span class="string">'id'</span>: <span class="string">'10001'</span>, <span class="string">'name'</span> :<span class="string">'Mike'</span>, <span class="string">'age'</span> : <span class="number">20</span>&#125;)</span><br><span class="line">  writer.writerow(&#123;<span class="string">'id'</span>: <span class="string">'10002'</span>, <span class="string">'name'</span> : <span class="string">'Bob'</span>, <span class="string">'age'</span> : <span class="number">22</span>&#125;)</span><br><span class="line">  writer.writerow(&#123;<span class="string">'id'</span>: <span class="string">'10003'</span>, <span class="string">'name'</span> :<span class="string">'Jordan'</span>, <span class="string">'age'</span> : <span class="number">21</span>&#125;)</span><br></pre></td></tr></table></figure></li><li>读取<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>,<span class="string">'r'</span>, encoding=<span class="string">'utf 8'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">  reader = csv.reader(csvfile)</span><br><span class="line">  <span class="comment"># fileData = list(reader)   #fileData[0][0]</span></span><br><span class="line">  <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">    print(<span class="string">'row'</span>+str(row.line_num)+str(row))</span><br><span class="line"></span><br><span class="line"><span class="comment">#或</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read csv(<span class="string">'data.csv'</span>)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure></li></ul><h3 id="关系型数据库存储"><a href="#关系型数据库存储" class="headerlink" title="关系型数据库存储"></a>关系型数据库存储</h3><p>关系型数据库是基于关系模型的数据库，而关系模型是通过二维表来保存的，所以它的存储方式就是行列组成的表，每一列是一个字段，每一行是一条记录。<br>表可以看作某个实体的集合，而实体之间存在联系，这就需要表与表之间的关联关系来体现，如主键外键的关联关系。多个表组成一个数据库，也就是关系型数据库。</p><h4 id="MySQL存储"><a href="#MySQL存储" class="headerlink" title="MySQL存储"></a>MySQL存储</h4><p>相关链接:</p><ul><li>GitHub: <a href="https://github.com/PyMySQL/PyMySQL" target="_blank" rel="noopener">https://github.com/PyMySQL/PyMySQL</a></li><li>官方文档：<a href="http://pymysql.readthedocs.io/" target="_blank" rel="noopener">http://pymysql.readthedocs.io/</a></li><li>PyPI: <a href="https://pypi.python.org/pypi/PyMySQL" target="_blank" rel="noopener">https://pypi.python.org/pypi/PyMySQL</a></li></ul><p>安装：<code>pip install pymysql</code><br>相关操作:</p><ul><li>连接、创建数据库<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>,user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>)</span><br><span class="line">cursor = db.cursor()        <span class="comment">#获得操作游标</span></span><br><span class="line">cursor.execute(<span class="string">'SELECT VERSION()'</span>)</span><br><span class="line">data = cursor.fetchone()    <span class="comment">#获得第一条数据</span></span><br><span class="line">print(<span class="string">'Database version:'</span>, data)</span><br><span class="line">cursor.execute(<span class="string">"CREATE DATABASE spiders DEFAULT CHARACTER SET utf8"</span>)</span><br><span class="line">db.close()</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">Database version: (<span class="string">'5.7.29-log'</span>,)</span><br></pre></td></tr></table></figure></li><li>创建表<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS students (id VARCHAR(255) NOT NULL, name VARCHAR(255) NOT NULL, age INT NOT NULL, PRIMARY KEY (id))'</span></span><br><span class="line">cursor.execute(sql)</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure></li><li>插入数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">id = <span class="string">'20120001'</span></span><br><span class="line">user = <span class="string">'Bob'</span></span><br><span class="line">age = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'INSERT INTO students(id, name, age) values(%s, %s, %s)'</span>  <span class="comment">#采用格式化符%而非字符串拼接</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql, (id, user, age))</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>改进,根据字典动态构造：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20120001'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span></span><br><span class="line">&#125;</span><br><span class="line">table = <span class="string">'students'</span></span><br><span class="line">keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">sql = <span class="string">'INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES (&#123;values&#125;)'</span>.format(table=table, keys=keys, values=values)</span><br><span class="line"><span class="comment">#INSERT INTO students(id , name, age) VALUES (%s, %s, %s)</span></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">if</span> cursor.execute(sql, tuple(data.values())):</span><br><span class="line">        print(<span class="string">'Successful'</span>)</span><br><span class="line">        db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Failed'</span>)</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>这里涉及事务的问题，事务的特性如下：<ul><li>原子性（atomicity)：事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做。</li><li>一致性（consistency)：事务必须使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。</li><li>隔离性（isolation)：一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。</li><li>持久性（durability)：持续性也称永久性（permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。</li></ul></li><li>更新数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">sql = <span class="string">'UPDATE students SET age = %s WHERE name = %s'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql, (<span class="number">25</span>, <span class="string">'Bob'</span>))</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br></pre></td></tr></table></figure>改进,字典传值+去重:<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20120001'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">table = <span class="string">'students'</span></span><br><span class="line">keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'INSERT INTO &#123;table&#125;(&#123;keys&#125;) VALUES (&#123;values&#125;) ON DUPLICATE KEY UPDATE'</span>.format(table=table, keys=keys,</span><br><span class="line">                                                                                     values=values)</span><br><span class="line"><span class="comment">#INSERT INTO students(id, name, age) VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE</span></span><br><span class="line">update = <span class="string">','</span>.join([<span class="string">" &#123;key&#125; = %s"</span>.format(key=key) <span class="keyword">for</span> key <span class="keyword">in</span> data])</span><br><span class="line">sql += update</span><br><span class="line"><span class="comment">#INSERT INTO students(id, name, age) VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE id = %s, name = %s, age = %s</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">if</span> cursor.execute(sql, tuple(data.values()) * <span class="number">2</span>):</span><br><span class="line">        print(<span class="string">'Successful'</span>)</span><br><span class="line">        db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Failed'</span>)</span><br><span class="line">    db.rollback()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure></li><li>删除数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">table = <span class="string">'students'</span></span><br><span class="line">condition = <span class="string">'age &gt; 20'</span></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">sql = <span class="string">'DELETE FROM  &#123;table&#125; WHERE &#123;condition&#125;'</span>.format(table=table, condition=condition)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    db.rollback()</span><br><span class="line"></span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure></li><li>查询数据<br>逐行读取(注意游标类似全局变量)<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'SELECT * FROM students WHERE age &gt;= 20'</span></span><br><span class="line">db = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, password=<span class="string">'123456'</span>, port=<span class="number">3306</span>, db=<span class="string">'spiders'</span>)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    print(<span class="string">'Count:'</span>, cursor.rowcount)</span><br><span class="line">    row = cursor.fetchone()</span><br><span class="line">    <span class="comment"># results = cursor.fetchall()</span></span><br><span class="line">    <span class="keyword">while</span> row:</span><br><span class="line">        print(<span class="string">'Row:'</span>, row)</span><br><span class="line">        row = cursor.fetchone()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'Error'</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="非关系型数据库存储"><a href="#非关系型数据库存储" class="headerlink" title="非关系型数据库存储"></a>非关系型数据库存储</h3><p>NoSQL,全称Not Only SQL，意为不仅仅是SQL，泛指非关系型数据库。NoSQL是基于键值对的，且不需要经过SQL层的解析，数据之间没有耦合性，性能非常高。<br>非关系型数据库又可细分如下。</p><ul><li>键值存储数据库：代表有Redis、Voldemort和Oracle BDB等。</li><li>列存储数据库：代表有Cassandra、HBase和Riak等。</li><li>文档型数据库：代表有CouchDB和MongoDB等。</li><li>图形数据库：代表有Neo4J、lnfoGrid和Infinite Graph等。</li></ul><p>对于爬虫的数据存储来说一条数据可能存在某些字段提取失败而缺失的情况，而且数据可能随时调整。另外，数据之间还存在嵌套关系。如果使用关系型数据库存储，一是需要提前建表，二是如果存在数据嵌套关系的话，需要进行序列化操作才可以存储，非常不方便。如果用了非关系型数据库，就可以避免一些麻烦，更简单高效。</p><h4 id="MongoDB存储"><a href="#MongoDB存储" class="headerlink" title="MongoDB存储"></a>MongoDB存储</h4><p>MongoDB 是由C++语言编写的非关系型数据库，是一个基于分布式文件存储的开源数据库系统，其内容存储形式类似JSON对象，字段值可以包含其他文档、数组及文档数组，非常灵活。<br>相关链接:</p><ul><li>GitHub: <a href="https://github.com/mongodb/mongo-python-driver" target="_blank" rel="noopener">https://github.com/mongodb/mongo-python-driver</a></li><li>官方文档：<a href="https://api.mongodb.com/python/current/" target="_blank" rel="noopener">https://api.mongodb.com/python/current/</a></li><li>PyPI: <a href="https://pypi.python.org/pypi/pymongo" target="_blank" rel="noopener">https://pypi.python.org/pypi/pymongo</a><br>安装:<code>pip install pymongo</code><br>相关操作:</li><li>初始化<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"></span><br><span class="line"><span class="comment">#连接MongoDB</span></span><br><span class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</span><br><span class="line"><span class="comment"># client = MongoClient('mongodb://localhost:27017/')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#指定数据库</span></span><br><span class="line">db = client.test</span><br><span class="line"><span class="comment"># db = client['test']</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#指定集合</span></span><br><span class="line">collection = db.students</span><br><span class="line"><span class="comment"># collection = db['students']</span></span><br></pre></td></tr></table></figure></li><li>插入数据<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">student1 = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170101'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Jordan'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">student2 = &#123;</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'20170202'</span>,</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Mike'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">'gender'</span>: <span class="string">'male'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#insert返回一个ObjectId类型的_id值</span></span><br><span class="line">result = collection.insert(student1)</span><br><span class="line">result = collection.insert([student1, student2])</span><br><span class="line"></span><br><span class="line"><span class="comment">#官方推荐使用更加严格的方法</span></span><br><span class="line"><span class="comment">#返回的是InsertOneResult对象</span></span><br><span class="line">result = collection.insert_one(student1)</span><br><span class="line">print(result.inserted_id)</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回的是InsertManyResult对象</span></span><br><span class="line">result = collection.insert_many([student1, student2])</span><br><span class="line">print(result.inserted_ids)</span><br></pre></td></tr></table></figure></li><li>查询<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">查询单条数据</span><br><span class="line">result = collection.find_one(&#123;<span class="string">'name'</span>: <span class="string">'Mike'</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据ObjectId查询</span></span><br><span class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</span><br><span class="line"></span><br><span class="line">result = collection.find_one(&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'593278c115c2602667ec6bae'</span>)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多条数据查询</span></span><br><span class="line">results = collection.find(&#123;<span class="string">'age'</span>: <span class="number">20</span>&#125;)  <span class="comment">#返回Cursor类型，相当于生成器</span></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 条件查询</span></span><br><span class="line">results = collection.find(&#123;<span class="string">'age'</span>: &#123;<span class="string">'$gt'</span>: <span class="number">20</span>&#125;&#125;)</span><br><span class="line"><span class="comment"># 正则匹配查询</span></span><br><span class="line">results = collection.find(&#123;<span class="string">'age'</span>: &#123;<span class="string">'$Regex'</span>: <span class="string">'^M.*'</span>&#125;&#125;)</span><br></pre></td></tr></table></figure>比较符号表如下:<br><img src="https://s3.ax1x.com/2021/01/22/soZbse.png" alt=""></li></ul><p>功能符号表如下:<br><img src="https://s3.ax1x.com/2021/01/22/soZXdA.png" alt=""></p><ul><li>计数<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">count = collection.find(<span class="string">'age'</span>: <span class="number">20</span>&#125;).count()</span><br></pre></td></tr></table></figure></li><li>排序<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING)</span><br></pre></td></tr></table></figure></li><li>偏移<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 偏移2，忽略前两个元素，得到第三个及以后的元素</span></span><br><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING).skip(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定个数</span></span><br><span class="line">results = collection.find().sort(<span class="string">'name'</span>, pymongo.ASCENDING).skip(<span class="number">2</span>).limit(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据量大时采用以下办法</span></span><br><span class="line">collection.find(&#123;<span class="string">'_id'</span>: &#123;<span class="string">'$gt'</span>: ObjectId(<span class="string">'593278c815c2602678bb2b8d'</span>)&#125;&#125;)</span><br></pre></td></tr></table></figure></li><li>更新<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">condition = &#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;</span><br><span class="line">student = collection.find_one(condition)</span><br><span class="line">student[<span class="string">'age'</span>] = <span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回结果是字典形式，ok代表执行成功，nModified代表影响的数据条数</span></span><br><span class="line">result = collection.update(condition, student)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只更新该字段，不影响其他已存在字段</span></span><br><span class="line">result = collection.update_one(condition, &#123;<span class="string">'$set'</span>: student&#125;)</span><br><span class="line">print(result.matched_count, result.modified_count)</span><br></pre></td></tr></table></figure></li><li>删除<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">result = collection.remove(&#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;)</span><br><span class="line"></span><br><span class="line">result = collection.delete_one(&#123;<span class="string">'name'</span>: <span class="string">'Kevin'</span>&#125;)</span><br><span class="line">result = collection.delete_many(&#123;<span class="string">'age'</span>: &#123;<span class="string">'$lt'</span>: <span class="number">25</span>&#125;&#125;)</span><br><span class="line">print(result.deleted_count)</span><br></pre></td></tr></table></figure></li><li>其他操作<ul><li>另外，PyMongo还提供了一些组合方法，如find_one_and_delete()、find one and_replace()和find_one_and_update()，它们是查找后删除、替换和更新操作，其用法与上述方法基本一致。</li><li>还可以对索引进行操作，相关方法有create_index()、create_indexes()和drop_index()等。</li><li>关于PyMongo的详细用法，可以参见官方文档：<br><a href="http://api.mongodb.com/python/current/api/pymongo/collection.html" target="_blank" rel="noopener">http://api.mongodb.com/python/current/api/pymongo/collection.html</a></li><li>另外，还有对数据库和集合本身等的一些操作，可以参见官方文档：<br><a href="http://api.mongodb.com/python/current/api/pymongo" target="_blank" rel="noopener">http://api.mongodb.com/python/current/api/pymongo</a></li></ul></li></ul><h4 id="Redis存储"><a href="#Redis存储" class="headerlink" title="Redis存储"></a>Redis存储</h4><p>GitHub: <a href="https://github.com/andymccurdy/redis-py" target="_blank" rel="noopener">https://github.com/andymccurdy/redis-py</a><br>官方文档：<a href="https://redis-py.readthedocs.io/" target="_blank" rel="noopener">https://redis-py.readthedocs.io/</a></p><ul><li>初始化<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> StrictRedis</span><br><span class="line"></span><br><span class="line">redis = StrictRedis(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, password=<span class="string">'foobared'</span>)</span><br><span class="line">redis.set(<span class="string">'name'</span>, <span class="string">'Bob'</span>)</span><br><span class="line">print(redis.get(<span class="string">'name'</span>))</span><br></pre></td></tr></table></figure>或<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> StrictRedis, ConnectionPool</span><br><span class="line"></span><br><span class="line">pool = ConnectionPool(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, password=<span class="string">'foobared'</span>)</span><br><span class="line">redis = StrictRedis(connection_pool=pool)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过URL构建pool</span></span><br><span class="line">url = <span class="string">'redis://:foobared@localhost:6379/0'</span></span><br><span class="line">pool = ConnectionPool.from_url(url)</span><br><span class="line">redis = StrictRedis(connection_pool=pool)</span><br></pre></td></tr></table></figure></li><li>键操作</li><li>字符串操作</li><li>列表操作</li><li>散列操作</li><li>集合操作</li><li>有序集合操作</li><li>RedisDump<ul><li>redis-dump</li><li>redis-load</li></ul></li></ul><h2 id="Ch-5-Ajax数据爬取"><a href="#Ch-5-Ajax数据爬取" class="headerlink" title="Ch 5 Ajax数据爬取"></a>Ch 5 Ajax数据爬取</h2><h3 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h3><h3 id="Ajax分析方法"><a href="#Ajax分析方法" class="headerlink" title="Ajax分析方法"></a>Ajax分析方法</h3><h3 id="Ajax结果提取"><a href="#Ajax结果提取" class="headerlink" title="Ajax结果提取"></a>Ajax结果提取</h3><h3 id="分析Ajax爬取今日头条节拍美图"><a href="#分析Ajax爬取今日头条节拍美图" class="headerlink" title="分析Ajax爬取今日头条节拍美图"></a>分析Ajax爬取今日头条节拍美图</h3><h2 id="Ch-6-动态渲染页面爬取"><a href="#Ch-6-动态渲染页面爬取" class="headerlink" title="Ch 6 动态渲染页面爬取"></a>Ch 6 动态渲染页面爬取</h2><h3 id="Selenium的使用"><a href="#Selenium的使用" class="headerlink" title="Selenium的使用"></a>Selenium的使用</h3><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    input = browser.find_element_by_id(<span class="string">'kw'</span>)</span><br><span class="line">    input.send_keys(<span class="string">'Python'</span>)</span><br><span class="line">    input.send_keys(Keys.ENTER)</span><br><span class="line">    wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">    wait.until(EC.presence_of_element_located((By.ID, <span class="string">'content_left'</span>)))</span><br><span class="line">    print(browser.current_url)</span><br><span class="line">    print(browser.get_cookies())</span><br><span class="line">    print(browser.page_source)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    browser.close()</span><br></pre></td></tr></table></figure><p>浏览器首先会跳转到百度，然后在搜索框中输入 Python，接着跳转到搜索结果页。</p><h4 id="声明浏览器对象"><a href="#声明浏览器对象" class="headerlink" title="声明浏览器对象"></a>声明浏览器对象</h4><p>Selenium 支持非常多的浏览器，如 Chrome、Firefox、Edge 等，还有 Android、BlackBerry 等手机端的浏览器。另外，也支持无界面浏览器 PhantomJS。用如下方式初始化：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser = webdriver.Firefox()</span><br><span class="line">browser = webdriver.Edge()</span><br><span class="line">browser = webdriver.PhantomJS()</span><br><span class="line">browser = webdriver.Safari()</span><br></pre></td></tr></table></figure><h4 id="访问页面"><a href="#访问页面" class="headerlink" title="访问页面"></a>访问页面</h4><p>可以用 get() 方法来请求网页，参数传入链接 URL 即可。比如，这里用 get() 方法访问淘宝，然后打印出源代码，代码如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">print(browser.page_source)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h4 id="查找节点"><a href="#查找节点" class="headerlink" title="查找节点"></a>查找节点</h4><ul><li>单个节点<br>比如，<code>find_element_by_name()</code> 是根据 name 值获取，<code>find_element_by_id()</code> 是根据 id 获取。另外，还有根据 XPath、CSS 选择器等获取的方式。<br>代码实现：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">input_first = browser.find_element_by_id(<span class="string">'q'</span>)</span><br><span class="line">input_second = browser.find_element_by_css_selector(<span class="string">'#q'</span>)</span><br><span class="line">input_third = browser.find_element_by_xpath(<span class="string">'//*[@id="q"]'</span>)</span><br><span class="line">print(input_first, input_second, input_third)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure>这里列出所有获取单个节点的方法：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">find_element_by_id</span><br><span class="line">find_element_by_name</span><br><span class="line">find_element_by_xpath</span><br><span class="line">find_element_by_link_text</span><br><span class="line">find_element_by_partial_link_text</span><br><span class="line">find_element_by_tag_name</span><br><span class="line">find_element_by_class_name</span><br><span class="line">find_element_by_css_selector</span><br></pre></td></tr></table></figure>另外，Selenium 还提供了通用方法 <code>find_element()</code>，它需要传入两个参数：查找方式 By 和值。实际上，它就是 <code>find_element_by_id()</code> 这种方法的通用函数版本，比如 <code>find_element_by_id(id)</code> 就等价于 <code>find_element(By.ID, id)</code>，二者得到的结果完全一致。示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">input_first = browser.find_element(By.ID, <span class="string">'q'</span>)</span><br></pre></td></tr></table></figure></li><li>多个节点<br>使用 <code>find_elements()</code> ，示例：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">lis = browser.find_elements_by_css_selector(<span class="string">'.service-bd li'</span>)</span><br><span class="line">print(lis)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure>得到的内容变成了列表类型，列表中的每个节点都是 <code>WebElement</code> 类型。<br>这里列出所有获取多个节点的方法：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">find_elements_by_id</span><br><span class="line">find_elements_by_name</span><br><span class="line">find_elements_by_xpath</span><br><span class="line">find_elements_by_link_text</span><br><span class="line">find_elements_by_partial_link_text</span><br><span class="line">find_elements_by_tag_name</span><br><span class="line">find_elements_by_class_name</span><br><span class="line">find_elements_by_css_selector</span><br></pre></td></tr></table></figure>也可以直接用 find_elements() 方法来选择，这时可以这样写：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">lis = browser.find_elements(By.CSS_SELECTOR, <span class="string">'.service-bd li'</span>)</span><br></pre></td></tr></table></figure></li></ul><h4 id="节点交互"><a href="#节点交互" class="headerlink" title="节点交互"></a>节点交互</h4><p>Selenium 可以驱动浏览器来执行一些操作，也就是说可以让浏览器模拟执行一些动作。比较常见的用法有：输入文字时用 send_keys() 方法，清空文字时用 clear() 方法，点击按钮时用 click() 方法。示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">input = browser.find_element_by_id(<span class="string">'q'</span>)</span><br><span class="line">input.send_keys(<span class="string">'iPhone'</span>)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line">input.clear()</span><br><span class="line">input.send_keys(<span class="string">'iPad'</span>)</span><br><span class="line">button = browser.find_element_by_class_name(<span class="string">'btn-search'</span>)</span><br><span class="line">button.click()</span><br></pre></td></tr></table></figure><p>更多的操作可以参见官方文档的交互动作介绍：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement。</a></p><h4 id="动作链"><a href="#动作链" class="headerlink" title="动作链"></a>动作链</h4><p>另外一些操作，没有特定的执行对象，比如鼠标拖曳、键盘按键等，这些动作用另一种方式来执行，那就是动作链。<br>比如，现在实现一个节点的拖曳操作，将某个节点从一处拖曳到另外一处，可以这样实现：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ActionChains</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">browser.switch_to.frame(<span class="string">'iframeResult'</span>)</span><br><span class="line">source = browser.find_element_by_css_selector(<span class="string">'#draggable'</span>)</span><br><span class="line">target = browser.find_element_by_css_selector(<span class="string">'#droppable'</span>)</span><br><span class="line">actions = ActionChains(browser)</span><br><span class="line">actions.drag_and_drop(source, target)</span><br><span class="line">actions.perform()</span><br></pre></td></tr></table></figure><p>更多的动作链操作参考官方文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains。</a></p><h4 id="执行-JavaScript"><a href="#执行-JavaScript" class="headerlink" title="执行 JavaScript"></a>执行 JavaScript</h4><p>对于某些操作，Selenium API 并没有提供。比如，下拉进度条，它可以直接模拟运行 JavaScript，此时使用 execute_script() 方法即可实现，代码如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight)'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'alert("To Bottom")'</span>)</span><br></pre></td></tr></table></figure><h4 id="获取节点信息"><a href="#获取节点信息" class="headerlink" title="获取节点信息"></a>获取节点信息</h4><ul><li>获取属性<br>使用 get_attribute() 方法来获取节点的属性，示例如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'https://www.zhihu.com/explore'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">logo = browser.find_element_by_id(<span class="string">'zh-top-link-logo'</span>)</span><br><span class="line">print(logo)</span><br><span class="line">print(logo.get_attribute(<span class="string">'class'</span>))</span><br></pre></td></tr></table></figure></li><li>获取文本值<br>每个 WebElement 节点都有 text 属性，直接调用这个属性就可以得到节点内部的文本信息，这相当于 Beautiful Soup 的 <code>get_text()</code> 方法、pyquery 的 <code>text()</code> 方法，示例如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'https://www.zhihu.com/explore'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'zu-top-add-question'</span>)</span><br><span class="line">print(input.text)</span><br></pre></td></tr></table></figure></li><li>获取 id、位置、标签名和大小<br>示例如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'https://www.zhihu.com/explore'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'zu-top-add-question'</span>)</span><br><span class="line">print(input.id)</span><br><span class="line">print(input.location)</span><br><span class="line">print(input.tag_name)</span><br><span class="line">print(input.size)</span><br></pre></td></tr></table></figure><h4 id="切换-Frame"><a href="#切换-Frame" class="headerlink" title="切换 Frame"></a>切换 Frame</h4>网页中有一种节点叫作 iframe，也就是子 Frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。Selenium 打开页面后，它默认是在父级 Frame 里面操作，而此时如果页面中还有子 Frame，它是不能获取到子 Frame 里面的节点的。这时就需要使用 switch_to.frame() 方法来切换 Frame。示例如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> NoSuchElementException</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">browser.switch_to.frame(<span class="string">'iframeResult'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    logo = browser.find_element_by_class_name(<span class="string">'logo'</span>)</span><br><span class="line"><span class="keyword">except</span> NoSuchElementException:</span><br><span class="line">    print(<span class="string">'NO LOGO'</span>)</span><br><span class="line">browser.switch_to.parent_frame()</span><br><span class="line">logo = browser.find_element_by_class_name(<span class="string">'logo'</span>)</span><br><span class="line">print(logo)</span><br><span class="line">print(logo.text)</span><br></pre></td></tr></table></figure></li></ul><h4 id="延时等待"><a href="#延时等待" class="headerlink" title="延时等待"></a>延时等待</h4><ul><li><p>隐式等待<br>当使用隐式等待执行测试的时候，如果 Selenium 没有在 DOM 中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。换句话说，当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是 0。示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.implicitly_wait(<span class="number">10</span>)</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'zu-top-add-question'</span>)</span><br><span class="line">print(input)</span><br></pre></td></tr></table></figure></li><li><p>显式等待<br>指定要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，就返回查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常。示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com/'</span>)</span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">input = wait.until(EC.presence_of_element_located((By.ID, <span class="string">'q'</span>)))</span><br><span class="line">button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, <span class="string">'.btn-search'</span>)))</span><br><span class="line">print(input, button)</span><br></pre></td></tr></table></figure><p>这里首先引入 WebDriverWait 这个对象，指定最长等待时间，然后调用它的 until() 方法，传入要等待条件 expected_conditions。比如，这里传入了 presence_of_element_located 这个条件，代表节点出现的意思，其参数是节点的定位元组，也就是 ID 为 q 的节点搜索框。</p><p>这样可以做到的效果就是，在 10 秒内如果 ID 为 q 的节点（即搜索框）成功加载出来，就返回该节点；如果超过 10 秒还没有加载出来，就抛出异常。</p><p>对于按钮，可以更改一下等待条件，比如改为 element_to_be_clickable，也就是可点击，所以查找按钮时查找 CSS 选择器为.btn-search 的按钮，如果 10 秒内它是可点击的，也就是成功加载出来了，就返回这个按钮节点；如果超过 10 秒还不可点击，也就是没有加载出来，就抛出异常。</p><p>等待条件还有很多，比如判断标题内容，判断某个节点内是否出现了某文字等。如下所示：<br><img src="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210207-165415-0596.png" alt=""><br>更多等待条件的参数及用法，参考官方文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions。</a></p></li></ul><h4 id="前进和后退"><a href="#前进和后退" class="headerlink" title="前进和后退"></a>前进和后退</h4><p>示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com/'</span>)</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com/'</span>)</span><br><span class="line">browser.get(<span class="string">'https://www.python.org/'</span>)</span><br><span class="line">browser.back()</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line">browser.forward()</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h4 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a>Cookies</h4><p>获取、添加、删除 Cookies 等。示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">print(browser.get_cookies())</span><br><span class="line">browser.add_cookie(&#123;<span class="string">'name'</span>: <span class="string">'name'</span>, <span class="string">'domain'</span>: <span class="string">'www.zhihu.com'</span>, <span class="string">'value'</span>: <span class="string">'germey'</span>&#125;)</span><br><span class="line">print(browser.get_cookies())</span><br><span class="line">browser.delete_all_cookies()</span><br><span class="line">print(browser.get_cookies())</span><br></pre></td></tr></table></figure><h4 id="选项卡管理"><a href="#选项卡管理" class="headerlink" title="选项卡管理"></a>选项卡管理</h4><p>示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'window.open()'</span>)</span><br><span class="line">print(browser.window_handles)</span><br><span class="line">browser.switch_to_window(browser.window_handles[<span class="number">1</span>])</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line">browser.switch_to_window(browser.window_handles[<span class="number">0</span>])</span><br><span class="line">browser.get(<span class="string">'https://python.org'</span>)</span><br></pre></td></tr></table></figure><h4 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h4><p>节点未找到的异常，示例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException, NoSuchElementException</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">except</span> TimeoutException:</span><br><span class="line">    print(<span class="string">'Time Out'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.find_element_by_id(<span class="string">'hello'</span>)</span><br><span class="line"><span class="keyword">except</span> NoSuchElementException:</span><br><span class="line">    print(<span class="string">'No Element'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    browser.close()</span><br></pre></td></tr></table></figure><p>更多的异常类，可以参考官方文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions。</a></p><h4 id="Chrome-Headless-模式"><a href="#Chrome-Headless-模式" class="headerlink" title="Chrome Headless 模式"></a>Chrome Headless 模式</h4><p>从 Chrome 59 版本开始，已经开始支持 Headless 模式，也就是无界面模式，这样爬取的时候就不会弹出浏览器了。如果要使用此模式，请把 Chrome 升级到 59 版本及以上。启用 Headless 模式的方式如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">chrome_options = webdriver.ChromeOptions()</span><br><span class="line">chrome_options.add_argument(<span class="string">'--headless'</span>)</span><br><span class="line">browser = webdriver.Chrome(chrome_options=chrome_options)</span><br></pre></td></tr></table></figure><h3 id="Splash的使用"><a href="#Splash的使用" class="headerlink" title="Splash的使用"></a>Splash的使用</h3><p>Splash是一个JavaScript渲染服务，是一个带有HTTP API的轻量级浏览器，同时对接了Python中的Twisted和QT库。<br>安装运行：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker run -p <span class="number">8050</span>:<span class="number">8050</span> scrapinghub/splash</span><br></pre></td></tr></table></figure><p>以守护态运行：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker run -d -p <span class="number">8050</span>:<span class="number">8050</span> scrapinghub/splash</span><br></pre></td></tr></table></figure><p>Splash 使用未绑定的内存缓冲，因此它最终会占用所有的内存。一个解决的办法是在它占用过量内存时进行重启。 Splash中的 –maxrss 参数正是这个作用。您还可以在Docker中添加 –memory 选项。</p><p>在正式产品中固定使用同一个版本的Splash会是一个好的做法。相比于使用 scrapinghub/splash 来说 使用像 scrapinghub/splash:2.0 这样的可能会更好</p><p>如果您希望设置Splash使用的最大内存为4GB，并且加上守护进程，崩溃重启这些特性，您可以使用下面的命令</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker run -d -p <span class="number">8050</span>:<span class="number">8050</span> --memory=<span class="number">4.5</span>G --restart=always scrapinghub/splash:<span class="number">3.5</span> --maxrss <span class="number">4000</span></span><br></pre></td></tr></table></figure><p>当然，您可能需要一个负载均衡。这样您可以在Splash中进行与Aquarium或者HAProxy 相关的配置</p><h4 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h4><ul><li>利用Splash，可以实现如下功能：<ul><li>异步方式处理多个网页渲染过程；</li><li>获取渲染后的页面的源代码或截图；</li><li>通过关闭图片渲染或者使用Adblock规则来加快页面渲染速度；</li><li>可执行特定的JavaScript脚本；</li><li>可通过Lua脚本来控制页面渲染过程；</li><li>获取渲染的详细过程并通过HAR（HTTP Archive）格式呈现。</li></ul></li></ul><h4 id="Splash-Lua脚本"><a href="#Splash-Lua脚本" class="headerlink" title="Splash Lua脚本"></a>Splash Lua脚本</h4><p>Splash可以通过Lua脚本执行一系列渲染操作，这样我们就可以用Splash来模拟类似Chrome、PhantomJS的操作了。</p><ul><li><p>入口及返回值<br>实例：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">  splash:wait(<span class="number">0.5</span>)</span><br><span class="line">  <span class="keyword">local</span> title = splash:evaljs(<span class="string">"document.title"</span>)</span><br><span class="line">  <span class="keyword">return</span> &#123;title=title&#125;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">Splash Response: Object</span><br><span class="line">  title: <span class="string">"百度一下，你就知道"</span></span><br></pre></td></tr></table></figure><p>通过evaljs()方法传人JavaScript脚本，而document.title的执行结果就是返回网页标题，执行完毕后将其赋值给一个title变盘，随后将其返回。<br>main()方法名称必须是固定的，Splash会默认调用这个方法。该方法的返回值既可以是字典形式，也可以是字符串形式，最后都会转化为Splash HTTP Response。</p></li><li><p>异步处理<br>实例：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  <span class="keyword">local</span> example_urls = &#123;<span class="string">"www.baidu.com"</span>, <span class="string">"www.taobao.com"</span>, <span class="string">"www.zhihu.com"</span>&#125;</span><br><span class="line">  <span class="keyword">local</span> urls = args.urls <span class="keyword">or</span> example_urls</span><br><span class="line">  <span class="keyword">local</span> results = &#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> index, url <span class="keyword">in</span> <span class="built_in">ipairs</span>(urls) <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">local</span> ok, reason = splash:go(<span class="string">"http://"</span> .. url)</span><br><span class="line">    <span class="keyword">if</span> ok <span class="keyword">then</span></span><br><span class="line">      splash:wait(<span class="number">2</span>)</span><br><span class="line">      results[url] = splash:png()</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line">  <span class="keyword">return</span> results</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>运行结果是三个站点的截图。<br>在脚本内调用的 <code>wait()</code> 方法类似于Python中的 <code>sleep()</code> ，其参数为等待的秒数。当 Splash 执行到此方法时，它会转而去处理其他任务，然后在指定的时间过后再回来继续处理。</p><p>这里值得注意的是，Lua 脚本中的字符串拼接和 Python 不同，它使用的是 <code>..</code> 操作符，而不是 <code>+</code> 。如果有必要，可以简单了解一下 Lua 脚本的语法，详见：<a href="http://www.runoob.com/lua/lua-basic-syntax.html" target="_blank" rel="noopener">http://www.runoob.com/lua/lua-basic-syntax.html</a></p><p>另外，这里做了加载时的异常检测。<code>go()</code> 方法会返回加载页面的结果状态，如果页面出现 4xx 或 5xx 状态码，<code>ok</code> 变量就为空，就不会返回加载后的图片。</p></li></ul><h4 id="Splash对象属性"><a href="#Splash对象属性" class="headerlink" title="Splash对象属性"></a>Splash对象属性</h4><ul><li><p><code>args</code><br>该属性可以获取加载时配置的参数，比如 URL，如果为 GET 请求，它还可以获取 GET 请求参数；如果为 POST 请求，它可以获取表单提交的数据。Splash 也支持使用第二个参数直接作为 args，例如：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">    <span class="keyword">local</span> url = args.url</span><br><span class="line">    <span class="comment">--local url = splash.args.url   --两者等价</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>js_enabled</code><br>Splash的 JavaScript 执行开关，可以将其配置为 true 或 false 来控制是否执行 JavaScript 代码，默认为 true。示例：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">  splash.js_enabled = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">local</span> title = splash:evaljs(<span class="string">"document.title"</span>)</span><br><span class="line">  <span class="keyword">return</span> &#123;title=title&#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>此时运行结果就会抛出异常，一般来说，不用设置此属性，默认开启即可。</p></li><li><p><code>resource_timeout</code><br>设置加载的超时时间，单位为秒。如果设置为 0 或 nil（类似 Python 中的 None），代表不检测超时。<br>这里将超时时间设置为 0.1 秒。如果在 0.1 秒之内没有得到响应，就会抛出异常，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    splash.resource_timeout = <span class="number">0.1</span></span><br><span class="line">    <span class="built_in">assert</span>(splash:go(<span class="string">'https://www.taobao.com'</span>))</span><br><span class="line">    <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>images_enabled</code><br>设置图片是否加载，默认情况下是加载的。禁用该属性后，可以节省网络流量并提高网页加载速度。但是需要注意的是，禁用图片加载可能会影响 JavaScript 渲染。因为禁用图片之后，它的外层 DOM 节点的高度会受影响，进而影响 DOM 节点的位置。因此，如果 JavaScript 对图片节点有操作的话，其执行就会受到影响。</p><p>另外，Splash 使用了缓存。如果一开始加载出来了网页图片，然后禁用了图片加载，再重新加载页面，之前加载好的图片可能还会显示出来，这时直接重启 Splash 即可。</p><p>禁用图片加载的示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash.images_enabled = <span class="literal">false</span></span><br><span class="line">  <span class="built_in">assert</span>(splash:go(<span class="string">'https://www.jd.com'</span>))</span><br><span class="line">  <span class="keyword">return</span> &#123;png=splash:png()&#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>plugins_enabled</code><br>控制浏览器插件（如 Flash 插件）是否开启。默认情况下，此属性是 false，表示不开启。可以使用如下代码控制其开启和关闭：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">splash.plugins_enabled = <span class="literal">true</span>/<span class="literal">false</span></span><br></pre></td></tr></table></figure></li><li><p><code>scroll_position</code><br>控制页面上下或左右滚动。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  <span class="built_in">assert</span>(splash:go(<span class="string">'https://www.taobao.com'</span>))</span><br><span class="line">  splash.scroll_position = &#123;x=<span class="number">100</span>, y=<span class="number">400</span>&#125;</span><br><span class="line">  <span class="keyword">return</span> &#123;png=splash:png()&#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="Splash对象的方法"><a href="#Splash对象的方法" class="headerlink" title="Splash对象的方法"></a>Splash对象的方法</h4><p>官方文档：<a href="https://splash.readthedocs.io/en/stable/scripting-ref.html，" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/scripting-ref.html，</a><br>针对页面元素的 API 操作： <a href="https://splash.readthedocs.io/en/stable/scripting-element-object.html。" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/scripting-element-object.html。</a></p><ul><li><p><code>go()</code><br>用来请求某个链接，可以模拟 GET 和 POST 请求，同时支持传入请求头、表单等数据，其用法如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">ok, reason = splash:go&#123;url, baseurl=<span class="literal">nil</span>, headers=<span class="literal">nil</span>, http_method=<span class="string">"GET"</span>, body=<span class="literal">nil</span>, formdata=<span class="literal">nil</span>&#125;</span><br></pre></td></tr></table></figure><p>参数说明如下。</p><ul><li><code>url</code>：请求的 URL。</li><li><code>baseurl</code>：可选参数，默认为空，表示资源加载相对路径。</li><li><code>headers</code>：可选参数，默认为空，表示请求头。</li><li><code>http_method</code>：可选参数，默认为 GET，同时支持 POST。</li><li><code>body</code>：可选参数，默认为空，发 POST 请求时的表单数据，使用的 Content-type 为 application/json。</li><li><code>formdata</code>：可选参数，默认为空，POST 的时候的表单数据，使用的 Content-type 为 application/x-www-form-urlencoded。</li></ul><p>该方法的返回结果是结果 ok 和原因 reason 的组合，如果 ok 为空，代表网页加载出现了错误，此时 reason 变量中包含了错误的原因，否则证明页面加载成功。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  <span class="keyword">local</span> ok, reason = splash:go&#123;<span class="string">"http://httpbin.org/post"</span>, http_method=<span class="string">"POST"</span>, body=<span class="string">"name=Germey"</span>&#125;</span><br><span class="line">  <span class="keyword">if</span> ok <span class="keyword">then</span></span><br><span class="line">        <span class="keyword">return</span> splash:html()</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>wait()</code><br>控制页面的等待时间，使用方法如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">ok, reason = splash:wait&#123;<span class="built_in">time</span>, cancel_on_redirect=<span class="literal">false</span>, cancel_on_error=<span class="literal">true</span>&#125;</span><br></pre></td></tr></table></figure><p>参数说明如下。</p><ul><li>time：等待的秒数。</li><li>cancel_on_redirect：可选参数，默认为 false，表示如果发生了重定向就停止等待，并返回重定向结果。</li><li>cancel_on_error：可选参数，默认为 false，表示如果发生了加载错误，就停止等待。</li></ul><p>返回结果同样是结果 ok 和原因 reason 的组合。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    splash:go(<span class="string">"https://www.taobao.com"</span>)</span><br><span class="line">    splash:wait(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> &#123;html=splash:html()&#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>jsfunc()</code><br>可以直接调用 JavaScript 定义的方法，但是所调用的方法需要用双中括号包围，相当于实现了 JavaScript 方法到 Lua 脚本的转换。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  <span class="keyword">local</span> get_div_count = splash:jsfunc(<span class="string">[[</span></span><br><span class="line"><span class="string">  function () &#123;</span></span><br><span class="line"><span class="string">    var body = document.body;</span></span><br><span class="line"><span class="string">    var divs = body.getElementsByTagName('div');</span></span><br><span class="line"><span class="string">    return divs.length;</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">  ]]</span>)</span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">  <span class="keyword">return</span> (<span class="string">"There are %s DIVs"</span>):<span class="built_in">format</span>(</span><br><span class="line">    get_div_count())</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>关于 JavaScript 到 Lua 脚本的更多转换细节，可以参考官方文档：<a href="https://splash.readthedocs.io/en/stable/scripting-ref.html#splash-jsfunc。" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/scripting-ref.html#splash-jsfunc。</a></p></li><li><p><code>evaljs()</code><br>执行 JavaScript 代码并返回最后一条 JavaScript 语句的返回结果，使用方法如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">local</span> title = splash:evaljs(<span class="string">"document.title"</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>runjs()</code><br>执行 JavaScript 代码，与 evaljs() 的功能类似，但是更偏向于<strong>执行某些动作</strong>或<strong>声明某些方法</strong>。例如：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">  splash:runjs(<span class="string">"foo = function() &#123; return 'bar' &#125;"</span>)</span><br><span class="line">  <span class="keyword">local</span> result = splash:evaljs(<span class="string">"foo()"</span>)</span><br><span class="line">  <span class="keyword">return</span> result</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>autoload()</code><br>设置每个页面访问时自动加载的对象，使用方法如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">ok, reason = splash:autoload&#123;source_or_url, source=<span class="literal">nil</span>, url=<span class="literal">nil</span>&#125;</span><br></pre></td></tr></table></figure><p>参数说明如下。</p><ul><li>source_or_url：JavaScript 代码或者 JavaScript 库链接。</li><li>source：JavaScript 代码。</li><li>url：JavaScript 库链接</li></ul><p>但是此方法只负责加载 JavaScript 代码或库，不执行任何操作。如果要执行操作，可以调用 evaljs() 或 runjs() 方法。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:autoload(<span class="string">[[</span></span><br><span class="line"><span class="string">    function get_document_title()&#123;</span></span><br><span class="line"><span class="string">      return document.title;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  ]]</span>)</span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:evaljs(<span class="string">"get_document_title()"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>使用 autoload() 方法加载某些方法库，如 jQuery，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  <span class="built_in">assert</span>(splash:autoload(<span class="string">"https://code.jquery.com/jquery-2.1.3.min.js"</span>))</span><br><span class="line">  <span class="built_in">assert</span>(splash:go(<span class="string">"https://www.taobao.com"</span>))</span><br><span class="line">  <span class="keyword">local</span> version = splash:evaljs(<span class="string">"$.fn.jquery"</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="string">'JQuery version: '</span> .. version</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>call_later()</code><br>通过设置定时任务和延迟时间来实现任务延时执行，并且可以在执行前通过 cancel() 方法重新执行定时任务。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  <span class="keyword">local</span> snapshots = &#123;&#125;</span><br><span class="line">  <span class="keyword">local</span> timer = splash:call_later(<span class="function"><span class="keyword">function</span><span class="params">()</span></span></span><br><span class="line">    snapshots[<span class="string">"a"</span>] = splash:png()</span><br><span class="line">    splash:wait(<span class="number">1.0</span>)</span><br><span class="line">    snapshots[<span class="string">"b"</span>] = splash:png()</span><br><span class="line">  <span class="keyword">end</span>, <span class="number">0.2</span>)</span><br><span class="line">  splash:go(<span class="string">"https://www.taobao.com"</span>)</span><br><span class="line">  splash:wait(<span class="number">3.0</span>)</span><br><span class="line">  <span class="keyword">return</span> snapshots</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>这里设置了一个定时任务，0.2 秒的时候获取网页截图，然后等待 1 秒，1.2 秒时再次获取网页截图，第一次截图时网页还没有加载出来，截图为空，第二次网页便加载成功了。</p></li><li><p><code>http_get()</code><br>模拟发送 HTTP 的 GET 请求，使用方法如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">response = splash:http_get&#123;url, headers=<span class="literal">nil</span>, follow_redirects=<span class="literal">true</span>&#125;</span><br></pre></td></tr></table></figure><p>参数说明如下。</p><ul><li>url：请求 URL。</li><li>headers：可选参数，默认为空，请求头。</li><li>follow_redirects：可选参数，表示是否启动自动重定向，默认为 true。</li></ul><p>示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  <span class="keyword">local</span> treat = <span class="built_in">require</span>(<span class="string">"treat"</span>)</span><br><span class="line">  <span class="keyword">local</span> response = splash:http_get(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">    html=treat.as_string(response.body),</span><br><span class="line">    url=response.url,</span><br><span class="line">    <span class="built_in">status</span>=response.<span class="built_in">status</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>http_post()</code><br>模拟发送 POST 请求，多了一个参数 body，使用方法如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">response = splash:http_post&#123;url, headers=<span class="literal">nil</span>, follow_redirects=<span class="literal">true</span>, body=<span class="literal">nil</span>&#125;</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  <span class="keyword">local</span> treat = <span class="built_in">require</span>(<span class="string">"treat"</span>)</span><br><span class="line">  <span class="keyword">local</span> json = <span class="built_in">require</span>(<span class="string">"json"</span>)</span><br><span class="line">  <span class="keyword">local</span> response = splash:http_post&#123;<span class="string">"http://httpbin.org/post"</span>,     </span><br><span class="line">      body=json.encode(&#123;name=<span class="string">"Germey"</span>&#125;),</span><br><span class="line">      headers=&#123;[<span class="string">"content-type"</span>]=<span class="string">"application/json"</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">    html=treat.as_string(response.body),</span><br><span class="line">    url=response.url,</span><br><span class="line">    <span class="built_in">status</span>=response.<span class="built_in">status</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>set_content()</code><br>设置页面的内容，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    <span class="built_in">assert</span>(splash:set_content(<span class="string">"&lt;html&gt;&lt;body&gt;&lt;h1&gt;hello&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;"</span>))</span><br><span class="line">    <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>html()</code><br>用来获取网页的源代码，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://httpbin.org/get"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:html()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>png()</code><br>用来获取 PNG 格式的网页截图，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.taobao.com"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>jpeg()</code><br>来获取 JPEG 格式的网页截图，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.taobao.com"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:jpeg()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>har()</code><br>用来获取页面加载过程描述，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:har()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>url()</code><br>获取当前正在访问的 URL，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:url()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>get_cookies()</code><br>获取当前页面的 Cookies，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:get_cookies()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>add_cookie()</code><br>为当前页面添加 Cookie，用法如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">cookies = splash:add_cookie&#123;name, value, <span class="built_in">path</span>=<span class="literal">nil</span>, domain=<span class="literal">nil</span>, expires=<span class="literal">nil</span>, httpOnly=<span class="literal">nil</span>, secure=<span class="literal">nil</span>&#125;</span><br></pre></td></tr></table></figure><p>该方法的各个参数代表 Cookie 的各个属性。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    splash:add_cookie&#123;<span class="string">"sessionid"</span>, <span class="string">"237465ghgfsd"</span>, <span class="string">"/"</span>, domain=<span class="string">"http://example.com"</span>&#125;</span><br><span class="line">    splash:go(<span class="string">"http://example.com/"</span>)</span><br><span class="line">    <span class="keyword">return</span> splash:html()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>clear_cookies()</code><br>清除所有的 Cookies，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    splash:go(<span class="string">"https://www.baidu.com/"</span>)</span><br><span class="line">    splash:clear_cookies()</span><br><span class="line">    <span class="keyword">return</span> splash:get_cookies()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>get_viewport_size()</code><br>获取当前浏览器页面的大小，即宽高，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    splash:go(<span class="string">"https://www.baidu.com/"</span>)</span><br><span class="line">    <span class="keyword">return</span> splash:get_viewport_size()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>set_viewport_size()</code><br>设置当前浏览器页面的大小，即宽高，用法如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    splash:set_viewport_size(<span class="number">400</span>, <span class="number">700</span>)</span><br><span class="line">    <span class="built_in">assert</span>(splash:go(<span class="string">"http://cuiqingcai.com"</span>))</span><br><span class="line">    <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>set_viewport_full()</code><br>设置浏览器全屏显示，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    splash:set_viewport_full()</span><br><span class="line">    <span class="built_in">assert</span>(splash:go(<span class="string">"http://cuiqingcai.com"</span>))</span><br><span class="line">    <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>set_user_agent()</code><br>设置浏览器的 User-Agent，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">  splash:set_user_agent(<span class="string">'Splash'</span>)</span><br><span class="line">  splash:go(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:html()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>set_custom_headers()</code><br>设置请求头，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">  splash:set_custom_headers(&#123;</span><br><span class="line">    [<span class="string">"User-Agent"</span>] = <span class="string">"Splash"</span>,</span><br><span class="line">    [<span class="string">"Site"</span>] = <span class="string">"Splash"</span>,</span><br><span class="line">  &#125;)</span><br><span class="line">  splash:go(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:html()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>select()</code><br>选中符合条件的第一个节点，如果有多个节点符合条件，则只会返回一个，其参数是 CSS 选择器。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com/"</span>)</span><br><span class="line">  <span class="built_in">input</span> = splash:<span class="built_in">select</span>(<span class="string">"#kw"</span>)</span><br><span class="line">  <span class="built_in">input</span>:send_text(<span class="string">'Splash'</span>)</span><br><span class="line">  splash:wait(<span class="number">3</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>这里首先访问了百度，然后选中了搜索框，随后调用了 send_text() 方法填写了文本，然后返回网页截图。</p></li><li><p><code>select_all()</code><br>选中所有符合条件的节点，其参数是 CSS 选择器。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">  <span class="keyword">local</span> treat = <span class="built_in">require</span>(<span class="string">'treat'</span>)</span><br><span class="line">  <span class="built_in">assert</span>(splash:go(<span class="string">"http://quotes.toscrape.com/"</span>))</span><br><span class="line">  <span class="built_in">assert</span>(splash:wait(<span class="number">0.5</span>))</span><br><span class="line">  <span class="keyword">local</span> texts = splash:select_all(<span class="string">'.quote .text'</span>)</span><br><span class="line">  <span class="keyword">local</span> results = &#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> index, text <span class="keyword">in</span> <span class="built_in">ipairs</span>(texts) <span class="keyword">do</span></span><br><span class="line">    results[index] = text.node.innerHTML</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line">  <span class="keyword">return</span> treat.as_array(results)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p><code>mouse_click()</code><br>模拟鼠标点击操作，传入的参数为坐标值 x 和 y。此外，也可以直接选中某个节点，然后调用此方法，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">  splash:go(<span class="string">"https://www.baidu.com/"</span>)</span><br><span class="line">  <span class="built_in">input</span> = splash:<span class="built_in">select</span>(<span class="string">"#kw"</span>)</span><br><span class="line">  <span class="built_in">input</span>:send_text(<span class="string">'Splash'</span>)</span><br><span class="line">  submit = splash:<span class="built_in">select</span>(<span class="string">'#su'</span>)</span><br><span class="line">  submit:mouse_click()</span><br><span class="line">  splash:wait(<span class="number">3</span>)</span><br><span class="line">  <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>这里首先选中页面的输入框，输入了文本，然后选中 “提交” 按钮，调用了 mouse_click() 方法提交查询，然后页面等待三秒，返回截图。</p></li></ul><h4 id="Splash-API调用"><a href="#Splash-API调用" class="headerlink" title="Splash API调用"></a>Splash API调用</h4><p>和 Python 程序结合使用并抓取 JavaScript 渲染的页面， Splash 提供了一些 HTTP API 接口，只需要请求这些接口并传递相应的参数即可，下面简要介绍这些接口。</p><ul><li><p>render.html<br>此接口用于获取 JavaScript 渲染的页面的 HTML 代码，接口地址就是 Splash 的运行地址加此接口名称，例如 <a href="http://localhost:8050/render.html。Python" target="_blank" rel="noopener">http://localhost:8050/render.html。Python</a> 实现代码如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'http://localhost:8050/render.html?url=https://www.baidu.com'</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p>此接口还可以指定其他参数，比如通过 wait 指定等待秒数。如果要确保页面完全加载出来，可以增加等待时间，例如：<br><code>url = &#39;http://localhost:8050/render.html?url=https://www.taobao.com&amp;wait=5&#39;</code></p><p>另外，此接口还支持代理设置、图片加载设置、Headers 设置、请求方法设置，具体的用法可以参见官方文档 <a href="https://splash.readthedocs.io/en/stable/api.html#render-html。" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/api.html#render-html。</a></p></li><li><p>render.png<br>此接口可以获取网页截图，其参数比 render.html 多了几个，比如通过 width 和 height 来控制宽高，返回的是 PNG 格式的图片二进制数据。示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">curl http://localhost:<span class="number">8050</span>/render.png?url=https://www.taobao.com&amp;wait=<span class="number">5</span>&amp;width=<span class="number">1000</span>&amp;height=<span class="number">700</span></span><br></pre></td></tr></table></figure><p>Python 实现，可以将返回的二进制数据保存为 PNG 格式的图片，具体如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://localhost:8050/render.png?url=https://www.jd.com&amp;wait=5&amp;width=1000&amp;height=700'</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'taobao.png'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br></pre></td></tr></table></figure><p>详细的参数设置可以参考官网文档 <a href="https://splash.readthedocs.io/en/stable/api.html#render-png。" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/api.html#render-png。</a></p></li><li><p>render.jpeg<br>此接口和 render.png 类似，不过它返回的是 JPEG 格式的图片二进制数据。</p><p>另外，此接口比 render.png 多了参数 quality，它用来设置图片质量。</p></li><li><p>render.har<br>此接口用于获取页面加载的 HAR 数据，示例如下：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">curl http://localhost:<span class="number">8050</span>/render.har?url=https://www.jd.com&amp;wait=<span class="number">5</span></span><br></pre></td></tr></table></figure><p>它的返回结果非常多，是一个 JSON 格式的数据，其中包含页面加载过程中的 HAR 数据。</p></li><li><p>render.json<br>此接口包含了前面接口的所有功能，返回结果是 JSON 格式，通过传入不同参数控制其返回结果。比如，传入 html=1，返回结果即会增加源代码数据；传入 png=1，返回结果即会增加页面 PNG 截图数据；传入 har=1，则会获得页面 HAR 数据。例如：</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">curl http://localhost:<span class="number">8050</span>/render.json?url=https://httpbin.org&amp;html=<span class="number">1</span>&amp;har=<span class="number">1</span></span><br></pre></td></tr></table></figure><p>更多参数设置，具体可以参考官方文档：<a href="https://splash.readthedocs.io/en/stable/api.html#render-json。" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/api.html#render-json。</a></p></li><li><p>execute<br>此接口是最为强大的接口。前面说了很多 Splash Lua 脚本的操作，用此接口便可实现与 Lua 脚本的对接。</p><p>前面的 render.html 和 render.png 等接口对于一般的 JavaScript 渲染页面是足够了，但是如果要实现一些交互操作的话，它们还是无能为力，这里就需要使用 execute 接口了。</p><p>通过 lua_source 参数传递了转码后的 Lua 脚本，通过 execute 接口获取了最终脚本的执行结果。Python 实现代码如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">lua = <span class="string">'''</span></span><br><span class="line"><span class="string">function main(splash, args)</span></span><br><span class="line"><span class="string">  local treat = require("treat")</span></span><br><span class="line"><span class="string">  local response = splash:http_get("http://httpbin.org/get")</span></span><br><span class="line"><span class="string">    return &#123;</span></span><br><span class="line"><span class="string">    html=treat.as_string(response.body),</span></span><br><span class="line"><span class="string">    url=response.url,</span></span><br><span class="line"><span class="string">    status=response.status</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://localhost:8050/execute?lua_source='</span> + quote(lua)</span><br><span class="line">response = requests.get(url)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure></li></ul><h4 id="Splash负载均衡配置"><a href="#Splash负载均衡配置" class="headerlink" title="Splash负载均衡配置"></a>Splash负载均衡配置</h4><p>用 Splash 做页面抓取时，如果爬取的量非常大，任务非常多，用一个 Splash 服务来处理的话，压力太大了，此时可以考虑搭建一个负载均衡器来把压力分散到各个服务器上。这相当于多台机器多个服务共同参与任务的处理，可以减小单个 Splash 服务的压力。</p><ul><li><p>配置 Splash 服务</p></li><li><p>配置负载均衡<br>选用任意一台带有公网 IP 的主机来配置负载均衡。首先，在这台主机上装好 Nginx，然后修改 Nginx 的配置文件 nginx.conf，添加如下内容：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    upstream splash &#123;</span><br><span class="line">        least_conn;</span><br><span class="line">        server <span class="number">41.159</span><span class="number">.27</span><span class="number">.223</span>:<span class="number">8050</span>;</span><br><span class="line">        server <span class="number">41.159</span><span class="number">.27</span><span class="number">.221</span>:<span class="number">8050</span>;</span><br><span class="line">        server <span class="number">41.159</span><span class="number">.27</span><span class="number">.9</span>:<span class="number">8050</span>;</span><br><span class="line">        server <span class="number">41.159</span><span class="number">.117</span><span class="number">.119</span>:<span class="number">8050</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen <span class="number">8050</span>;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http://splash;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过 upstream 字段定义了一个名字叫作 splash 的服务集群配置。其中 least_conn 代表最少链接负载均衡，它适合处理请求处理时间长短不一造成服务器过载的情况。不指定配置，默认以轮询策略实现负载均衡，每个服务器的压力相同。此策略适合服务器配置相当、无状态且短平快的服务使用。<br>另外，还可以指定权重，配置如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">upstream splash &#123;</span><br><span class="line">    server <span class="number">41.159</span><span class="number">.27</span><span class="number">.223</span>:<span class="number">8050</span> weight=<span class="number">4</span>;</span><br><span class="line">    server <span class="number">41.159</span><span class="number">.27</span><span class="number">.221</span>:<span class="number">8050</span> weight=<span class="number">2</span>;</span><br><span class="line">    server <span class="number">41.159</span><span class="number">.27</span><span class="number">.9</span>:<span class="number">8050</span> weight=<span class="number">2</span>;</span><br><span class="line">    server <span class="number">41.159</span><span class="number">.117</span><span class="number">.119</span>:<span class="number">8050</span> weight=<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后，还有一种 IP 散列负载均衡，配置如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">upstream splash &#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server <span class="number">41.159</span><span class="number">.27</span><span class="number">.223</span>:<span class="number">8050</span>;</span><br><span class="line">    server <span class="number">41.159</span><span class="number">.27</span><span class="number">.221</span>:<span class="number">8050</span>;</span><br><span class="line">    server <span class="number">41.159</span><span class="number">.27</span><span class="number">.9</span>:<span class="number">8050</span>;</span><br><span class="line">    server <span class="number">41.159</span><span class="number">.117</span><span class="number">.119</span>:<span class="number">8050</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>服务器根据请求客户端的 IP 地址进行散列计算，确保使用同一个服务器响应请求，这种策略适合有状态的服务，比如用户登录后访问某个页面的情形。对于 Splash 来说，不需要应用此设置。</p><p>可以根据不同的情形选用不同的配置，配置完成后重启一下 Nginx 服务：<br><code>sudo nginx -s reload</code></p></li><li><p>配置认证<br>现在 Splash 是可以公开访问的，如果不想让其公开访问，还可以配置认证，这仍然借助于 Nginx。可以在 server 的 location 字段中添加 auth_basic 和 auth_basic_user_file 字段，具体配置如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    upstream splash &#123;</span><br><span class="line">        least_conn;</span><br><span class="line">        server <span class="number">41.159</span><span class="number">.27</span><span class="number">.223</span>:<span class="number">8050</span>;</span><br><span class="line">        server <span class="number">41.159</span><span class="number">.27</span><span class="number">.221</span>:<span class="number">8050</span>;</span><br><span class="line">        server <span class="number">41.159</span><span class="number">.27</span><span class="number">.9</span>:<span class="number">8050</span>;</span><br><span class="line">        server <span class="number">41.159</span><span class="number">.117</span><span class="number">.119</span>:<span class="number">8050</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen <span class="number">8050</span>;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http://splash;</span><br><span class="line">            auth_basic <span class="string">"Restricted"</span>;</span><br><span class="line">            auth_basic_user_file /etc/nginx/conf.d/.htpasswd;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里使用的用户名和密码配置放置在 /etc/nginx/conf.d 目录下，我们需要使用 htpasswd 命令创建。例如，创建一个用户名为 admin 的文件，相关命令如下：<br><code>htpasswd -c .htpasswd admin</code><br>配置完成后，重启一下 Nginx 服务。</p></li><li><p>测试<br>利用 <a href="http://httpbin.org/get" target="_blank" rel="noopener">http://httpbin.org/get</a> 测试即可，实现代码如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">lua = <span class="string">'''</span></span><br><span class="line"><span class="string">function main(splash, args)</span></span><br><span class="line"><span class="string">  local treat = require("treat")</span></span><br><span class="line"><span class="string">  local response = splash:http_get("http://httpbin.org/get")</span></span><br><span class="line"><span class="string">  return treat.as_string(response.body)</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://splash:8050/execute?lua_source='</span> + quote(lua)</span><br><span class="line">response = requests.get(url, auth=(<span class="string">'admin'</span>, <span class="string">'admin'</span>))</span><br><span class="line">ip = re.search(<span class="string">'(\d+\.\d+\.\d+\.\d+)'</span>, response.text).group(<span class="number">1</span>)</span><br><span class="line">print(ip)</span><br></pre></td></tr></table></figure></li></ul><h3 id="使用Selenium爬取淘宝商品"><a href="#使用Selenium爬取淘宝商品" class="headerlink" title="使用Selenium爬取淘宝商品"></a>使用Selenium爬取淘宝商品</h3><h2 id="Ch-7-验证码的识别"><a href="#Ch-7-验证码的识别" class="headerlink" title="Ch 7 验证码的识别"></a>Ch 7 验证码的识别</h2><h3 id="图形验证码识别"><a href="#图形验证码识别" class="headerlink" title="图形验证码识别"></a>图形验证码识别</h3><h3 id="滑动验证码识别"><a href="#滑动验证码识别" class="headerlink" title="滑动验证码识别"></a>滑动验证码识别</h3><h3 id="点触验证码识别"><a href="#点触验证码识别" class="headerlink" title="点触验证码识别"></a>点触验证码识别</h3><h3 id="宫格验证码识别"><a href="#宫格验证码识别" class="headerlink" title="宫格验证码识别"></a>宫格验证码识别</h3><h2 id="Ch-8-代理的使用"><a href="#Ch-8-代理的使用" class="headerlink" title="Ch 8 代理的使用"></a>Ch 8 代理的使用</h2><h3 id="代理的设置"><a href="#代理的设置" class="headerlink" title="代理的设置"></a>代理的设置</h3><h4 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy = <span class="string">'127.0.0.1:9743'</span></span><br><span class="line"><span class="comment">#需认证的代理</span></span><br><span class="line"><span class="comment"># proxy = 'username:password@127.0.0.1:9743'</span></span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy</span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure><p>SOCKS5代理：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># pip3 install PySocks</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> socks</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">socks.set_default_proxy(socks.SOCKS5, <span class="string">'127.0.0.1'</span>, <span class="number">9742</span>)</span><br><span class="line">socket.socket = socks.socksocket</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure><h4 id="requests"><a href="#requests" class="headerlink" title="requests"></a>requests</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxy = <span class="string">'127.0.0.1:9743'</span></span><br><span class="line"><span class="comment">#需认证的代理</span></span><br><span class="line"><span class="comment"># proxy = 'username:password@127.0.0.1:9743'</span></span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">'http://httpbin.org/get'</span>, proxies=proxies)</span><br><span class="line">    print(response.text)</span><br><span class="line"><span class="keyword">except</span> requests.exceptions.ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'Error'</span>, e.args)</span><br></pre></td></tr></table></figure><p>SOCKS5代理：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pip install 'requests[socks]'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxy = <span class="string">'127.0.0.1:9742'</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://'</span> + proxy,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://'</span> + proxy</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">'http://httpbin.org/get'</span>, proxies=proxies)</span><br><span class="line">    print(response.text)</span><br><span class="line"><span class="keyword">except</span> requests.exceptions.ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'Error'</span>, e.args)</span><br></pre></td></tr></table></figure><p>或：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> socks</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line">socks.set_default_proxy(socks.SOCKS5, <span class="string">'127.0.0.1'</span>, <span class="number">9742</span>)</span><br><span class="line">socket.socket = socks.socksocket</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">    print(response.text)</span><br><span class="line"><span class="keyword">except</span> requests.exceptions.ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'Error'</span>, e.args)</span><br></pre></td></tr></table></figure><h4 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h4><ul><li>Chrome<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">proxy = <span class="string">'127.0.0.1:9743'</span></span><br><span class="line"></span><br><span class="line">chrome_options = webdriver.ChromeOptions()</span><br><span class="line">chrome_options.add_argument(<span class="string">'--proxy-server=http://'</span> + proxy)</span><br><span class="line">chrome = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line">chrome.get(<span class="string">'http://httpbin.org/get'</span>)</span><br></pre></td></tr></table></figure>认证代理：<br>需要在本地创建一个manifest.json配置文件和background脚本来设置认证代理。运行代码之后本地会生成一个proxy_auth__plugin.zip文件来保存当前配置。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line">ip = <span class="string">'127.0.0.1'</span></span><br><span class="line">port = <span class="number">9743</span></span><br><span class="line">username = <span class="string">'foo'</span></span><br><span class="line">password = <span class="string">'bar'</span></span><br><span class="line"></span><br><span class="line">manifest_json = <span class="string">"""</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    "version": "1.0.0",</span></span><br><span class="line"><span class="string">    "manifest_version": 2,</span></span><br><span class="line"><span class="string">    "name": "Chrome Proxy",</span></span><br><span class="line"><span class="string">    "permissions": [</span></span><br><span class="line"><span class="string">        "proxy",</span></span><br><span class="line"><span class="string">        "tabs",</span></span><br><span class="line"><span class="string">        "unlimitedStorage",</span></span><br><span class="line"><span class="string">        "storage",</span></span><br><span class="line"><span class="string">        "&lt;all_urls&gt;",</span></span><br><span class="line"><span class="string">        "webRequest",</span></span><br><span class="line"><span class="string">        "webRequestBlocking"</span></span><br><span class="line"><span class="string">    ],</span></span><br><span class="line"><span class="string">    "background": &#123;</span></span><br><span class="line"><span class="string">        "scripts": ["background.js"]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">background_js = <span class="string">"""</span></span><br><span class="line"><span class="string">var config = &#123;</span></span><br><span class="line"><span class="string">        mode: "fixed_servers",</span></span><br><span class="line"><span class="string">        rules: &#123;</span></span><br><span class="line"><span class="string">          singleProxy: &#123;</span></span><br><span class="line"><span class="string">            scheme: "http",</span></span><br><span class="line"><span class="string">            host: "%(ip)s",</span></span><br><span class="line"><span class="string">            port: %(port)s</span></span><br><span class="line"><span class="string">          &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">chrome.proxy.settings.set(&#123;value: config, scope: "regular"&#125;, function() &#123;&#125;);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">function callbackFn(details) &#123;</span></span><br><span class="line"><span class="string">    return &#123;</span></span><br><span class="line"><span class="string">        authCredentials: &#123;</span></span><br><span class="line"><span class="string">            username: "%(username)s",</span></span><br><span class="line"><span class="string">            password: "%(password)s"</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">chrome.webRequest.onAuthRequired.addListener(</span></span><br><span class="line"><span class="string">            callbackFn,</span></span><br><span class="line"><span class="string">            &#123;urls: ["&lt;all_urls&gt;"]&#125;,</span></span><br><span class="line"><span class="string">            ['blocking']</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span> % &#123;<span class="string">'ip'</span>: ip, <span class="string">'port'</span>: port, <span class="string">'username'</span>: username, <span class="string">'password'</span>: password&#125;</span><br><span class="line"></span><br><span class="line">plugin_file = <span class="string">'proxy_auth_plugin.zip'</span></span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(plugin_file, <span class="string">'w'</span>) <span class="keyword">as</span> zp:</span><br><span class="line">    zp.writestr(<span class="string">"manifest.json"</span>, manifest_json)</span><br><span class="line">    zp.writestr(<span class="string">"background.js"</span>, background_js)</span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">"--start-maximized"</span>)</span><br><span class="line">chrome_options.add_extension(plugin_file)</span><br><span class="line">browser = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line">browser.get(<span class="string">'http://httpbin.org/get'</span>)</span><br></pre></td></tr></table></figure></li><li>PhatomJS<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">service_args = [</span><br><span class="line">    <span class="string">'--proxy=127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'--proxy-type=http'</span></span><br><span class="line">]</span><br><span class="line"><span class="comment"># 认证代理</span></span><br><span class="line"><span class="comment"># service_args = [</span></span><br><span class="line"><span class="comment">#     '--proxy=127.0.0.1:9743',</span></span><br><span class="line"><span class="comment">#     '--proxy-type=http',</span></span><br><span class="line"><span class="comment">#     '--proxy-auth=username:password'</span></span><br><span class="line"><span class="comment"># ]</span></span><br><span class="line"></span><br><span class="line">browser = webdriver.PhantomJS(service_args=service_args)</span><br><span class="line">browser.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">print(browser.page_source)</span><br></pre></td></tr></table></figure></li></ul><h3 id="代理池的维护"><a href="#代理池的维护" class="headerlink" title="代理池的维护"></a>代理池的维护</h3><h4 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h4><p><img src="https://s3.ax1x.com/2021/01/23/s7gFxA.png" alt="代理池架构"></p><p>代理池分为4个模块：存储模块、获取模块、检测模块和接口模块。</p><ul><li>存储模块<br>存储模块使用Redis的有序集合，用来做代理的去重和状态标识，同时它也是中心模块和基础模块，将其他模块串联起来。</li><li>获取模块<br>获取模块定时从代理网站获取代理，将获取的代理传递给存储模块，并保存到数据库。</li><li>检测模块<br>检测模块定时通过存储模块获取所有代理，并对代理进行检测，根据不同的检测结果对代理设置不同的标识。</li><li>接口模块<br>接口模块通过WebAPI提供服务接口，接口通过连接数据库并通过Web形式返回可用的代理。</li></ul><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>参考项目地址：<a href="https://github.com/Python3WebSpider/ProxyPool" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ProxyPool</a></p><ul><li><p>存储模块<br>对于代理池来说，有序集合中每个元素的分数可以作为判断一个代理是否可用的标志，100为最高分，代表最可用，0为最低分，代表最不可用。如果要获取可用代理，可以从代理池中随机获取分数最高的代理。<br>设置分数规则如下：</p><ul><li>分数100为可用，检测器会定时循环检测每个代理可用情况，一旦检测到有可用的代理就立即置为100，检测到不可用就将分数减1，分数减至0后代理移除。</li><li>新获取的代理的分数为10，如果测试可行，分数立即置为100，不可行则分数减1，分数减至0后代理移除。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">from</span> proxypool.exceptions <span class="keyword">import</span> PoolEmptyException</span><br><span class="line"><span class="keyword">from</span> proxypool.schemas.proxy <span class="keyword">import</span> Proxy</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> REDIS_HOST, REDIS_PORT, REDIS_PASSWORD, REDIS_DB, REDIS_KEY</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> PROXY_SCORE_MAX, PROXY_SCORE_MIN, PROXY_SCORE_INIT</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> choice</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">from</span> proxypool.utils.proxy <span class="keyword">import</span> is_valid_proxy, convert_proxy_or_proxies</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">REDIS_CLIENT_VERSION = redis.__version__</span><br><span class="line">IS_REDIS_VERSION_2 = REDIS_CLIENT_VERSION.startswith(<span class="string">'2.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisClient</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    redis connection client of proxypool</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, db=REDIS_DB, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        init redis client</span></span><br><span class="line"><span class="string">        :param host: redis host</span></span><br><span class="line"><span class="string">        :param port: redis port</span></span><br><span class="line"><span class="string">        :param password: redis password</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.db = redis.StrictRedis(host=host, port=port, password=password, db=db, decode_responses=<span class="literal">True</span>, **kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 函数参数中的冒号是参数的类型建议符，告诉程序员希望传入的实参的类型。</span></span><br><span class="line">    <span class="comment"># 函数后面跟着的箭头是函数返回值的类型建议符，用来说明该函数返回的值是什么类型。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, proxy: Proxy, score=PROXY_SCORE_INIT)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        add proxy and set it to init score</span></span><br><span class="line"><span class="string">        :param proxy: proxy, ip:port, like 8.8.8.8:88</span></span><br><span class="line"><span class="string">        :param score: int score</span></span><br><span class="line"><span class="string">        :return: result</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_valid_proxy(<span class="string">f'<span class="subst">&#123;proxy.host&#125;</span>:<span class="subst">&#123;proxy.port&#125;</span>'</span>):</span><br><span class="line">            logger.info(<span class="string">f'invalid proxy <span class="subst">&#123;proxy&#125;</span>, throw it'</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.exists(proxy):</span><br><span class="line">            <span class="keyword">if</span> IS_REDIS_VERSION_2:</span><br><span class="line">                <span class="keyword">return</span> self.db.zadd(REDIS_KEY, score, proxy.string())</span><br><span class="line">            <span class="keyword">return</span> self.db.zadd(REDIS_KEY, &#123;proxy.string(): score&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">random</span><span class="params">(self)</span> -&gt; Proxy:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        get random proxy</span></span><br><span class="line"><span class="string">        firstly try to get proxy with max score</span></span><br><span class="line"><span class="string">        if not exists, try to get proxy by rank</span></span><br><span class="line"><span class="string">        if not exists, raise error</span></span><br><span class="line"><span class="string">        :return: proxy, like 8.8.8.8:8</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># try to get proxy with max score</span></span><br><span class="line">        proxies = self.db.zrangebyscore(REDIS_KEY, PROXY_SCORE_MAX, PROXY_SCORE_MAX)</span><br><span class="line">        <span class="keyword">if</span> len(proxies):</span><br><span class="line">            <span class="keyword">return</span> convert_proxy_or_proxies(choice(proxies))</span><br><span class="line">        <span class="comment"># else get proxy by rank</span></span><br><span class="line">        proxies = self.db.zrevrange(REDIS_KEY, PROXY_SCORE_MIN, PROXY_SCORE_MAX)</span><br><span class="line">        <span class="keyword">if</span> len(proxies):</span><br><span class="line">            <span class="keyword">return</span> convert_proxy_or_proxies(choice(proxies))</span><br><span class="line">        <span class="comment"># else raise error</span></span><br><span class="line">        <span class="keyword">raise</span> PoolEmptyException</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decrease</span><span class="params">(self, proxy: Proxy)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        decrease score of proxy, if small than PROXY_SCORE_MIN, delete it</span></span><br><span class="line"><span class="string">        :param proxy: proxy</span></span><br><span class="line"><span class="string">        :return: new score</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> IS_REDIS_VERSION_2:</span><br><span class="line">            self.db.zincrby(REDIS_KEY, proxy.string(), <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.db.zincrby(REDIS_KEY, <span class="number">-1</span>, proxy.string())</span><br><span class="line">        score = self.db.zscore(REDIS_KEY, proxy.string())</span><br><span class="line">        <span class="comment">#相当于"&#123;&#125;".format()</span></span><br><span class="line">        logger.info(<span class="string">f'<span class="subst">&#123;proxy.string()&#125;</span> score decrease 1, current <span class="subst">&#123;score&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> score &lt;= PROXY_SCORE_MIN:</span><br><span class="line">            logger.info(<span class="string">f'<span class="subst">&#123;proxy.string()&#125;</span> current score <span class="subst">&#123;score&#125;</span>, remove'</span>)</span><br><span class="line">            self.db.zrem(REDIS_KEY, proxy.string())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exists</span><span class="params">(self, proxy: Proxy)</span> -&gt; bool:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        if proxy exists</span></span><br><span class="line"><span class="string">        :param proxy: proxy</span></span><br><span class="line"><span class="string">        :return: if exists, bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> self.db.zscore(REDIS_KEY, proxy.string()) <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max</span><span class="params">(self, proxy: Proxy)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        set proxy to max score</span></span><br><span class="line"><span class="string">        :param proxy: proxy</span></span><br><span class="line"><span class="string">        :return: new score</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        logger.info(<span class="string">f'<span class="subst">&#123;proxy.string()&#125;</span> is valid, set to <span class="subst">&#123;PROXY_SCORE_MAX&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> IS_REDIS_VERSION_2:</span><br><span class="line">            <span class="keyword">return</span> self.db.zadd(REDIS_KEY, PROXY_SCORE_MAX, proxy.string())</span><br><span class="line">        <span class="keyword">return</span> self.db.zadd(REDIS_KEY, &#123;proxy.string(): PROXY_SCORE_MAX&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(self)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        get count of proxies</span></span><br><span class="line"><span class="string">        :return: count, int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.db.zcard(REDIS_KEY)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">all</span><span class="params">(self)</span> -&gt; List[Proxy]:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        get all proxies</span></span><br><span class="line"><span class="string">        :return: list of proxies</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> convert_proxy_or_proxies(self.db.zrangebyscore(REDIS_KEY, PROXY_SCORE_MIN, PROXY_SCORE_MAX))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">batch</span><span class="params">(self, cursor, count)</span> -&gt; List[Proxy]:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        get batch of proxies</span></span><br><span class="line"><span class="string">        :param cursor: scan cursor</span></span><br><span class="line"><span class="string">        :param count: scan count</span></span><br><span class="line"><span class="string">        :return: list of proxies</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cursor, proxies = self.db.zscan(REDIS_KEY, cursor, count=count)</span><br><span class="line">        <span class="keyword">return</span> cursor, convert_proxy_or_proxies([i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> proxies])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    conn = RedisClient()</span><br><span class="line">    result = conn.random()</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>获取模块<br>获取模块定时从代理网站获取代理，将获取的代理传递给存储模块，并保存到数据库。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># base.py</span></span><br><span class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> GET_TIMEOUT</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseCrawler</span><span class="params">(object)</span>:</span></span><br><span class="line">    urls = []</span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    retry装饰器：</span></span><br><span class="line"><span class="string">    stop_max_attempt_number：在停止之前尝试的最大次数，最后一次如果还是有异常则会抛出异常，停止运行，默认为5次</span></span><br><span class="line"><span class="string">    wait_random_min：在两次调用方法停留时长，停留最短时间，默认为0,单位毫秒</span></span><br><span class="line"><span class="string">    wait_random_max：在两次调用方法停留时长，停留最长时间，默认为1000毫秒</span></span><br><span class="line"><span class="string">    wait_fixed：设置在两次retrying之间的停留时间</span></span><br><span class="line"><span class="string">    stop_max_delay:从被装饰的函数开始执行的时间点开始到函数成功运行结束或失败报错中止的时间点。单位：毫秒</span></span><br><span class="line"><span class="string">    retry_on_result：指定一个函数，如果指定的函数返回True，则重试，否则抛出异常退出</span></span><br><span class="line"><span class="string">    retry_on_exception: 指定一个函数，如果此函数返回指定异常，则会重试，如果不是指定的异常则会退出</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"><span class="meta">    @retry(stop_max_attempt_number=3, retry_on_result=lambda x: x is None, wait_fixed=2000)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fetch</span><span class="params">(self, url, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取html源码</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            kwargs.setdefault(<span class="string">'timeout'</span>, GET_TIMEOUT)</span><br><span class="line">            kwargs.setdefault(<span class="string">'verify'</span>, <span class="literal">False</span>)</span><br><span class="line">            response = requests.get(url, **kwargs)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                response.encoding = <span class="string">'utf-8'</span></span><br><span class="line">                <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @logger.catch</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        crawl main method</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.urls:</span><br><span class="line">            logger.info(<span class="string">f'fetching <span class="subst">&#123;url&#125;</span>'</span>)</span><br><span class="line">            html = self.fetch(url)</span><br><span class="line">            <span class="keyword">for</span> proxy <span class="keyword">in</span> self.parse(html):</span><br><span class="line">                logger.info(<span class="string">f'fetched proxy <span class="subst">&#123;proxy.string()&#125;</span> from <span class="subst">&#123;url&#125;</span>'</span>)</span><br><span class="line">                <span class="keyword">yield</span> proxy</span><br><span class="line"></span><br><span class="line"><span class="comment"># daili66.py</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery</span><br><span class="line"><span class="keyword">from</span> proxypool.schemas.proxy <span class="keyword">import</span> Proxy</span><br><span class="line"><span class="keyword">from</span> proxypool.crawlers.base <span class="keyword">import</span> BaseCrawler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">'http://www.66ip.cn/&#123;page&#125;.html'</span></span><br><span class="line">MAX_PAGE = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 继承BaseCrawler</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Daili66Crawler</span><span class="params">(BaseCrawler)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    daili66 crawler, http://www.66ip.cn/1.html</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    urls = [BASE_URL.format(page=page) <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, MAX_PAGE + <span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, html)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        parse html file to get proxies</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        doc = PyQuery(html)</span><br><span class="line">        trs = doc(<span class="string">'.containerbox table tr:gt(0)'</span>).items()</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">            host = tr.find(<span class="string">'td:nth-child(1)'</span>).text()</span><br><span class="line">            port = int(tr.find(<span class="string">'td:nth-child(2)'</span>).text())</span><br><span class="line">            <span class="keyword">yield</span> Proxy(host=host, port=port)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    crawler = Daili66Crawler()</span><br><span class="line">    <span class="keyword">for</span> proxy <span class="keyword">in</span> crawler.crawl():</span><br><span class="line">        print(proxy)</span><br></pre></td></tr></table></figure></li><li><p>检测模块<br>检测模块定时通过存储模块获取所有代理，并对代理进行检测，根据不同的检测结果对代理设置不同的标识。<br>这里使用异步请求库aiohttp来进行检测。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">from</span> proxypool.schemas <span class="keyword">import</span> Proxy</span><br><span class="line"><span class="keyword">from</span> proxypool.storages.redis <span class="keyword">import</span> RedisClient</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> TEST_TIMEOUT, TEST_BATCH, TEST_URL, TEST_VALID_STATUS, TEST_ANONYMOUS</span><br><span class="line"><span class="keyword">from</span> aiohttp <span class="keyword">import</span> ClientProxyConnectionError, ServerDisconnectedError, ClientOSError, ClientHttpProxyError</span><br><span class="line"><span class="keyword">from</span> asyncio <span class="keyword">import</span> TimeoutError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EXCEPTIONS = (</span><br><span class="line">    ClientProxyConnectionError,</span><br><span class="line">    ConnectionRefusedError,</span><br><span class="line">    TimeoutError,</span><br><span class="line">    ServerDisconnectedError,</span><br><span class="line">    ClientOSError,</span><br><span class="line">    ClientHttpProxyError,</span><br><span class="line">    AssertionError</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tester</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    tester for testing proxies in queue</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        init redis</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.redis = RedisClient()</span><br><span class="line">        self.loop = asyncio.get_event_loop()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, proxy: Proxy)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        test single proxy</span></span><br><span class="line"><span class="string">        :param proxy: Proxy object</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=<span class="literal">False</span>)) <span class="keyword">as</span> session:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                logger.debug(<span class="string">f'testing <span class="subst">&#123;proxy.string()&#125;</span>'</span>)</span><br><span class="line">                <span class="comment"># if TEST_ANONYMOUS（匿名） is True, make sure that</span></span><br><span class="line">                <span class="comment"># the proxy has the effect of hiding the real IP</span></span><br><span class="line">                <span class="keyword">if</span> TEST_ANONYMOUS:</span><br><span class="line">                    url = <span class="string">'https://httpbin.org/ip'</span></span><br><span class="line">                    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url, timeout=TEST_TIMEOUT) <span class="keyword">as</span> response:</span><br><span class="line">                        resp_json = <span class="keyword">await</span> response.json()</span><br><span class="line">                        origin_ip = resp_json[<span class="string">'origin'</span>]</span><br><span class="line">                    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url, proxy=<span class="string">f'http://<span class="subst">&#123;proxy.string()&#125;</span>'</span>, timeout=TEST_TIMEOUT) <span class="keyword">as</span> response:</span><br><span class="line">                        resp_json = <span class="keyword">await</span> response.json()</span><br><span class="line">                        anonymous_ip = resp_json[<span class="string">'origin'</span>]</span><br><span class="line">                    <span class="keyword">assert</span> origin_ip != anonymous_ip</span><br><span class="line">                    <span class="keyword">assert</span> proxy.host == anonymous_ip</span><br><span class="line">                <span class="keyword">async</span> <span class="keyword">with</span> session.get(TEST_URL, proxy=<span class="string">f'http://<span class="subst">&#123;proxy.string()&#125;</span>'</span>, timeout=TEST_TIMEOUT,</span><br><span class="line">                                       allow_redirects=<span class="literal">False</span>) <span class="keyword">as</span> response:</span><br><span class="line">                    <span class="keyword">if</span> response.status <span class="keyword">in</span> TEST_VALID_STATUS:</span><br><span class="line">                        self.redis.max(proxy)</span><br><span class="line">                        logger.debug(<span class="string">f'proxy <span class="subst">&#123;proxy.string()&#125;</span> is valid, set max score'</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.redis.decrease(proxy)</span><br><span class="line">                        logger.debug(<span class="string">f'proxy <span class="subst">&#123;proxy.string()&#125;</span> is invalid, decrease score'</span>)</span><br><span class="line">            <span class="keyword">except</span> EXCEPTIONS:</span><br><span class="line">                self.redis.decrease(proxy)</span><br><span class="line">                logger.debug(<span class="string">f'proxy <span class="subst">&#123;proxy.string()&#125;</span> is invalid, decrease score'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @logger.catch</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        test main method</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># event loop of aiohttp</span></span><br><span class="line">        logger.info(<span class="string">'stating tester...'</span>)</span><br><span class="line">        count = self.redis.count()</span><br><span class="line">        logger.debug(<span class="string">f'<span class="subst">&#123;count&#125;</span> proxies to test'</span>)</span><br><span class="line">        cursor = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            logger.debug(<span class="string">f'testing proxies use cursor <span class="subst">&#123;cursor&#125;</span>, count <span class="subst">&#123;TEST_BATCH&#125;</span>'</span>)</span><br><span class="line">            cursor, proxies = self.redis.batch(cursor, count=TEST_BATCH)</span><br><span class="line">            <span class="keyword">if</span> proxies:</span><br><span class="line">                tasks = [self.test(proxy) <span class="keyword">for</span> proxy <span class="keyword">in</span> proxies]</span><br><span class="line">                self.loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cursor:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tester = Tester()</span><br><span class="line">    tester.run()</span><br></pre></td></tr></table></figure></li><li><p>接口模块<br>接口模块通过WebAPI提供服务接口，接口通过连接数据库并通过Web形式返回可用的代理。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, g</span><br><span class="line"><span class="keyword">from</span> proxypool.storages.redis <span class="keyword">import</span> RedisClient</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> API_HOST, API_PORT, API_THREADED</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__all__ = [<span class="string">'app'</span>]</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get redis client object</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(g, <span class="string">'redis'</span>):</span><br><span class="line">        g.redis = RedisClient()</span><br><span class="line">    <span class="keyword">return</span> g.redis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get home page, you can define your own templates</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'&lt;h2&gt;Welcome to Proxy Pool System&lt;/h2&gt;'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/random')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get a random proxy</span></span><br><span class="line"><span class="string">    :return: get a random proxy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    conn = get_conn()</span><br><span class="line">    <span class="keyword">return</span> conn.random().string()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/count')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_count</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get the count of proxies</span></span><br><span class="line"><span class="string">    :return: count, int</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    conn = get_conn()</span><br><span class="line">    <span class="keyword">return</span> str(conn.count())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(host=API_HOST, port=API_PORT, threaded=API_THREADED)</span><br></pre></td></tr></table></figure></li></ul><h4 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pipenv shell</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全部启动</span></span><br><span class="line">python run.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按需启动</span></span><br><span class="line">python run.py --processor getter</span><br><span class="line">python run.py --processor tester</span><br><span class="line">python run.py --processor server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 成功运行之后可以通过 http://localhost:5555/random 获取一个随机可用代理</span></span><br></pre></td></tr></table></figure><p>程序对接实现:</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxypool_url = <span class="string">'http://127.0.0.1:5555/random'</span></span><br><span class="line">target_url = <span class="string">'http://httpbin.org/get'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    get random proxy from proxypool</span></span><br><span class="line"><span class="string">    :return: proxy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> requests.get(proxypool_url).text.strip()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(url, proxy)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    use proxy to crawl page</span></span><br><span class="line"><span class="string">    :param url: page url</span></span><br><span class="line"><span class="string">    :param proxy: proxy, such as 8.8.8.8:8888</span></span><br><span class="line"><span class="string">    :return: html</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    proxies = &#123;<span class="string">'http'</span>: <span class="string">'http://'</span> + proxy&#125;</span><br><span class="line">    <span class="keyword">return</span> requests.get(url, proxies=proxies).text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    main method, entry point</span></span><br><span class="line"><span class="string">    :return: none</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    proxy = get_random_proxy()</span><br><span class="line">    print(<span class="string">'get random proxy'</span>, proxy)</span><br><span class="line">    html = crawl(target_url, proxy)</span><br><span class="line">    print(html)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="付费代理的使用"><a href="#付费代理的使用" class="headerlink" title="付费代理的使用"></a>付费代理的使用</h3><h3 id="ADSL拨号代理"><a href="#ADSL拨号代理" class="headerlink" title="ADSL拨号代理"></a>ADSL拨号代理</h3><p>ADSL(Asymmetric Digital Subscriber Line，非对称数字用户环路），它的上行和下行带宽不对称，采用频分复用技术把普通的电话线分成了电话、上行和下行3个相对独立的信道，从而避免了相互之间的干扰。<br>ADSL通过拨号的方式上网，需要输入ADSL账号和密码，每次拨号就更换一个IP。IP分布在多个A段，如果IP都能使用，则意味着E量级可达千万。如果我们将ADSL主机作为代理，每隔一段时间主机拨号就换一个IP，这样可以有效防止IP被封禁。另外，主机的稳定性很好，代理响应速度很快。<br>参考项目：<a href="https://github.com/Python3WebSpider/AdslProxy" target="_blank" rel="noopener">https://github.com/Python3WebSpider/AdslProxy</a></p><ul><li>准备工作</li><li>设置代理服务器</li><li>动态获取IP</li><li>存储模块</li><li>拨号模块</li><li>接口模块</li></ul><h3 id="使用代理爬取微信公众号文章"><a href="#使用代理爬取微信公众号文章" class="headerlink" title="使用代理爬取微信公众号文章"></a>使用代理爬取微信公众号文章</h3><p>参考项目：<a href="https://github.com/Python3WebSpider/Weixin" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Weixin</a></p><ul><li>主要实现功能：<ul><li>修改代理池检测链接为搜狗微信站点。</li><li>构造Redis爬取队列，用队列实现请求的存取。</li><li>实现异常处理，失败的请求重新加入队列 。</li><li>实现翻页和提取文章列表，并把对应请求加入队列。</li><li>实现微信文章的信息的提取。</li><li>将提取到的信息保存到MySQL。</li></ul></li><li>构造请求<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> weixin.config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeixinRequest</span><span class="params">(Request)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url, callback, method=<span class="string">'GET'</span>, headers=None, need_proxy=False, fail_time=<span class="number">0</span>, timeout=TIMEOUT)</span>:</span></span><br><span class="line">        <span class="comment"># 调用父类Request的__init__()方法</span></span><br><span class="line">        Request.__init__(self, method, url, headers)</span><br><span class="line">        <span class="comment"># 添加需要的额外属性</span></span><br><span class="line">        self.callback = callback</span><br><span class="line">        self.need_proxy = need_proxy</span><br><span class="line">        self.fail_time = fail_time</span><br><span class="line">        self.timeout = timeout</span><br></pre></td></tr></table></figure></li><li>实现请求队列<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> StrictRedis</span><br><span class="line"><span class="keyword">from</span> weixin.config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pickle <span class="keyword">import</span> dumps, loads</span><br><span class="line"><span class="keyword">from</span> weixin.request <span class="keyword">import</span> WeixinRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisQueue</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化Redis</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.db = StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        向队列添加序列化后的Request</span></span><br><span class="line"><span class="string">        :param request: 请求对象</span></span><br><span class="line"><span class="string">        :param fail_time: 失败次数</span></span><br><span class="line"><span class="string">        :return: 添加结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(request, WeixinRequest):</span><br><span class="line">            <span class="keyword">return</span> self.db.rpush(REDIS_KEY, dumps(request))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        取出下一个Request并反序列化</span></span><br><span class="line"><span class="string">        :return: Request or None</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.db.llen(REDIS_KEY):</span><br><span class="line">            <span class="keyword">return</span> loads(self.db.lpop(REDIS_KEY))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.db.delete(REDIS_KEY)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">empty</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.db.llen(REDIS_KEY) == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    db = RedisQueue()</span><br><span class="line">    start_url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    weixin_request = WeixinRequest(url=start_url, callback=<span class="string">'hello'</span>, need_proxy=<span class="literal">True</span>)</span><br><span class="line">    db.add(weixin_request)</span><br><span class="line">    request = db.pop()</span><br><span class="line">    print(request)</span><br><span class="line">    print(request.callback, request.need_proxy)</span><br></pre></td></tr></table></figure></li><li>MySQL存储<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> weixin.config <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host=MYSQL_HOST, username=MYSQL_USER, password=MYSQL_PASSWORD, port=MYSQL_PORT,</span></span></span><br><span class="line"><span class="function"><span class="params">                 database=MYSQL_DATABASE)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        MySQL初始化</span></span><br><span class="line"><span class="string">        :param host:</span></span><br><span class="line"><span class="string">        :param username:</span></span><br><span class="line"><span class="string">        :param password:</span></span><br><span class="line"><span class="string">        :param port:</span></span><br><span class="line"><span class="string">        :param database:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.db = pymysql.connect(host, username, password, database, charset=<span class="string">'utf8'</span>, port=port)</span><br><span class="line">            self.cursor = self.db.cursor()</span><br><span class="line">        <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, table, data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        插入数据</span></span><br><span class="line"><span class="string">        :param table:</span></span><br><span class="line"><span class="string">        :param data:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">        values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">        sql_query = <span class="string">'insert into %s (%s) values (%s)'</span> % (table, keys, values)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.cursor.execute(sql_query, tuple(data.values()))</span><br><span class="line">            self.db.commit()</span><br><span class="line">        <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line">            self.db.rollback()</span><br></pre></td></tr></table></figure></li><li>修改代理池<br>将测试代理的网址替换为目标爬取网址。筛选出可用代理</li><li>调度请求（主方法）<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Session</span><br><span class="line"><span class="keyword">from</span> weixin.config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> weixin.db <span class="keyword">import</span> RedisQueue</span><br><span class="line"><span class="keyword">from</span> weixin.mysql <span class="keyword">import</span> MySQL</span><br><span class="line"><span class="keyword">from</span> weixin.request <span class="keyword">import</span> WeixinRequest</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> ReadTimeout, ConnectionError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider</span><span class="params">()</span>:</span></span><br><span class="line">    base_url = <span class="string">'http://weixin.sogou.com/weixin'</span></span><br><span class="line">    keyword = <span class="string">'NBA'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'</span>,</span><br><span class="line">        <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate'</span>,</span><br><span class="line">        <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8,en;q=0.6,ja;q=0.4,zh-TW;q=0.2,mt;q=0.2'</span>,</span><br><span class="line">        <span class="string">'Cache-Control'</span>: <span class="string">'max-age=0'</span>,</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</span><br><span class="line">        <span class="string">'Cookie'</span>: <span class="string">'IPLOC=CN1100; SUID=6FEDCF3C541C940A000000005968CF55; SUV=1500041046435211; ABTEST=0|1500041048|v1; SNUID=CEA85AE02A2F7E6EAFF9C1FE2ABEBE6F; weixinIndexVisited=1; JSESSIONID=aaar_m7LEIW-jg_gikPZv; ld=Wkllllllll2BzGMVlllllVOo8cUlllll5G@HbZllll9lllllRklll5@@@@@@@@@@; LSTMV=212%2C350; LCLKINT=4650; ppinf=5|1500042908|1501252508|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZTo1NDolRTUlQjQlOTQlRTUlQkElODYlRTYlODklOEQlRTQlQjglQTglRTklOUQlOTklRTglQTclODV8Y3J0OjEwOjE1MDAwNDI5MDh8cmVmbmljazo1NDolRTUlQjQlOTQlRTUlQkElODYlRTYlODklOEQlRTQlQjglQTglRTklOUQlOTklRTglQTclODV8dXNlcmlkOjQ0Om85dDJsdUJfZWVYOGRqSjRKN0xhNlBta0RJODRAd2VpeGluLnNvaHUuY29tfA; pprdig=ppyIobo4mP_ZElYXXmRTeo2q9iFgeoQ87PshihQfB2nvgsCz4FdOf-kirUuntLHKTQbgRuXdwQWT6qW-CY_ax5VDgDEdeZR7I2eIDprve43ou5ZvR0tDBlqrPNJvC0yGhQ2dZI3RqOQ3y1VialHsFnmTiHTv7TWxjliTSZJI_Bc; sgid=27-27790591-AVlo1pzPiad6EVQdGDbmwnvM; PHPSESSID=mkp3erf0uqe9ugjg8os7v1e957; SUIR=CEA85AE02A2F7E6EAFF9C1FE2ABEBE6F; sct=11; ppmdig=1500046378000000b7527c423df68abb627d67a0666fdcee; successCount=1|Fri, 14 Jul 2017 15:38:07 GMT'</span>,</span><br><span class="line">        <span class="string">'Host'</span>: <span class="string">'weixin.sogou.com'</span>,</span><br><span class="line">        <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    session = Session()</span><br><span class="line">    queue = RedisQueue()</span><br><span class="line">    mysql = MySQL()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        从代理池获取代理</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response = requests.get(PROXY_POOL_URL)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                print(<span class="string">'Get Proxy'</span>, response.text)</span><br><span class="line">                <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化工作</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 全局更新Headers</span></span><br><span class="line">        self.session.headers.update(self.headers)</span><br><span class="line">        start_url = self.base_url + <span class="string">'?'</span> + urlencode(&#123;<span class="string">'query'</span>: self.keyword, <span class="string">'type'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">        weixin_request = WeixinRequest(url=start_url, callback=self.parse_index, need_proxy=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 调度第一个请求</span></span><br><span class="line">        self.queue.add(weixin_request)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        解析索引页</span></span><br><span class="line"><span class="string">        :param response: 响应</span></span><br><span class="line"><span class="string">        :return: 新的响应</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        doc = pq(response.text)</span><br><span class="line">        items = doc(<span class="string">'.news-box .news-list li .txt-box h3 a'</span>).items()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            url = item.attr(<span class="string">'href'</span>)</span><br><span class="line">            weixin_request = WeixinRequest(url=url, callback=self.parse_detail)</span><br><span class="line">            <span class="keyword">yield</span> weixin_request</span><br><span class="line">        next = doc(<span class="string">'#sogou_next'</span>).attr(<span class="string">'href'</span>)</span><br><span class="line">        <span class="keyword">if</span> next:</span><br><span class="line">            url = self.base_url + str(next)</span><br><span class="line">            weixin_request = WeixinRequest(url=url, callback=self.parse_index, need_proxy=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">yield</span> weixin_request</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        解析详情页</span></span><br><span class="line"><span class="string">        :param response: 响应</span></span><br><span class="line"><span class="string">        :return: 微信公众号文章</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        doc = pq(response.text)</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'title'</span>: doc(<span class="string">'.rich_media_title'</span>).text(),</span><br><span class="line">            <span class="string">'content'</span>: doc(<span class="string">'.rich_media_content'</span>).text(),</span><br><span class="line">            <span class="string">'date'</span>: doc(<span class="string">'#post-date'</span>).text(),</span><br><span class="line">            <span class="string">'nickname'</span>: doc(<span class="string">'#js_profile_qrcode &gt; div &gt; strong'</span>).text(),</span><br><span class="line">            <span class="string">'wechat'</span>: doc(<span class="string">'#js_profile_qrcode &gt; div &gt; p:nth-child(3) &gt; span'</span>).text()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">yield</span> data</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request</span><span class="params">(self, weixin_request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行请求</span></span><br><span class="line"><span class="string">        :param weixin_request: 请求</span></span><br><span class="line"><span class="string">        :return: 响应</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> weixin_request.need_proxy:</span><br><span class="line">                proxy = self.get_proxy()</span><br><span class="line">                <span class="keyword">if</span> proxy:</span><br><span class="line">                    proxies = &#123;</span><br><span class="line">                        <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">                        <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">return</span> self.session.send(weixin_request.prepare(),</span><br><span class="line">                                             timeout=weixin_request.timeout, allow_redirects=<span class="literal">False</span>, proxies=proxies)</span><br><span class="line">            <span class="keyword">return</span> self.session.send(weixin_request.prepare(), timeout=weixin_request.timeout, allow_redirects=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">except</span> (ConnectionError, ReadTimeout) <span class="keyword">as</span> e:</span><br><span class="line">            print(e.args)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(self, weixin_request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        错误处理</span></span><br><span class="line"><span class="string">        :param weixin_request: 请求</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        weixin_request.fail_time = weixin_request.fail_time + <span class="number">1</span></span><br><span class="line">        print(<span class="string">'Request Failed'</span>, weixin_request.fail_time, <span class="string">'Times'</span>, weixin_request.url)</span><br><span class="line">        <span class="keyword">if</span> weixin_request.fail_time &lt; MAX_FAILED_TIME:</span><br><span class="line">            self.queue.add(weixin_request)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">schedule</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        调度请求</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> self.queue.empty():</span><br><span class="line">            weixin_request = self.queue.pop()</span><br><span class="line">            callback = weixin_request.callback</span><br><span class="line">            print(<span class="string">'Schedule'</span>, weixin_request.url)</span><br><span class="line">            response = self.request(weixin_request)</span><br><span class="line">            <span class="keyword">if</span> response <span class="keyword">and</span> response.status_code <span class="keyword">in</span> VALID_STATUSES:</span><br><span class="line">                results = list(callback(response))</span><br><span class="line">                <span class="keyword">if</span> results:</span><br><span class="line">                    <span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">                        print(<span class="string">'New Result'</span>, type(result))</span><br><span class="line">                        <span class="keyword">if</span> isinstance(result, WeixinRequest):</span><br><span class="line">                            self.queue.add(result)</span><br><span class="line">                        <span class="keyword">if</span> isinstance(result, dict):</span><br><span class="line">                            self.mysql.insert(<span class="string">'articles'</span>, result)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.error(weixin_request)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.error(weixin_request)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        入口</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.start()</span><br><span class="line">        self.schedule()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = Spider()</span><br><span class="line">    spider.run()</span><br></pre></td></tr></table></figure></li><li>运行</li></ul><h2 id="Ch-9-模拟登录"><a href="#Ch-9-模拟登录" class="headerlink" title="Ch 9 模拟登录"></a>Ch 9 模拟登录</h2><h3 id="模拟登录并爬取GitHub"><a href="#模拟登录并爬取GitHub" class="headerlink" title="模拟登录并爬取GitHub"></a>模拟登录并爬取GitHub</h3><h3 id="Cookie池的搭建"><a href="#Cookie池的搭建" class="headerlink" title="Cookie池的搭建"></a>Cookie池的搭建</h3><h2 id="Ch-10-APP的爬取"><a href="#Ch-10-APP的爬取" class="headerlink" title="Ch 10 APP的爬取"></a>Ch 10 APP的爬取</h2><h3 id="Charles的使用"><a href="#Charles的使用" class="headerlink" title="Charles的使用"></a>Charles的使用</h3><h3 id="mitmproxy的使用"><a href="#mitmproxy的使用" class="headerlink" title="mitmproxy的使用"></a>mitmproxy的使用</h3><h3 id="mitmdump爬取“得到”电子书信息"><a href="#mitmdump爬取“得到”电子书信息" class="headerlink" title="mitmdump爬取“得到”电子书信息"></a>mitmdump爬取“得到”电子书信息</h3><h3 id="APPium的基本使用"><a href="#APPium的基本使用" class="headerlink" title="APPium的基本使用"></a>APPium的基本使用</h3><h3 id="APPium爬取微信朋友圈"><a href="#APPium爬取微信朋友圈" class="headerlink" title="APPium爬取微信朋友圈"></a>APPium爬取微信朋友圈</h3><h3 id="APPium-mitmdump爬取京东商品"><a href="#APPium-mitmdump爬取京东商品" class="headerlink" title="APPium+mitmdump爬取京东商品"></a>APPium+mitmdump爬取京东商品</h3><h2 id="Ch-11-pyspider框架使用"><a href="#Ch-11-pyspider框架使用" class="headerlink" title="Ch 11 pyspider框架使用"></a>Ch 11 pyspider框架使用</h2><p><span style="border-bottom:2px solid red">pyspider框架应用场景较为单一，且可扩展程度不足，比较适合爬取固定分页展示内容的数据。</span></p><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><ul><li>pyspider是由国人binux编写的强大的网络爬虫系统，其GitHub地址为<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a>，官方文档地址为<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a>。</li><li>pyspider带有强大的 WebUI、脚本编辑器、任务监控器、项目管理器以及结果处理器，它支持多种数据库后端、多种消息队列、JavaScript渲染页面的爬取，使用起来非常方便。</li><li>pyspider开发快速便捷，适合中小型项目。</li></ul><h4 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h4><ul><li>提供方便易用的WebUI系统，可视化地编写和调试爬虫。</li><li>提供爬取进度监控、爬取结果查看、爬虫项目管理等功能。</li><li>支持多种后端数据库，如MySQL、MongoDB、Redis、SQLite、Elasticsearch、PostgreSQL。</li><li>支持多种消息队列，如RabbitMQ、Beanstalk、Redis、Kombu。</li><li>提供优先级控制、失败重试、定时抓取等功能。</li><li>对接了PhantomJS，可以抓取JavaScript渲染的页面。</li><li>支持单机和分布式部署，支持Docker部署。</li></ul><h4 id="与Scrapy比较"><a href="#与Scrapy比较" class="headerlink" title="与Scrapy比较"></a>与Scrapy比较</h4><ul><li>pyspider提供了WebUI，爬虫的编写、调试都是在WebUI中进行的。而Scrapy原生是不具备这个功能的，它采用的是代码和命令行操作，但可以通过对接Portia实现可视化配置。</li><li>pyspider调试非常方便，WebUI操作便捷直观。Scrapy则是使用parse命令进行调试，不够方便。</li><li>pyspider支持PhantomJS来进行JavaScript谊染页面的采集。Scrapy可以对接Scrapy-Splash组件，这需要额外配置。</li><li><span style="border-bottom:2px solid red">pyspider中内置了pyquery作为选择器</span>。Scrapy对接了XPath、css选择器和正则匹配。</li><li>pyspider的可扩展程度不足，可配制化程度不高。Scrapy可以通过对接Middleware、Pipeline、Extension等组件实现非常强大的功能，模块之间的耦合程度低，可扩展程度极高。</li></ul><h4 id="pyspider架构"><a href="#pyspider架构" class="headerlink" title="pyspider架构"></a>pyspider架构</h4><p>pyspider的架构主要分为Scheduler（调度器）、Fetcher（ 抓取器）、Processer（处理器）三个部分，整个爬取过程受到Monitor（监控器）的监控，抓取的结果被Result Worker（结果处理器）处理，如图：<br><img src="https://s3.ax1x.com/2021/01/17/sse47d.png" alt="pyspider 架构图"></p><p>Scheduler发起任务调度，Fetcher负责抓取网页内容，Processer负责解析网页内容，然后将新生成的Request发给Scheduler进行调度，将生成的提取结果输出保存。<br>执行过程如下：</p><ul><li>每个pyspider的项目对应一个Python脚本，该脚本中定义了一个Handler类，它有一个on_start()方法。爬取首先调用on_start()方法生成最初的抓取任务，然后发送给Scheduler进行调度。</li><li>Scheduler将抓取任务分发给Fetcher进行抓取，Fetcher执行并得到响应，随后将响应发送给Processer。</li><li>Processer处理响应并提取出新的URL生成新的抓取任务，然后通过消息队列的方式通知Schduler当前抓取任务执行情况，并将新生成的抓取任务发送给 Scheduler。如果生成了新的提取结果，则将其发送到结果队列等待Result Worker处理。</li><li>Scheduler接收到新的抓取任务，然后查询数据库，判断其如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其发送回Fetcher进行抓取。</li><li>不断重复以上工作，直到所有的任务都执行完毕，抓取结束。</li><li>抓取结束后，程序会回调on_finished()方法，这里可以定义后处理过程。</li></ul><h3 id="基本使用-1"><a href="#基本使用-1" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h4><ul><li>官方文档：<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a></li><li>PyPI：<a href="https://pypi.python.org/pypi/pyspider" target="_blank" rel="noopener">https://pypi.python.org/pypi/pyspider</a></li><li>GitHub：<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a></li><li>官方教程：<a href="http://docs.pyspider.org/en/latest/tutorial" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/tutorial</a></li><li>在线实例：<a href="http://demo.pyspider.org" target="_blank" rel="noopener">http://demo.pyspider.org</a></li></ul><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ul><li><code>pip3 install pyspider</code></li><li>PhantomJS安装：<a href="https://gitee.com/xxyrs/filehouse/raw/master/phantomjs-2.1.1-windows.zip" target="_blank" rel="noopener">phantomjs-2.1.1-windows.zip</a>，解压后将bin路径配置到用户Path中</li><li>常见错误及解决：<br><a href="https://www.cnblogs.com/Mayfly-nymph/p/10808088.html" target="_blank" rel="noopener">https://www.cnblogs.com/Mayfly-nymph/p/10808088.html</a><br><a href="https://github.com/binux/pyspider/issues/898" target="_blank" rel="noopener">https://github.com/binux/pyspider/issues/898</a><br><a href="https://www.cnblogs.com/shaosks/p/6856086.html" target="_blank" rel="noopener">https://www.cnblogs.com/shaosks/p/6856086.html</a></li><li>启动：<code>pyspider [all]</code></li></ul><h4 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspider.libs.base_handler <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config = &#123;</span><br><span class="line">        <span class="string">'itag'</span>: <span class="string">'v223'</span>,</span><br><span class="line">        <span class="string">"headers"</span>:&#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: UserAgent().random,</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @every(minutes=24 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.crawl(<span class="string">'http://travel.qunar.com/travelbook/list.htm'</span>, callback=self.index_page, fetch_type=<span class="string">'js'</span>, validate_cert=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @config(age=10 * 24 * 60 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.doc(<span class="string">'li &gt; .tit &gt; a'</span>).items():</span><br><span class="line">            <span class="comment">#fetch_type='js'，使用PhantomJS渲染</span></span><br><span class="line">            self.crawl(each.attr.href, callback=self.detail_page, validate_cert=<span class="literal">False</span>, fetch_type=<span class="string">'js'</span>)</span><br><span class="line">        <span class="comment">#获取下一页链接</span></span><br><span class="line">        next = response.doc(<span class="string">'.next'</span>).attr.href</span><br><span class="line">        self.crawl(next, callback=self.index_page, validate_cert=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @config(priority=2)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'url'</span>: response.url,</span><br><span class="line">            <span class="string">'title'</span>: response.doc(<span class="string">'#booktitle'</span>).text(),</span><br><span class="line">            <span class="string">'date'</span>: response.doc(<span class="string">'.when .data'</span>).text(),</span><br><span class="line">            <span class="string">'day'</span>: response.doc(<span class="string">'.howlong .data'</span>).text(),</span><br><span class="line">            <span class="string">'who'</span>: response.doc(<span class="string">'.who .data'</span>).text(),</span><br><span class="line">            <span class="string">'text'</span>: response.doc(<span class="string">'#b_panel_schedule'</span>).text(),</span><br><span class="line">            <span class="comment"># "btall": [(x.find('a').text(), x.find('a').eq(1).attr.href[0:59]) for x in response.doc('.dlist li').items()],</span></span><br><span class="line">            <span class="string">'image'</span>: response.doc(<span class="string">'.cover_img'</span>).attr.src</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h4 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h4><p><img src="https://s3.ax1x.com/2021/01/18/sy04PI.png" alt="新建项目"></p><h4 id="爬取首页"><a href="#爬取首页" class="headerlink" title="爬取首页"></a>爬取首页</h4><p><img src="https://s3.ax1x.com/2021/01/18/sywvjO.png" alt="爬取首页"></p><h4 id="爬取详情页"><a href="#爬取详情页" class="headerlink" title="爬取详情页"></a>爬取详情页</h4><p><img src="https://s3.ax1x.com/2021/01/18/sysBwD.png" alt="爬取详情页"></p><p>爬取的页面无法显示图片，出现此现象的原因是pyspider默认发送HTTP请求，请求的HTML文档本身就不包含img节点。但是在浏览器中我们看到了图片，这是因为这张图片是后期经过JavaScript出现的。<br>pyspider内部对接了PhantomJS，将index_page()中生成抓取详情页的请求方法添加一个参数 <code>fetch_type=&#39;js&#39;</code>即可</p><h4 id="启动爬虫"><a href="#启动爬虫" class="headerlink" title="启动爬虫"></a>启动爬虫</h4><ul><li>在最左侧可以定义项目的分组，以便管理。</li><li>rate/burst代表当前的爬取速率，rate代表每秒发出多少个请求，burst（并发数）相当于流量控制中的令牌桶算法的令牌数，rate和burst设置的越大，爬取速率越快。</li><li>process中的5m、1h、1d指的是最近5分、l小时、l天内的请求情况，all代表所有的请求情况。请求由不同颜色表示，蓝色的代表等待被执行的请求，绿色的代表成功的请求，黄色的代表请求失败后等待重试的请求，红色的代表失败次数过多被忽略的请求，这样可以直观知道爬取的进度和请求情况。</li><li>点击Active Tasks，可查看最近请求的详细状况。</li><li>点击Results，查看所有爬取结果。</li></ul><p><img src="https://s3.ax1x.com/2021/01/18/s61te0.png" alt=""></p><h3 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h3><p>参见官方文档：<a href="http://docs.pyspider.org/" target="_blank" rel="noopener">http://docs.pyspider.org/</a></p><h4 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h4><p><a href="http://docs.pyspider.org/en/latest/Command-Line/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/Command-Line/</a></p><h4 id="crawl-方法"><a href="#crawl-方法" class="headerlink" title="crawl()方法"></a>crawl()方法</h4><p><a href="http://docs.pyspider.org/en/latest/apis/self.crawl/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/apis/self.crawl/</a></p><h4 id="任务区分"><a href="#任务区分" class="headerlink" title="任务区分"></a>任务区分</h4><p>在pyspider中判断两个任务是否是重复的，使用的是该任务对应的URL的MD5值作为任务的唯一ID，如果ID相同，那么两个任务就会判定为相同，其中一个就不会爬取。这时可以重写task_id()方法，改变这个ID的计算方式来实现不同任务的区分，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pyspider. libs. utils <span class="keyword">import</span> mdsstring</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_taskid</span><span class="params">(self, task)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> mdsstring(task[<span class="string">'url ’]+json.dumps(task['</span>fetch<span class="string">'].get ('</span>data <span class="string">' , " )) )</span></span><br></pre></td></tr></table></figure><h4 id="全局配置"><a href="#全局配置" class="headerlink" title="全局配置"></a>全局配置</h4><p>pyspider可以使用crawl_config来指定全局的配置，配置中的参数会和crawl()方法创建任务时的参数合井。<br>如要全局配置一个Headers，可以定义如下代码：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">  crawl_config = &#123;</span><br><span class="line">      <span class="string">'headers'</span>:&#123;</span><br><span class="line">          <span class="string">'User-Agent'</span>:<span class="string">'GoogleBot'</span>,</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="定时爬取"><a href="#定时爬取" class="headerlink" title="定时爬取"></a>定时爬取</h4><p>通过every属性来设置爬取的时间间隔：<br>在有效时间内爬取不会重复。所以要把有效时间设置得比重复时间更短，即age的时间小于minutes的时间这样才可以实现定时爬取。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#minutes为爬取的时间间隔，单位为分钟</span></span><br><span class="line"><span class="comment">#或写为seconds秒数</span></span><br><span class="line"><span class="meta">@every(minutes=24 * 60)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">  self.crawl(<span class="string">'http://www.example.org'</span>, callback=self.index_page)</span><br><span class="line"></span><br><span class="line"><span class="comment">#age为任务的有效时间，单位为秒</span></span><br><span class="line"><span class="meta">@config(age=10*24*60*60)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self)</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="项目状态"><a href="#项目状态" class="headerlink" title="项目状态"></a>项目状态</h4><ul><li>TODO：它是项目刚刚被创建还未实现时的状态。</li><li>STOP：如果想停止某项目的抓取，可以将项目的状态设置为STOP 。</li><li>CHECKING：正在运行的项目被修改后就会变成CHECKING状态，项目在中途出错需要调整的时候会遇到这种情况。</li><li>DEBUG/RUNNING：这两个状态对项目的运行没有影响，状态设置为任意一个，项目都可以运行，但是可以用二者来区分项目是否已经测试通过。</li><li>PAUSE：当爬取过程中出现连续多次错误时，项目会自动设置为PAUSE状态，并等待一定时间后继续爬取。</li></ul><h4 id="删除项目"><a href="#删除项目" class="headerlink" title="删除项目"></a>删除项目</h4><p>pyspider中没有直接删除项目的选项。如要删除任务，那么将项目的状态设置为STOP，将分组的名称设置为delete，等待24小时，则项目会自动删除。</p><h2 id="Ch-12-Scrapy框架使用"><a href="#Ch-12-Scrapy框架使用" class="headerlink" title="Ch 12 Scrapy框架使用"></a>Ch 12 Scrapy框架使用</h2><h3 id="Scrapy框架介绍"><a href="#Scrapy框架介绍" class="headerlink" title="Scrapy框架介绍"></a>Scrapy框架介绍</h3><p>Scrapy是一个基于Twisted的异步处理框架，是纯Python实现的爬虫框架，架构清晰，模块之间的耦合程度低，可扩展性极强，可以灵活完成各种需求。</p><h4 id="架构介绍"><a href="#架构介绍" class="headerlink" title="架构介绍"></a>架构介绍</h4><ul><li>Scrapy框架的架构，如图。<br><img src="https://s3.ax1x.com/2021/01/29/yiwNYd.png" alt="Scrapy架构"></li><li>分为如下的几个部分。<ul><li>Engine。引擎，处理整个系统的数据流处理、触发事务，是整个框架的核心。</li><li>Item。项目，定义了爬取结果的数据结构，爬取的数据会被赋值成该Item对象。</li><li>Scheduler。调度器，接受引擎发过来的请求并将其加入队列中，在引擎再次请求的时候将请求提供给引擎。</li><li>Downloader。下载器，下载网页内容，并将网页内容返回给Spiders。</li><li>Spiders。爬虫，定义了爬取的逻辑和网页的解析规则，主要负责解析响应并生成提取结果和新的请求。</li><li>Item Pipeline。项目管道，负责处理由爬虫从网页中抽取的项目，主要任务是清洗、验证和存储数据。</li><li>Downloader Middlewares。下载器中间件，位于引擎和下载器之间的钩子框架，主要处理引擎与下载器之间的请求及响应 。</li><li>Spider Middlewares。爬虫中间件，位于引擎和爬虫之间的钩子框架，主要处理爬虫输入的响应和输出的结果及新的请求 。</li></ul></li></ul><h4 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h4><ul><li>Scrapy中的数据流由引擎控制，数据流的过程如下。<ul><li>Engine首先打开一个网站，找到处理该网站的Spider，并向该Spider请求第一个要爬取的URL。</li><li>Engine从Spider中获取到第一个要爬取的URL，并通过Scheduler以Request的形式调度。</li><li>Engine向Scheduler请求下一个要爬取的URL。</li><li>Scheduler返回下一个要爬取的URL给Engine,Engine将URL通过Downloader MiddJewares转发给Downloader下载。</li><li>一旦页面下载完毕，Downloader生成该页面的Response，并将其通过Downloader Middlewares发送给Engine。</li><li>Engine从下载器中接收到Response，并将其通过Spider Middlewares发送给Spider处理。</li><li>Spider处理Response，并返回爬取到的Item及新的Request给Engine。</li><li>Engine将Spider返回的Item给Item Pipeline，将新的Request给Scheduler。</li><li>重复第2步到第8步，直到Scheduler中没有更多的Request, Engine关闭该网站，爬取结束。通过多个组件的相互协作、不同组件完成工作的不同、组件对异步处理的支持，Scrapy最大限度地利用了网络带宽，大大提高了数据爬取和处理的效率。</li></ul></li></ul><h4 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h4><p>项目文件基本结构如下所示:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scrapy.cfg</span><br><span class="line">project/</span><br><span class="line">  __init__.py</span><br><span class="line">  items.py</span><br><span class="line">  pipelines.py</span><br><span class="line">  settings.py</span><br><span class="line">  middlewares.py</span><br><span class="line">  spiders/</span><br><span class="line">    __init__.py</span><br><span class="line">    spiderl.py</span><br><span class="line">    spider2.py</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><ul><li>各个文件的功能描述如下。<ul><li>scrapy.cfg ：Scrapy项目的配置文件，其内定义了项目的配置文件路径、部署相关信息等内容。</li><li>items.py：定义了Item数据结构，所有的Item的定义都可以放这里。</li><li>pipelines.py：定义了Item Pipeline的实现，所有的Item Pipeline的实现都可以放这里。</li><li>settings.py：定义了项目的全局配置。</li><li>middlewares.py：它定义Spider Middlewares和Downloader Middlewares的实现。</li><li>spiders：其内包含一个个Spider的实现，每个Spider都有一个文件。</li></ul></li></ul><h3 id="Scrapy入门"><a href="#Scrapy入门" class="headerlink" title="Scrapy入门"></a>Scrapy入门</h3><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>安装：<code>pip install scrapy</code><br>官方文档：<a href="https://docs.scrapy.org/en/latest/intro/tutorial.html" target="_blank" rel="noopener">https://docs.scrapy.org/en/latest/intro/tutorial.html</a><br>中文文档：<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/tutorial.html" target="_blank" rel="noopener">https://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/tutorial.html</a></p><h4 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h4><p><code>scrapy startproject tutorial</code><br>在本目录创建一个名为tutorial的项目文件夹。文件夹结构如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scrapy.cfg      # Scrapy部署时的配置文件</span><br><span class="line">tutorial/        # 项目的模块，需要从这里引入</span><br><span class="line">  __init__.py</span><br><span class="line">  items.py        # Items的定义，定义爬取的数据结构</span><br><span class="line">  middlewares.py  # Middlewares的定义，定义爬取时的中间件</span><br><span class="line">  pipelines.py    # Pipelines的定义，定义数据管道</span><br><span class="line">  settings.py     # 配置文件</span><br><span class="line">  spiders/         # 放置Spiders的文件夹</span><br><span class="line">    __init__.py</span><br></pre></td></tr></table></figure><h4 id="创建Spider"><a href="#创建Spider" class="headerlink" title="创建Spider"></a>创建Spider</h4><p>Spider是自己定义的类，用来从网页里抓取内容，并解析抓取的结果。这个类必须继承Scrapy提供的Spider类scrapy.Spider，还要定义Spider的名称和起始请求，以及怎样处理爬取后的结果的方法。<br>使用命令行创建一个Spider：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd tutorial</span><br><span class="line">scrapy genspider quotes quotes.toscrape.com</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第一个参数是Spider的名称，第二个参数是网站域名</span></span><br></pre></td></tr></table></figure><p>执行完毕之后，spiders文件夹中多了一个quotes.py，就是刚刚创建的Spider，内容如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'quotes'</span></span><br><span class="line">    allowed_domains = [<span class="string">'quotes.toscrape.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># name，是每个项目唯一的名字，用来区分不同的Spider。</span></span><br><span class="line"><span class="comment"># allowed domains，允许爬取的域名，如果初始或后续的请求链接不是这个域名下的，则请求链接会被过滤掉。</span></span><br><span class="line"><span class="comment"># start_urls，包含了Spider在启动时爬取的url列表，初始请求是由它来定义的。</span></span><br><span class="line"><span class="comment"># parse，是Spider的一个方法。默认情况下，被调用时start_urls里面的链接构成的请求完成下载执行后，返回的响应</span></span><br><span class="line"><span class="comment">#        就会作为唯一的参数传递给这个函数。该方法负责解析返回的响应、提取数据或者进一步生成要处理的请求。</span></span><br></pre></td></tr></table></figure><h4 id="创建Item"><a href="#创建Item" class="headerlink" title="创建Item"></a>创建Item</h4><p>Item是保存爬取数据的容器，它的使用方法和字典类似。不过，相比字典，Item多了额外的保护机制，可以避免拼写错误或者定义字段错误。<br>创建Item需要继承scrapy.Item类，并且定义类型为scrapy.Field的字段。观察目标网站，我们可以获取到到内容有text、author、tags。<br>定义Item，此时将items.py修改如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuoteItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    text = scrapy.Field()</span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field()</span><br></pre></td></tr></table></figure><h4 id="解析Response"><a href="#解析Response" class="headerlink" title="解析Response"></a>解析Response</h4><p>Spider内的parse()方法的参数resposne是start_urls里面的链接爬取后的结果。所以在parse())方法中，我们可以直接对response变量包含的内容进行解析，比如浏览请求结果的网页源代码，或者进一步分析源代码内容，或者找出结果中的链接而得到下一个请求。<br>网页中既有我们想要的结果，又有下一页的链接，这两部分内容我们都要进行处理。提取的方式可以是css选择器或XPath选择器。这里使用css选择器进行选择，parse()方法的改写如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    quotes = response.css(<span class="string">'.quote'</span>)</span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">        <span class="comment"># 结果是长度为1的列表，所以需要用extract_first()方法来获取第一个元素</span></span><br><span class="line">        text = quote.css (<span class="string">'.text::text'</span>).extract_first()</span><br><span class="line">        author = quote.css(<span class="string">'.author::text'</span>).extract.first()</span><br><span class="line">        <span class="comment"># 获取所有的标签，所以用extract()方法获取整个列表</span></span><br><span class="line">        tags = quote.css(<span class="string">'.tags .tag::text'</span>).extract()</span><br></pre></td></tr></table></figure><h4 id="使用Item"><a href="#使用Item" class="headerlink" title="使用Item"></a>使用Item</h4><p>Item可以理解为一个字典，在声明的时候需要实例化，然后依次用刚才解析的结果赋值Item的每一个字段，最后将Item返回。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    quotes = response.css(<span class="string">'.quote'</span>)</span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">        item = QuoteItem()</span><br><span class="line">        <span class="comment"># 结果是长度为1的列表，所以需要用extract_first()方法来获取第一个元素</span></span><br><span class="line">        item[<span class="string">'text'</span>] = quote.css(<span class="string">'.text::text'</span>).extract_first()</span><br><span class="line">        item[<span class="string">'author'</span>] = quote.css(<span class="string">'.author::text'</span>).extract_first()</span><br><span class="line">        <span class="comment"># 获取所有的标签，所以用extract()方法获取整个列表</span></span><br><span class="line">        item[<span class="string">'tags'</span>] = quote.css(<span class="string">'.tags .tag::text'</span>).extract()</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h4 id="后续Request"><a href="#后续Request" class="headerlink" title="后续Request"></a>后续Request</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> QuoteItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'quotes'</span></span><br><span class="line">    allowed_domains = [<span class="string">'quotes.toscrape.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        quotes = response.css(<span class="string">'.quote'</span>)</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">            item = QuoteItem()</span><br><span class="line">            <span class="comment"># 结果是长度为1的列表，所以需要用extract_first()方法来获取第一个元素</span></span><br><span class="line">            item[<span class="string">'text'</span>] = quote.css(<span class="string">'.text::text'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'author'</span>] = quote.css(<span class="string">'.author::text'</span>).extract_first()</span><br><span class="line">            <span class="comment"># 获取所有的标签，所以用extract()方法获取整个列表</span></span><br><span class="line">            item[<span class="string">'tags'</span>] = quote.css(<span class="string">'.tags .tag::text'</span>).extract()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">        next = response.css(<span class="string">'.pager .next a::attr(href)'</span>).extract_first()</span><br><span class="line">        url = response.urljoin(next)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br></pre></td></tr></table></figure><ul><li>callback：回调函数。当指定了该回调函数的请求完成之后，获取到响应，引擎会将该响应作为参数传递给这个回调函数。回调函数进行解析或生成下一个请求，回调函数如上文的parse()所示。</li><li>urljoin()：可以将相对URL构造成一个绝对的URL。例如，获取到的下一页地址是/page/2, urljoin()方法处理后得到的结果就是：<a href="http://quotes.toscrape.com/page/2/。" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/。</a></li><li>最后一行代码通过url和callback变量构造了一个新的请求，回调函数callback依然使用parse()方法。这个请求完成后，响应会重新经过parse方法处理，得到第二页的解析结果，然后生成第二页的下一页，也就是第三页的请求。这样爬虫就进入了一个循环，直到最后一页。</li></ul><h4 id="运行-1"><a href="#运行-1" class="headerlink" title="运行"></a>运行</h4><p><code>scrapy crawl quotes</code></p><h4 id="保存到文件"><a href="#保存到文件" class="headerlink" title="保存到文件"></a>保存到文件</h4><p>将爬取的结果保存成JSON文件，可以执行如下命令：<br><code>scrapy crawl quotes -o quotes.json</code><br>命令运行后，项目内多了一个quotes.json文件，文件包含了刚才抓取的所有内容。<br>另外还可以每一个Item输出一行JSON，输出后缀为jl，为jsonline的缩写，命令如下所示：<br><code>scrapy crawl quotes -o quotes.jl</code><br>或<br><code>scrapy crawl quotes -o quotes.jsonlines</code><br>输出格式支持多种，例如csv、xml、pickle、marshal等，还支持ftp、s3等远程输出，另外还可以通过自定义ItemExporter来实现其他的输出。<br>例如，下面命令对应的输出分别为csv、xml、pickle、marshal格式以及ftp远程输出：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.csv</span><br><span class="line">scrapy crawl quotes -o quotes.xml</span><br><span class="line">scrapy crawl quotes -o quotes.pickle</span><br><span class="line">scrapy crawl quotes -o quotes.marshal</span><br><span class="line">scrapy crawl quotes -o ftp://user:pass@ftp.example.com/path/to/quotes.csv</span><br></pre></td></tr></table></figure><p>其中，ftp输出需要正确配置用户名、密码、地址、输出路径，否则会报错。<br>通过Scrapy提供的Feed Export，可以轻松地输出抓取结果到文件。对于一些小型项目来说，这应该足够了。不过如果想要更复杂的输出，如输出到数据库等，我们可以使用Item Pileline来完成。</p><h4 id="使用Item-Pipeline"><a href="#使用Item-Pipeline" class="headerlink" title="使用Item Pipeline"></a>使用Item Pipeline</h4><ul><li><p>进行更复杂的操作，如将结果保存到MongoDB数据库，或者筛选某些有用的Item，则可以定义Item Pileline来实现。</p></li><li><p>Item Pipeline为项目管道。当Item生成后，它会自动被送到Item Pipeline进行处理，常用Item Pipeline来做如下操作：</p><ul><li>清理 HTML 数据 。</li><li>验证爬取数据，检查爬取字段。</li><li>查重井丢弃重复内容 。</li><li>将爬取结果保存到数据库 。</li></ul></li><li><p>要实现Item Pipeline，只需要定义一个类并实现process_item()方法即可。启用Item Pipeline后，Item Pipeline会自动调用这个方法。process_item()方法必须返回包含数据的字典或Item对象，或者抛出DropItem异常 。</p></li><li><p>process_item()方法有两个参数。一个参数是item，每次Spider生成的Item都会作为参数传递过来。另一个参数是spider，就是Spider的实例。</p></li><li><p>如下实现一个Item Pipeline，筛掉text长度大于50的Item，并将结果保存到MongoDB。修改项目里的pipelines.py文件，内容如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.limit = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'text'</span>]:</span><br><span class="line">            <span class="keyword">if</span> len(item[<span class="string">'text'</span>]) &gt; self.limit:</span><br><span class="line">                item[<span class="string">'text'</span>] = item[<span class="string">'text'</span>][<span class="number">0</span>:self.limit].rstrip() + <span class="string">'...'</span></span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> DropItem(<span class="string">'Missing Text'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        name = item.__class__.__name__</span><br><span class="line">        self.db[name].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure></li><li><p>MongoPipeline类实现了API定义的另外几个方法。</p><ul><li>from_crawler()。是一个类方法，用＠classmethod标识，是一种依赖注入的方式。它的参数就是crawler，通过crawler我们可以拿到全局配置的每个配置信息。在全局配置settings.py中，可以定义MONGO_URI和MONGO_DB来指定MongoDB连接需要的地址和数据库名称，拿到配置信息之后返回类对象即可。所以这个方法的定义主要是用来获取settings.py中的配置的。</li><li>open_spider()。当Spider开启时，这个方法被调用。上文程序中主要进行了一些初始化操作。</li><li>close_spider()。当Spider关闭时，这个方法会调用。上文程序中将数据库连接关闭。</li><li>process_item()方法则执行了数据插入操作 。</li></ul></li><li><p>settings.py中加入如下内容：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">  <span class="string">'tutorial.pipelines.TextPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">  <span class="string">'tutorial.pipelines.MongoPipeline'</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line">MONGO_URI=<span class="string">'localhost'</span></span><br><span class="line">MONGO_DB=<span class="string">'tutorial'</span></span><br></pre></td></tr></table></figure><p>赋值ITEM_PIPELINES字典，键名是Pipeline的类名称，键值是调用优先级，是一个数字，数字越小则对应的Pipeline越先被调用。爬取结束后， MongoDB中创建了一个tutorial的数据库、Quoteltem的表。</p></li><li><p>项目地址：<a href="https://github.com/Python3WebSpider/ScrapyTutorial" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyTutorial</a></p></li></ul><h3 id="Selector用法"><a href="#Selector用法" class="headerlink" title="Selector用法"></a>Selector用法</h3><p>Selector是基于lxml来构建的，支持XPath选择器、css选择器以及正则表达式，功能全面，解析速度和准确度非常高。</p><h4 id="直接使用"><a href="#直接使用" class="headerlink" title="直接使用"></a>直接使用</h4><p>Selector是一个可以独立使用的模块。可以直接利用Selector这个类来构建一个选择器对象，然后调用它的相关方法如xpath()、css()等来提取数据。<br>例如，针对一段HTML代码，可以用如下方式构建Selector对象来提取数据：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line">body= <span class="string">'&lt;html&gt;&lt;head&gt;&lt;title&gt;Hello World&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;'</span></span><br><span class="line">selector = Selector(text=body)</span><br><span class="line">title = selector.xpath(<span class="string">'//title/text()'</span>).extract_first()</span><br><span class="line">print(title)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">Hello World</span><br></pre></td></tr></table></figure><h4 id="Scrapy-shell"><a href="#Scrapy-shell" class="headerlink" title="Scrapy shell"></a>Scrapy shell</h4><p>这里借助Scrapy shell来模拟Scrapy请求的过程，讲解相关的提取方法。<br>用官方文档的一个样例页面来做演示：<a href="https://docs.scrapy.org/en/latest/topics/selectors.html。" target="_blank" rel="noopener">https://docs.scrapy.org/en/latest/topics/selectors.html。</a><br>开启Scrapy shell，在命令行输入如下命令：<br><code>scrapy shell https://docs.scrapy.org/en/latest/topics/selectors.html</code><br>就进入到Scrapy shell模式。这个过程是Scrapy发起了一次请求，请求的URL就是刚才命令行下输入的URL，然后把一些可操作的变量传递给我们，如request、response等，如图。<br><img src="https://s3.ax1x.com/2021/01/30/ykLL8g.png" alt="Scrapy shell"></p><p>我们可以在命令行模式下输入命令调用对象的一些操作方法，回车之后实时显示结果。这与Python的命令行交互模式是类似的。</p><h4 id="XPath选择器"><a href="#XPath选择器" class="headerlink" title="XPath选择器"></a>XPath选择器</h4><p>response有一个属性selector，调用response.selector返回的内容就相当于用response的body构造了一个Selector对象。通过这个Selector对象我们可以调用解析方法如xpath()、css()等，通过向方法传入XPath或css选择器参数就可以实现信息的提取。<br>实例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 返回类型是Selector组成的列表，即Selectorlist类型，</span></span><br><span class="line"><span class="comment"># SelectorList和Selector都可以继续调用xpath()和css()等方法来进一步提取数据。</span></span><br><span class="line">result = response.selector.xpath(<span class="string">'//a/text()'</span>)</span><br><span class="line"><span class="comment"># selector可以省略，等价于下面的写法</span></span><br><span class="line">response.xpath()</span><br><span class="line">response.css()</span><br><span class="line"><span class="comment"># 选择器的最前方加（点），这代表提取元素内部的数据，如果没有加点，则代表从根节点开始提取</span></span><br><span class="line">result.xpath(<span class="string">'./img/@href="img1"'</span>)</span><br><span class="line">result[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">response.xpath().extract()  <span class="comment"># 提取节点内的文本内容</span></span><br><span class="line">response.xpath().extract_first(<span class="string">'default param'</span>) <span class="comment"># 提取节点内的第一个文本内容,None则返回默认值</span></span><br></pre></td></tr></table></figure><h4 id="CSS选择器-1"><a href="#CSS选择器-1" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><p>用法类似XPath选择器,实例如下:</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">response.css(<span class="string">'a[href="img1.html"]::text'</span>).extract_first() <span class="comment"># 获取文本</span></span><br><span class="line">response.css(<span class="string">'a[href="img1.html"] img::attr(src)'</span>).extract_first() <span class="comment"># 获取属性</span></span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">'//a'</span>).css(<span class="string">'img'</span>).xpath(<span class="string">'@src'</span>).extract()  <span class="comment"># css选择器与XPath选择器完全兼容</span></span><br></pre></td></tr></table></figure><h4 id="正则匹配"><a href="#正则匹配" class="headerlink" title="正则匹配"></a>正则匹配</h4><p>实例:</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 返回匹配的分组列表</span></span><br><span class="line">response.css(<span class="string">'a[href="img1.html"]::text'</span>).re(<span class="string">'(.*?):\s(.*)'</span>)</span><br><span class="line"><span class="comment"># 返回第一个匹配的分组</span></span><br><span class="line">response.css(<span class="string">'a[href="img1.html"]::text'</span>).re_first(<span class="string">'(.*?):\s(.*)'</span>)</span><br></pre></td></tr></table></figure><h3 id="Spider用法"><a href="#Spider用法" class="headerlink" title="Spider用法"></a>Spider用法</h3><h4 id="Spider运行流程"><a href="#Spider运行流程" class="headerlink" title="Spider运行流程"></a>Spider运行流程</h4><ul><li>Spider要做的事就是如下两件：<ul><li>定义爬取网站的动作。</li><li>分析爬取下来的网页。</li></ul></li><li>对于Spider类来说，整个爬取循环过程如下所述。<ul><li>以初始的URL初始化Request，并设置回调函数。当该Request成功请求并返回时，Response生成井作为参数传给该回调函数。</li><li>在回调函数内分析返回的网页内容。返回结果有两种形式，一种是解析到的有效结果返回字典或Item对象，它们可以经过处理后（或直接）保存。另一种是解析得到下一个（如下一页）链接，可以利用此链接构造Request并设置新的回调函数，返回Request等待后续调度。</li><li>如果返回的是字典或Item对象，可通过Feed Exports等组件将退回结果存入到文件。如果设置了Pipeline的话，可以使用Pipeline处理（如过滤、修正等）并保存。</li><li>如果返回的是Reqeust，那么Request执行成功得到Response之后，Response会被传递给Request中定义的回调函数，在回调函数中我们可以再次使用选择器来分析新得到的网页内容，并根据分析的数据生成Item。<br>通过以上几步循环往复进行，即可完成站点的爬取。</li></ul></li></ul><h4 id="Spider类分析"><a href="#Spider类分析" class="headerlink" title="Spider类分析"></a>Spider类分析</h4><p>scrapy.spiders.Spider这个类提供了start requests()方法的默认实现，读取并请求start_urls属性，并根据返回的结果调用parse()方法解析结果。</p><ul><li>Scrapy有如下一些基础属性。<ul><li>name。爬虫名称，是定义Spider名字的字符串。Spider的名字定义了Scrapy如何定位并初始化Spider，必须是唯一的。不过可以生成多个相同的Spider实例，数量没有限制。</li><li>allowed_domains。允许爬取的域名，是可选配置，不在此范围的链接不会被跟进爬取。</li><li>start_urls。起始URL列表，当没有实现start_requests()方法时，默认会从这个列表开始抓取。</li><li>custom_settings。一个字典，是专属于本Spider的配置，此设置会覆盖项目全局的设置，必须在初始化前被更新，并定义成类变量。</li><li>crawler。由from_crawler()方法设置，代表的是本Spider类对应的Crawler对象。Crawler对象包含了很多项目组件，利用它可以获取项目的一些配置信息，如最常见的获取项目的设置信息，即Settings。</li><li>settings。是一个Settings对象，利用它可以直接获取项目的全局设置变量。</li></ul></li><li>除了基础属性，Spider还有一些常用的方法。<ul><li>start_requests()。此方法用于生成初始请求，必须返回一个可迭代对象。此方法会默认使用start_urls里面的URL来构造Request，而且Request是GET请求方式。如果想在启动时以POST方式访问某个站点，可以直接重写这个方法，发送POST请求时使用FormRequest即可。</li><li>parse()。当Response没有指定回调函数时，该方法会默认被调用。它负责处理Response，处理返回结果，并从中提取想要的数据和下一步的请求，然后返回。该方法需要返回一个包含Request或Item的可迭代对象。</li><li>closed()。当Spider关闭时，该方法会被调用，在这里－般会定义释放资源的一些操作或其他收尾操作。</li></ul></li></ul><h3 id="Downloader-Middleware用法"><a href="#Downloader-Middleware用法" class="headerlink" title="Downloader Middleware用法"></a>Downloader Middleware用法</h3><p>Downloader Middleware即下载中间件，是处于Scrapy的Request和Response之间的处理模块。用于实现修改User-Agent、处理重定向、设置代理、失败重试、设置Cookies等功能。</p><ul><li>在整个架构���起作用的位置是以下两个。<ul><li>在Scheduler调度出队列的Request发送给Doanloader下载之前，也就是在Request执行下载之前对其进行修改。</li><li>在下载后生成的Response发送给Spider之前，在生成Resposne被Spider解析之前对其进行修改。</li></ul></li></ul><h4 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h4><p>Scrapy已经提供了许多Downloader Middleware，如负责失败重试、向动重定向等功能的Middleware，它们被DOWNLOADER_MIDDLEWARES_BASE变量所定义。<br>DOWNLOADER_MIDDLEWARES_BASE变量的内容如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="string">'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'</span>: <span class="number">300</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'</span>: <span class="number">350</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>: <span class="number">400</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="number">500</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>: <span class="number">550</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware'</span>: <span class="number">560</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'</span>: <span class="number">580</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.httpcom pression.HttpCompressionMiddleware'</span>: <span class="number">590</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'</span>: <span class="number">600</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'</span>: <span class="number">700</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>: <span class="number">750</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.stats.DownloaderStats'</span>; <span class="number">850</span>,</span><br><span class="line"><span class="string">'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware'</span>: <span class="number">900</span>,</span><br></pre></td></tr></table></figure><p>这是一个字典格式，字典的键名是Scrapy内置的Downloader Middleware的名称，键值代表了调用的优先级，优先级是一个数字，数字越小代表越靠近 Scrapy引擎，数字越大代表越靠近Downloader，数字小的Downloader Middleware会被优先调用。<br>如果自己定义的Downloader Middleware要添加到项目里，DOWNLOADER_MIDDLEWARES_BASE变量不能直接修改。Scrapy提供了另外一个设置变量DOWNLOADER_MIDDLEWARES，直接修改这个变量就可以添加自己定义的Downloader Middleware，以及禁用DOWNLOADER_MIDDLEWARES_BASE里面定义的Downloader Middleware。</p><h4 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a>核心方法</h4><p>每个Downloader Middleware都定义了一个或多个方法的类，核心的方法有如下三个，我们只需要实现至少一个方法，就可以定义一个Downloader Middleware。</p><ul><li>process_request(request, spider)<br>Request被Scrapy引擎调度给Downloader之前，process_request()方法就会被调用，也就是在Request从队列里调度出来到Downloader下载执行之前，都可以用process_request()方法对Request进行处理。方法的返回值必须为None、Response对象、Request对象之一 ，或者抛出IgnoreRequest异常 。<ul><li>process_request()方法的参数有如下两个。<ul><li>request，Request对象，即被处理的Request。</li><li>spider，Spdier对象，即此Request对应的Spider。</li></ul></li><li>返回类型不同，产生的效果也不同。如下。<ul><li>当返回是None时，Scrapy将继续处理该Request，接着执行其他Downloader Middleware的process_request()方法，一直到Downloader把 Request执行后得到Response才结束。这个过程其实就是修改Request的过程，不同的Downloader Middleware按照设置的优先级顺序依次对 Request进行修改，最后送至Downloader执行。</li><li>当返回为Response对象时，更低优先级的Downloader Middleware的process_request()和process_exception()方法就不会被继续调用，每个 Downloader Middleware的process_response()方法转而被依次调用。调用完毕之后，直接将Response对象发送给Spider来处理。</li><li>当返回为Request对象时，更低优先级的Downloader Middleware的process_request()方法会停止执行。这个Request会重新放到调度队列里，其实它就是一个全新的Request，等待被调度。如果被Scheduler调度了，那么所有的Downloader Middleware的process request()方法会被重新按照顺序执行。</li><li>如果IgnoreRequest异常抛出，则所有的Downloader Middleware的process_exception()方法会依次执行。如果没有一个方法处理这个异常，那么Request的errorback()方法就会回调。如果该异常还没有被处理，那么它便会被忽略。</li></ul></li></ul></li><li>process_response(request, response, spider)<br>Downloader执行Request下载之后，会得到对应的Response。Scrapy引擎便会将Response发送给Spider进行解析。在发送之前，都可以用process_response()方法来对Response进行处理。方法的返回值必须为Request对象、Response对象之一，或者抛出IgnoreRequest异常。<ul><li>process_response()方法的参数有如下三个。<ul><li>request，Request对象，即此Response对应的Request。</li><li>response，Response对象，即此被处理的Response 。</li><li>spider，Spider对象，即此Response对应的Spider。</li></ul></li><li>下面为不同的返回情况。<ul><li>当返回为Request对象时，更低优先级的Downloader Middleware的process_response()方法不会继续调用。该Request对象会重新放到调度队列里等待被调度，它相当于一个全新的Request。然后，该Request会被process_request()方法顺次处理。</li><li>当返回为Response对象时，更低优先级的Downloader Middleware的process_response()方法会继续调用，继续对该Response对象进行处理。</li><li>如果IgnoreRequest异常抛出，则Request的errorback()方法会回调。如果该异常还没有被处理，那么它便会被忽略。</li></ul></li></ul></li><li>process_exception(request, exception, spider)<br>当Downloader或process_request()方法抛出异常时，例如抛出IgnoreRequest异常，process_exception()方法就会被调用。方法的返回值必须为 None、Response对象、Request对象之一。<ul><li>process_exception()方法的参数有如下三个。<ul><li>request，Request对象，即产生异常的Request。</li><li>exception，Exception对象，即抛出的异常。</li><li>spdier，Spider 对象，即Request对应的Spider。</li></ul></li><li>下面为不同的返回值。<ul><li>当返回为None时，更低优先级的Downloader Middleware的process_exception()会被继续顺次调用，直到所有的方法都被调度完毕。</li><li>当返回为Response对象时，更低优先级的Downloader Middleware的process_exception()方法不再被继续调用，每个Downloader Middleware的process_response()方法转而被依次调用。</li><li>当返回为Request对象时，更低优先级的Downloader Middleware的process_exception()也不再被继续调用，该Request对象会重新放到调度队列里面等待被调度，它相当于一个全新的Request。然后，该Request又会被process_request()方法顺次次处理。</li></ul></li></ul></li></ul><h4 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h4><ul><li><p>项目地址：<a href="https://github.com/Python3WebSpider/ScrapyDownloaderTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyDownloaderTest</a></p></li><li><p>新建项目：<code>scrapy startproject scrapydownloadertest</code></p></li><li><p>新建spider：<code>scrapy genspider httpbin httpbin.org</code></p></li><li><p>修改Spider内容如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># httpbin.py</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HttpbinSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'httpbin'</span></span><br><span class="line">    allowed_domains = [<span class="string">'httpbin.org'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://httpbin.org/get'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.debug(response.text)</span><br><span class="line">        self.logger.debug(<span class="string">'Status Code: '</span> + str(response.status))</span><br></pre></td></tr></table></figure></li><li><p>修改请求的User-Agent有两种方式：</p><ul><li>修改settings里的USER_AGENT变量，一般用此方法，如下添加一行USER_AGENT定义：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># settings.py</span></span><br><span class="line">USER_AGENT = (<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 '</span></span><br><span class="line">              <span class="string">'(KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</span>)</span><br></pre></td></tr></table></figure></li><li>借助Donwload Middleware，在middlewares.py文件添加一个自定义的RandomUserAgentMiddleware类，并在settings.py文件中添加<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># settings.py</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">  <span class="string">'scrapydownloadertest.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># middlewares.py</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddleware</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.user_agents = [</span><br><span class="line">            <span class="string">'Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)'</span>,</span><br><span class="line">            <span class="string">'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2'</span>,</span><br><span class="line">            <span class="string">'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:15.0) Gecko/20100101 Firefox/15.0.1'</span></span><br><span class="line">        ]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.headers[<span class="string">'User-Agent'</span>] = random.choice(self.user_agents)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span></span><br><span class="line">        response.status = <span class="number">201</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure></li></ul></li><li><p>执行<code>scrapy crawl httpbin</code>获取Scrapy发送的Request信息如下，成功修改User-Agent并返回201状态码：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[httpbin] DEBUG: Status Code: <span class="number">201</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept"</span>: <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"</span>,</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">    <span class="string">"Accept-Language"</span>: <span class="string">"en"</span>,</span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)"</span>,</span><br><span class="line">    <span class="string">"X-Amzn-Trace-Id"</span>: <span class="string">"Root=1-60190098-419bb41a68b4c46b3f2eb440"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"60.33.123.25"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"http://httpbin.org/get"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="Spider-Middleware用法"><a href="#Spider-Middleware用法" class="headerlink" title="Spider Middleware用法"></a>Spider Middleware用法</h3><p>Spider Middleware有如下三个作用。</p><ul><li>在Downloader生成的Response发送给Spider之前对Response进行处理。</li><li>在Spider生成的Request发送给Scheduler之前对Request进行处理。</li><li>在Spider生成的Item发送给Item Pipeline之前对Item进行处理。</li></ul><h4 id="使用说明-1"><a href="#使用说明-1" class="headerlink" title="使用说明"></a>使用说明</h4><p>Scrapy已经提供了许多Spider Middleware，它们被SPIDER_MIDDLEWARES_BASE这个变盘所定义。<br>SPIDER_MIDDLEWARES_BASE变量的内容如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="string">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'</span>: <span class="number">50</span>,</span><br><span class="line"><span class="string">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'</span>: <span class="number">500</span>,</span><br><span class="line"><span class="string">'scrapy.spidermiddlewares.referer.RefererMiddleware'</span>: <span class="number">700</span>,</span><br><span class="line"><span class="string">'scrapy.spidermiddlewares.urllength.UrllengthMiddleware'</span>: <span class="number">800</span>,</span><br><span class="line"><span class="string">'scrapy.spidermiddlewares.depth.DepthMiddleware'</span>: <span class="number">900</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和Downloader Middleware一样，Spider Middleware首先加入到SPIDER_MIDDLEWARES设置中，该设置会和Scrapy中SPIDER_MIDDLEWARES_BASE定义的Spider Middleware合并。然后根据键值的数字优先级排序，得到一个有序列表。第一个Middleware是���靠近引擎的，最后一个Middleware是最靠近 Spider的。</p><h4 id="核心方法-1"><a href="#核心方法-1" class="headerlink" title="核心方法"></a>核心方法</h4><p>每个Spider Middleware都定义了以下一个或多个方法的类，核心方法有如下4个，只需要实现其中一个方法就可以定义一个Spider Middleware。下面是这4个方法的详细用法。</p><ul><li>process_spider_input(response, spider)<br>当Response被Spider Middleware处理时，process_spider_input()方法被调用。<ul><li>process_spider_input()方法的参数有如下两个。<ul><li>response，Response对象，即被处理的Response。</li><li>spider，Spider对象，即该Response对应的Spider。</li></ul></li><li>process_spider_input()应该返回None或者抛出一个异常。<ul><li>如果它返回None，Scrapy将会继续处理该Response，调用所有其他的Spider Middleware，直到Spider处理该Response。</li><li>如果它抛出一个异常，Scrapy将不会调用任何其他Spider Middleware的process_spider_input()方法，而调用Request的errback()方法。errback()的输出将会被重新输入到中间件中，使用process_spider_output()方法来处理，当其抛出异常时则调用 process_spider_exception()来处理。</li></ul></li></ul></li><li>process_spider_output(response, result, spider)<br>当Spider处理Response返回结果时，process_spider_output()方法被调用。<ul><li>process_spider_output()方法的参数有如下三个。<ul><li>response，Response对象，即生成该输出的Response。</li><li>result，包含Request或Item对象的可迭代对象，即Spider返回的结果。</li><li>spider，Spider对象，即其结果对应的Spider。</li></ul></li><li>process_spider_output()必须返回包含Request或Item对象的可迭代对象。</li></ul></li><li>process_spider_exception(response, exception, spider)<br>当Spider或Spider Middleware的process_spider_input()方法抛出异常时，process_spider_exception()方法被调用。<ul><li>process_spider_exception()方法的参数有如下三个。<ul><li>response，Response对象，即异常被抛出时被处理的Response。</li><li>exception，Exception对象，即被抛出的异常。</li><li>spider，Spider对象，即抛出该异常的Spider。</li></ul></li><li>process_spider_exception()必须要么返回None，要么返回一个包含Response或Item对象的可迭代对象。<ul><li>如果返回None，Scrapy将继续处理该异常，调用其他Spider Middleware中的process_spider_exception()方法，直到所有Spider Middleware都被调用。</li><li>如果返回一个可迭代对象，则其他Spider Middleware的process_spider_output()方法被调用，其他的process_spider_exception()不会被调用。</li></ul></li></ul></li><li>process_start_requests(start_requests, spider)<br>process_start_requests()方法以Spider启动的Request为参数被调用，执行的过程类似于process_spider_output()，只不过没有相关联的 Response，并且必须返回Request。<ul><li>process_start_requests()方法的参数有如下两个。<ul><li>start_requests，包含Request的可迭代对象，即Start Requests。</li><li>spider， Spider对象，即Start Requests所属的Spider。</li></ul></li><li>process_start_requests()必须返回另一个包含Request对象的可迭代对象。</li></ul></li></ul><h3 id="Item-Pipeline用法"><a href="#Item-Pipeline用法" class="headerlink" title="Item Pipeline用法"></a>Item Pipeline用法</h3><ul><li>Item Pipeline的调用发生在Spider产生Item之后。当Spider解析完Response之后，Item就会传递到Item Pipeline，被定义的Item Pipeline组件会顺次调用，完成一连串的处理过程，比如数据清洗、存储等。</li><li>Item Pipeline的主要功能有如下。<ul><li>清理HTML数据。</li><li>验证爬取数据，检查爬取字段。</li><li>查重并丢弃重复内容。</li><li>将爬取结果保存到数据库。</li></ul></li></ul><h4 id="核心方法-2"><a href="#核心方法-2" class="headerlink" title="核心方法"></a>核心方法</h4><ul><li>process_item(item, spider)<br>process_item()是<strong>必须要实现的方法</strong>，被定义的Item Pipeline会默认调用这个方法对Item进行处理。比如进行数据处理或者将数据写入到数据库等操作。必须返回Item类型的值或者抛出一个Drop Item异常。<ul><li>process_item()方法的参数有如下两个。<ul><li>item，Item对象，即被处理的Item 。</li><li>spider，Spider对象，即生成该Item的Spider。</li></ul></li><li>process_item()方法的返回类型归纳如下。<ul><li>如果返回的是Item对象，那么此Item会被低优先级的Item Pipeline的process_item()方法处理，直到所有的方法被调用完毕。</li><li>如果它抛出的是Drop Item异常，那么此Item会被丢弃，不再进行处理。</li></ul></li></ul></li><li>open_spider(self, spider)<br>open_spider()方法是在Spider开启的时候被自动调用的。在这里可以做一些初始化操作，如开启数据库连接等。其中，参数spider就是被开启的 Spider对象。</li><li>close_spider(spider)<br>close_spider()方法是在Spider关闭的时候自动调用的。在这里可以做一些收尾工作，如关闭数据库连接等。其中，参数spider就是被关闭的Spider对象。</li><li>from_crawler(cls, crawler)<br>from_crawler()方法是一个类方法，用＠classmethod标识，是一种依赖注入的方式。它的参数是crawler，通过crawler对象，可以拿到Scrapy的所有核心组件，如全局配置的每个信息，然后创建一个Pipeline实例。参数cls就是Class，最后返回一个Class实例。</li></ul><h4 id="项目示例"><a href="#项目示例" class="headerlink" title="项目示例"></a>项目示例</h4><p>新建项目：<code>scrapy startproject images360</code><br>新建一个Spider：<code>scrapy genspider images images.so.com</code></p><ul><li><p>构造请求<br>打开浏览器开发者模式，过滤器切换到XHR选项，分析Ajax请求。其中两个请求如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">https://image.so.com/zjl?ch=beauty&amp;sn=30&amp;listtype=new&amp;temp=1</span><br><span class="line">https://image.so.com/zjl?ch=beauty&amp;sn=60&amp;listtype=new&amp;temp=1</span><br></pre></td></tr></table></figure><p>可知，sn为偏移值，sn为30，返回前30张图片；sn为60，返回第31-60张图片。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># images.py</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider, Request</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">from</span> images360.items <span class="keyword">import</span> ImageItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagesSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'images'</span></span><br><span class="line">    allowed_domains = [<span class="string">'images.so.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://images.so.com/'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        data = &#123;<span class="string">'ch'</span>: <span class="string">'beauty'</span>, <span class="string">'listtype'</span>: <span class="string">'new'</span>&#125;</span><br><span class="line">        base_url = <span class="string">'https://image.so.com/zjl?'</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, self.settings.get(<span class="string">'MAX_PAGE'</span>) + <span class="number">1</span>):</span><br><span class="line">            data[<span class="string">'sn'</span>] = page * <span class="number">30</span></span><br><span class="line">            params = urlencode(data)</span><br><span class="line">            url = base_url + params</span><br><span class="line">            <span class="keyword">yield</span> Request(url, self.parse)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        result = json.loads(response.text)</span><br><span class="line">        <span class="keyword">for</span> image <span class="keyword">in</span> result.get(<span class="string">'list'</span>):</span><br><span class="line">            item = ImageItem()</span><br><span class="line">            item[<span class="string">'id'</span>] = image.get(<span class="string">'id'</span>)</span><br><span class="line">            item[<span class="string">'url'</span>] = image.get(<span class="string">'qhimg_url'</span>)</span><br><span class="line">            item[<span class="string">'title'</span>] = image.get(<span class="string">'title'</span>)</span><br><span class="line">            item[<span class="string">'thumb'</span>] = image.get(<span class="string">'qhimg_thumb'</span>)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></li><li><p>提取信息<br>定义Item：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># items.py</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    collection = table = <span class="string">'images'</span> <span class="comment">#分别代表MongoDB和MySQL存储名称</span></span><br><span class="line">    id = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    title = Field()</span><br><span class="line">    thumb = Field()</span><br></pre></td></tr></table></figure></li><li><p>存储信息<br>需要提前建数据库和建表。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pipelines.py</span></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        name = item.collection</span><br><span class="line">        self.db[name].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host, database, user, password, port)</span>:</span></span><br><span class="line">        self.host = host</span><br><span class="line">        self.database = database</span><br><span class="line">        self.user = user</span><br><span class="line">        self.password = password</span><br><span class="line">        self.port = port</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            host=crawler.settings.get(<span class="string">'MYSQL_HOST'</span>),</span><br><span class="line">            database=crawler.settings.get(<span class="string">'MYSQL_DATABASE'</span>),</span><br><span class="line">            user=crawler.settings.get(<span class="string">'MYSQL_USER'</span>),</span><br><span class="line">            password=crawler.settings.get(<span class="string">'MYSQL_PASSWORD'</span>),</span><br><span class="line">            port=crawler.settings.get(<span class="string">'MYSQL_PORT'</span>),</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.db = pymysql.connect(host=self.host, user=self.user, password=self.password, db=self.database, charset=<span class="string">'utf8'</span>,</span><br><span class="line">                                port=self.port)</span><br><span class="line">        self.cursor = self.db.cursor()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.db.close()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        print(item[<span class="string">'title'</span>])</span><br><span class="line">        data = dict(item)</span><br><span class="line">        keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">        values = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">        sql = <span class="string">'insert into %s (%s) values (%s)'</span> % (item.table, keys, values)</span><br><span class="line">        self.cursor.execute(sql, tuple(data.values()))</span><br><span class="line">        self.db.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    负责下载图片</span></span><br><span class="line"><span class="string">    file_path()：返回保存的文件名</span></span><br><span class="line"><span class="string">    item_completed()：处理下载成功和失败的情况</span></span><br><span class="line"><span class="string">    get_media_requests()：将Item对象的url字段取出生成Request对象，加入调度队列</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">        url = request.url</span><br><span class="line">        file_name = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> file_name</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        image_paths = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">'Image Downloaded Failed'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(item[<span class="string">'url'</span>])</span><br></pre></td></tr></table></figure></li><li><p>运行<br>修改settings.py：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">BOT_NAME = <span class="string">'images360'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'images360.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'images360.spiders'</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意调用顺序</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'images360.pipelines.ImagePipeline'</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">'images360.pipelines.MysqlPipeline'</span>: <span class="number">302</span>,</span><br><span class="line">    <span class="string">'images360.pipelines.MongoPipeline'</span>: <span class="number">302</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">IMAGES_STORE = <span class="string">'./images'</span></span><br><span class="line"></span><br><span class="line">MAX_PAGE = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'images360'</span></span><br><span class="line"></span><br><span class="line">MYSQL_HOST = <span class="string">'localhost'</span></span><br><span class="line">MYSQL_DATABASE = <span class="string">'images360'</span></span><br><span class="line">MYSQL_USER = <span class="string">'root'</span></span><br><span class="line">MYSQL_PASSWORD = <span class="string">'123456'</span></span><br><span class="line">MYSQL_PORT = <span class="number">3306</span></span><br></pre></td></tr></table></figure><p>执行：<code>scrapy crawl images</code></p></li></ul><h3 id="Scrapy对接Selenium"><a href="#Scrapy对接Selenium" class="headerlink" title="Scrapy对接Selenium"></a>Scrapy对接Selenium</h3><p>Scrapy抓取页面的方式和requests库类似，都是直接模拟HTTP请求，而Scrapy也不能抓取JavaScript动态渲染的页面。<br>在前文中抓取JavaScript渲染的页面有两种方式。</p><ul><li>一种是分析Ajax请求，找到其对应的接口抓取，Scrapy同样可以用此种方式抓取。</li><li>一种是直接用Selenium或Splash模拟浏览器进行抓取，</li></ul><p>项目地址：<a href="https://github.com/Python3WebSpider/ScrapySeleniumTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapySeleniumTest</a><br>新建项目：<code>scrapy startproject scrapyseleniumtest</code><br>新建一个Spider：<code>scrapy genspider taobao www.taobao.com</code><br>修改ROBOTSTXT OBEY：<code>ROBOTSTXT OBEY = False</code></p><ul><li>定义Item<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    collection = <span class="string">'products'</span></span><br><span class="line">    image = Field()</span><br><span class="line">    price = Field()</span><br><span class="line">    deal = Field()</span><br><span class="line">    title = Field()</span><br><span class="line">    shop = Field()</span><br><span class="line">    location = Field()</span><br></pre></td></tr></table></figure></li><li>实现自定义的Spider类<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request, Spider</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">from</span> scrapyseleniumtest.items <span class="keyword">import</span> ProductItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TaobaoSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'taobao'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.taobao.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://s.taobao.com/search?q='</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.settings.get(<span class="string">'KEYWORDS'</span>):</span><br><span class="line">            <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, self.settings.get(<span class="string">'MAX_PAGE'</span>) + <span class="number">1</span>):</span><br><span class="line">                url = self.base_url + quote(keyword)</span><br><span class="line">                <span class="comment">#meta传递分页页码，dongt_filter不去重</span></span><br><span class="line">                <span class="keyword">yield</span> Request(url=url, callback=self.parse, meta=&#123;<span class="string">'page'</span>: page&#125;, dont_filter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        products = response.xpath(</span><br><span class="line">            <span class="string">'//div[@id="mainsrp-itemlist"]//div[@class="items"][1]//div[contains(@class, "item")]'</span>)</span><br><span class="line">        <span class="keyword">for</span> product <span class="keyword">in</span> products:</span><br><span class="line">            item = ProductItem()</span><br><span class="line">            item[<span class="string">'price'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(@class, "price")]//text()'</span>).extract()).strip()</span><br><span class="line">            item[<span class="string">'title'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(@class, "title")]//text()'</span>).extract()).strip()</span><br><span class="line">            item[<span class="string">'shop'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(@class, "shop")]//text()'</span>).extract()).strip()</span><br><span class="line">            item[<span class="string">'image'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[@class="pic"]//img[contains(@class, "img")]/@data-src'</span>).extract()).strip()</span><br><span class="line">            item[<span class="string">'deal'</span>] = product.xpath(<span class="string">'.//div[contains(@class, "deal-cnt")]//text()'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'location'</span>] = product.xpath(<span class="string">'.//div[contains(@class, "location")]//text()'</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></li><li>对接Selenium<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> logging <span class="keyword">import</span> getLogger</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeleniumMiddleware</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, timeout=None, service_args=[])</span>:</span></span><br><span class="line">        self.logger = getLogger(__name__)</span><br><span class="line">        self.timeout = timeout</span><br><span class="line">        self.browser = webdriver.PhantomJS(service_args=service_args)</span><br><span class="line">        self.browser.set_window_size(<span class="number">1400</span>, <span class="number">700</span>)</span><br><span class="line">        self.browser.set_page_load_timeout(self.timeout)</span><br><span class="line">        self.wait = WebDriverWait(self.browser, self.timeout)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.browser.close()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        用PhantomJS抓取页面</span></span><br><span class="line"><span class="string">        :param request: Request对象</span></span><br><span class="line"><span class="string">        :param spider: Spider对象</span></span><br><span class="line"><span class="string">        :return: HtmlResponse</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.logger.debug(<span class="string">'PhantomJS is Starting'</span>)</span><br><span class="line">        page = request.meta.get(<span class="string">'page'</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.browser.get(request.url)</span><br><span class="line">            <span class="keyword">if</span> page &gt; <span class="number">1</span>:</span><br><span class="line">                input = self.wait.until(</span><br><span class="line">                    EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager div.form &gt; input'</span>)))</span><br><span class="line">                submit = self.wait.until(</span><br><span class="line">                    EC.element_to_be_clickable((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager div.form &gt; span.btn.J_Submit'</span>)))</span><br><span class="line">                input.clear()</span><br><span class="line">                input.send_keys(page)</span><br><span class="line">                submit.click()</span><br><span class="line">            self.wait.until(</span><br><span class="line">                EC.text_to_be_present_in_element((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager li.item.active &gt; span'</span>), str(page)))</span><br><span class="line">            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'.m-itemlist .items .item'</span>)))</span><br><span class="line">            <span class="keyword">return</span> HtmlResponse(url=request.url, body=self.browser.page_source, request=request, encoding=<span class="string">'utf-8'</span>,</span><br><span class="line">                                status=<span class="number">200</span>)</span><br><span class="line">        <span class="keyword">except</span> TimeoutException:</span><br><span class="line">            <span class="keyword">return</span> HtmlResponse(url=request.url, status=<span class="number">500</span>, request=request)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(timeout=crawler.settings.get(<span class="string">'SELENIUM_TIMEOUT'</span>),</span><br><span class="line">                  service_args=crawler.settings.get(<span class="string">'PHANTOMJS_SERVICE_ARGS'</span>))</span><br></pre></td></tr></table></figure></li><li>存储结果<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>), mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[item.collection].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure></li><li>运行<br>修改settings.py文件：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">BOT_NAME = <span class="string">'scrapyseleniumtest'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'scrapyseleniumtest.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'scrapyseleniumtest.spiders'</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapyseleniumtest.middlewares.SeleniumMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'scrapyseleniumtest.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">KEYWORDS = [<span class="string">'iPad'</span>]</span><br><span class="line"></span><br><span class="line">MAX_PAGE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">SELENIUM_TIMEOUT = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">PHANTOMJS_SERVICE_ARGS = [<span class="string">'--load-images=false'</span>, <span class="string">'--disk-cache=true'</span>]</span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line"></span><br><span class="line">MONGO_DB = <span class="string">'taobao'</span></span><br></pre></td></tr></table></figure>运行：<code>scrapy crawl taobao</code></li></ul><h3 id="Scrapy对接Splash"><a href="#Scrapy对接Splash" class="headerlink" title="Scrapy对接Splash"></a>Scrapy对接Splash</h3><p><strong>注意</strong>：使用代理时添加代理的方式应相应更改<code>request.meta[&#39;splash&#39;] [&#39;args&#39;][&#39;proxy&#39;] = uri</code><br>通过实现Downloader Middlewar实现了Selenium的对接，但这种方式是阻塞式的，也就是说这样就破坏了Scrapy异步处理的逻辑，速度会受到影响。为了不破坏其异步加载逻辑，我们可以使用Splash实现。<br>项目地址：<a href="https://github.com/Python3WebSpider/ScrapySplashTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapySplashTest</a><br>安装：<code>pip install scrapy-splash</code><br>新建项目：<code>scrapy startproject scrapysplashtest</code><br>新建一个Spider：<code>scrapy genspider taobao www.taobao.com</code></p><h4 id="添加配置"><a href="#添加配置" class="headerlink" title="添加配置"></a>添加配置</h4><p>参考Scrapy-Splash的配置说明进行一步步的配置，链接如下：<a href="http://github.com/scrapyplugins/scrapy-splash#configuration" target="_blank" rel="noopener">http://github.com/scrapyplugins/scrapy-splash#configuration</a></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># settings.py</span></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line">    <span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'scrapysplashtest.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">KEYWORDS = [<span class="string">'iPad'</span>]</span><br><span class="line"></span><br><span class="line">MAX_PAGE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">SPLASH_URL = <span class="string">'http://localhost:8050'</span></span><br><span class="line"></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br><span class="line">HTTPCACHE_STORAGE = <span class="string">'scrapy_splash.SplashAwareFSCacheStorage'</span></span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'taobao'</span></span><br></pre></td></tr></table></figure><h4 id="自定义Spider"><a href="#自定义Spider" class="headerlink" title="自定义Spider"></a>自定义Spider</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider, Request</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">from</span> scrapysplashtest.items <span class="keyword">import</span> ProductItem</span><br><span class="line"><span class="keyword">from</span> scrapy_splash <span class="keyword">import</span> SplashRequest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个Lua脚本</span></span><br><span class="line">script = <span class="string">"""</span></span><br><span class="line"><span class="string">function main(splash, args)</span></span><br><span class="line"><span class="string">  splash.images_enabled = false</span></span><br><span class="line"><span class="string">  assert(splash:go(args.url))</span></span><br><span class="line"><span class="string">  assert(splash:wait(args.wait))</span></span><br><span class="line"><span class="string">  js = string.format("document.querySelector('#mainsrp-pager div.form &gt; input').value=%d;document.querySelector('#mainsrp-pager div.form &gt; span.btn.J_Submit').click()", args.page)</span></span><br><span class="line"><span class="string">  splash:evaljs(js)</span></span><br><span class="line"><span class="string">  assert(splash:wait(args.wait))</span></span><br><span class="line"><span class="string">  return splash:html()</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TaobaoSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'taobao'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.taobao.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://s.taobao.com/search?q='</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.settings.get(<span class="string">'KEYWORDS'</span>):</span><br><span class="line">            <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, self.settings.get(<span class="string">'MAX_PAGE'</span>) + <span class="number">1</span>):</span><br><span class="line">                url = self.base_url + quote(keyword)</span><br><span class="line">                <span class="comment"># 对接Splash，传入Lua脚本</span></span><br><span class="line">                <span class="keyword">yield</span> SplashRequest(url, callback=self.parse, endpoint=<span class="string">'execute'</span>,</span><br><span class="line">                                    args=&#123;<span class="string">'lua_source'</span>: script, <span class="string">'page'</span>: page, <span class="string">'wait'</span>: <span class="number">7</span>&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        products = response.xpath(</span><br><span class="line">            <span class="string">'//div[@id="mainsrp-itemlist"]//div[@class="items"][1]//div[contains(@class, "item")]'</span>)</span><br><span class="line">        <span class="keyword">for</span> product <span class="keyword">in</span> products:</span><br><span class="line">            item = ProductItem()</span><br><span class="line">            item[<span class="string">'price'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(@class, "price")]//text()'</span>).extract()).strip()</span><br><span class="line">            item[<span class="string">'title'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(@class, "title")]//text()'</span>).extract()).strip()</span><br><span class="line">            item[<span class="string">'shop'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(@class, "shop")]//text()'</span>).extract()).strip()</span><br><span class="line">            item[<span class="string">'image'</span>] = <span class="string">''</span>.join(</span><br><span class="line">                product.xpath(<span class="string">'.//div[@class="pic"]//img[contains(@class, "img")]/@data-src'</span>).extract()).strip()</span><br><span class="line">            item[<span class="string">'deal'</span>] = product.xpath(<span class="string">'.//div[contains(@class, "deal-cnt")]//text()'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'location'</span>] = product.xpath(<span class="string">'.//div[contains(@class, "location")]//text()'</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h4 id="运行-2"><a href="#运行-2" class="headerlink" title="运行"></a>运行</h4><p>其他同Selenium对接相同<br>运行：<code>scrapy crawl taobao</code></p><h3 id="Scrapy通用爬虫"><a href="#Scrapy通用爬虫" class="headerlink" title="Scrapy通用爬虫"></a>Scrapy通用爬虫</h3><p>代码地址为：<a href="https://github.com/Python3WebSpider/ScrapyUniversal" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyUniversal</a></p><h4 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h4><p>官方文档链接为：<a href="http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider" target="_blank" rel="noopener">http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider</a><br>源码：<a href="https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider" target="_blank" rel="noopener">https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider</a></p><p>CrawlSpider是Scrapy提供的一个通用Spider。在Spider里，可以指定一些爬取规则来实现页面的提取，这些爬取规则由一个专门的数据结构Rule表示。 Rule里包含提取和跟进页面的配置，Spider会根据Rule来确定当前页面中的哪些链接需要继续爬取、哪些页面的爬取结果需要用哪个方法解析等。</p><p>CrawlSpider继承自Spider类。除了Spider类的所有方法和属性，还提供了一个非常重要的属性和方法。</p><ul><li>rules，爬取规则属性，是包含一个或多个Rule对象的列表。每个Rule对爬取网站的动作都做了定义，CrawlSpider会读取rules的每一个Rule并进行解析。</li><li>parse_start_url()，是一个可重写的方法。当start_urls里对应的Request得到Response时，该方法被调用，它会分析Response并必须返回Item 对象或者Request对象。</li></ul><p>这里最重要的内容莫过于Rule的定义了，它的定义和参数如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">contrib</span>.<span class="title">spiders</span>.<span class="title">Rule</span><span class="params">(link_extractor, callback=None, cb_kwargs=None, </span></span></span><br><span class="line"><span class="class"><span class="params">                                    follow=None, process_links=None, process_request=None)</span></span></span><br></pre></td></tr></table></figure><p>下面依次说明Rule的参数。</p><ul><li><p>link_extractor：Link Extractor对象。通过它，Spider可以知道从爬取的页面中提取哪些链接。提取出的链接会自动生成Request。它又是一个数据结构，一般常用LxmlLinkExtractor对象作为参数，其定义和参数如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">linkextractors</span>.<span class="title">lxmlhtml</span>.<span class="title">LxmlLinkExtractor</span><span class="params">(allow=<span class="params">()</span>, deny=<span class="params">()</span>, allow_domains=<span class="params">()</span>,</span></span></span><br><span class="line"><span class="class"><span class="params"> deny_domains=<span class="params">()</span>, deny_extensions=None, restrict_xpaths=<span class="params">()</span>, restrict_css=<span class="params">()</span>, </span></span></span><br><span class="line"><span class="class"><span class="params"> tags=<span class="params">(<span class="string">'a'</span>, <span class="string">'area'</span>)</span>, attrs=<span class="params">(<span class="string">'href'</span>,)</span>, canonicalize=False, unique=True, </span></span></span><br><span class="line"><span class="class"><span class="params"> process_value=None, strip=True)</span></span></span><br></pre></td></tr></table></figure><p>allow 是一个正则表达式或正则表达式列表，它定义了从当前页面提取出的链接哪些是符合要求的，只有符合要求的链接才会被跟进。<br>deny 则相反。<br>allow_domains 定义了符合要求的域名，只有此域名的链接才会被跟进生成新的 Request，它相当于域名白名单。<br>deny_domains 则相反，相当于域名黑名单。<br>restrict_xpaths 定义了从当前页面中 XPath 匹配的区域提取链接，其值是 XPath 表达式或 XPath 表达式列表。<br>restrict_css 定义了从当前页面中 CSS 选择器匹配的区域提取链接，其值是 CSS 选择器或 CSS 选择器列表。<br>还有一些其他参数代表了提取链接的标签、是否去重、链接的处理等内容，使用的频率不高。可以参考文档的参数说明：<br><a href="http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml。" target="_blank" rel="noopener">http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml。</a></p></li><li><p>callback，即回调函数，和之前定义 Request 的 callback 有相同的意义。每次从 link_extractor 中获取到链接时，该函数将会调用。该回调函数接收一个 response 作为其第一个参数，并返回一个包含 Item 或 Request 对象的列表。注意，避免使用 parse () 作为回调函数。由于 CrawlSpider 使用 parse () 方法来实现其逻辑，如果 parse () 方法覆盖了，CrawlSpider 将会运行失败。</p></li><li><p>cb_kwargs，字典，它包含传递给回调函数的参数。</p></li><li><p>follow，布尔值，即 True 或 False，它指定根据该规则从 response 提取的链接是否需要跟进。如果 callback 参数为 None，follow 默认设置为 True，否则默认为 False。</p></li><li><p>process_links，指定处理函数，从 link_extractor 中获取到链接列表时，该函数将会调用，它主要用于过滤。</p></li><li><p>process_request，同样是指定处理函数，根据该 Rule 提取到每个 Request 时，该函数都会调用，对 Request 进行处理。该函数必须返回 Request 或者 None。</p></li></ul><p>以上内容便是 CrawlSpider 中的核心 Rule 的基本用法。但这些内容可能还不足以完成一个 CrawlSpider 爬虫。下面我们利用 CrawlSpider 实现新闻网站的爬取实例，来更好地理解 Rule 的用法。</p><h4 id="Item-Loader"><a href="#Item-Loader" class="headerlink" title="Item Loader"></a>Item Loader</h4><p>我们了解了利用 CrawlSpider 的 Rule 来定义页面的爬取逻辑，这是可配置化的一部分内容。但是，Rule 并没有对 Item 的提取方式做规则定义。对于 Item 的提取，我们需要借助另一个模块 Item Loader 来实现。</p><p>Item Loader 提供一种便捷的机制来帮助我们方便地提取 Item。它提供的一系列 API 可以分析原始数据对 Item 进行赋值。Item 提供的是保存抓取数据的容器，而 Item Loader 提供的是填充容器的机制。有了它，数据的提取会变得更加规则化。 Item Loader 的 API 如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">loader</span>.<span class="title">ItemLoader</span><span class="params">([item, selector, response,] **kwargs)</span></span></span><br></pre></td></tr></table></figure><p>Item Loader 的 API 返回一个新的 Item Loader 来填充给定的 Item。如果没有给出 Item，则使用 default_item_class 中的类自动实例化。另外，它传入 selector 和 response 参数来使用选择器或响应参数实例化。 下面将依次说明 Item Loader 的 API 参数。</p><ul><li>item，Item 对象，可以调用 add_xpath ()、add_css () 或 add_value () 等方法来填充 Item 对象。</li><li>selector，Selector 对象，用来提取填充数据的选择器。</li><li>response，Response 对象，用于使用构造选择器的 Response。</li></ul><p>一个比较典型的 Item Loader 实例如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> project.items <span class="keyword">import</span> Product</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    loader = ItemLoader(item=Product(), response=response)</span><br><span class="line">    loader.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_name"]'</span>)</span><br><span class="line">    loader.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_title"]'</span>)</span><br><span class="line">    loader.add_xpath(<span class="string">'price'</span>, <span class="string">'//p[@id="price"]'</span>)</span><br><span class="line">    loader.add_css(<span class="string">'stock'</span>, <span class="string">'p#stock]'</span>)</span><br><span class="line">    loader.add_value(<span class="string">'last_updated'</span>, <span class="string">'today'</span>)</span><br><span class="line">    <span class="keyword">return</span> loader.load_item()</span><br></pre></td></tr></table></figure><p>这里首先声明一个 Product Item，用该 Item 和 Response 对象实例化 ItemLoader，调用 add_xpath () 方法把来自两个不同位置的数据提取出来，分配给 name 属性，再用 add_xpath ()、add_css ()、add_value () 等方法对不同属性依次赋值，最后调用 load_item () 方法实现 Item 的解析。</p><p>这种方式比较规则化，我们可以把一些参数和规则单独提取出来做成配置文件或存到数据库，即可实现可配置化。</p><p>另外，Item Loader 每个字段中都包含了一个 Input Processor（输入处理器）和一个 Output Processor（输出处理器）。Input Processor 收到数据时立刻提取数据，Input Processor 的结果被收集起来并且保存在 ItemLoader 内，但是不分配给 Item。收集到所有的数据后，load_item () 方法被调用来填充再生成 Item 对象。在调用时会先调用 Output Processor 来处理之前收集到的数据，然后再存入 Item 中，这样就生成了 Item。</p><p>下面将介绍一些内置的 Processor。</p><ul><li>Identity<br>Identity 是最简单的 Processor，不进行任何处理，直接返回原来的数据。</li><li>TakeFirst<br>TakeFirst 返回列表的第一个非空值，类似 extract_first () 的功能，常用作 Output Processor，如下所示：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst</span><br><span class="line"></span><br><span class="line">processor = TakeFirst()</span><br><span class="line">print(processor([<span class="string">''</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>经过此 Processor 处理后的结果返回了第一个不为空的值。</li><li>Join<br>Join 方法相当于字符串的 join () 方法，可以把列表拼合成字符串，字符串默认使用空格分隔，如下所示：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> Join</span><br><span class="line"></span><br><span class="line">processor = Join(<span class="string">','</span>)</span><br><span class="line">print(processor([<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]))</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">one,two,three</span><br></pre></td></tr></table></figure></li><li>Compose<br>Compose 是用给定的多个函数的组合而构造的 Processor，每个输入值被传递到第一个函数，其输出再传递到第二个函数，依次类推，直到最后一个函数返回整个处理器的输出，如下所示：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> Compose</span><br><span class="line"></span><br><span class="line">processor = Compose(str.upper, <span class="keyword">lambda</span> s: s.strip())</span><br><span class="line">print(processor(<span class="string">' hello world'</span>))</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">HELLO WORLD</span><br></pre></td></tr></table></figure>在这里我们构造了一个 Compose Processor，传入一个开头带有空格的字符串。Compose Processor 的参数有两个：第一个是 str.upper，它可以将字母全部转为大写；第二个是一个匿名函数，它调用 strip () 方法去除头尾空白字符。Compose 会顺次调用两个参数，最后返回结果的字符串全部转化为大���并且去除了开头的空格。</li><li>MapCompose<br>与 Compose 类似，MapCompose 可以迭代处理一个列表输入值，如下所示：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> MapCompose</span><br><span class="line"></span><br><span class="line">processor = MapCompose(str.upper, <span class="keyword">lambda</span> s: s.strip())</span><br><span class="line">print(processor([<span class="string">'Hello'</span>, <span class="string">'World'</span>, <span class="string">'Python'</span>]))</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">[<span class="string">'HELLO'</span>, <span class="string">'WORLD'</span>, <span class="string">'PYTHON'</span>]</span><br></pre></td></tr></table></figure></li><li>SelectJmes<br>SelectJmes 可以查询 JSON，传入 Key，返回查询所得的 Value。不过需要先安装 jmespath 库才可以使用它，命令如下所示：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pip3 install jmespath</span><br></pre></td></tr></table></figure>使用这个 Processor ，如下所示：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> SelectJmes</span><br><span class="line"></span><br><span class="line">proc = SelectJmes(<span class="string">'foo'</span>)</span><br><span class="line">processor = SelectJmes(<span class="string">'foo'</span>)</span><br><span class="line">print(processor(&#123;<span class="string">'foo'</span>: <span class="string">'bar'</span>&#125;))</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">bar</span><br></pre></td></tr></table></figure></li></ul><h4 id="新建项目-1"><a href="#新建项目-1" class="headerlink" title="新建项目"></a>新建项目</h4><p>首先新建一个 Scrapy 项目，名为 scrapyuniversal，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapy startproject scrapyuniversal</span><br></pre></td></tr></table></figure><p>创建一个 CrawlSpider，需要先制定一个模板。我们可以先看看有哪些可用模板，命令如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapy genspider -l</span><br></pre></td></tr></table></figure><p>运行结果如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">Available templates:</span><br><span class="line">  basic</span><br><span class="line">  crawl</span><br><span class="line">  csvfeed</span><br><span class="line">  xmlfeed</span><br></pre></td></tr></table></figure><p>之前创建 Spider 的时候，我们默认使用了第一个模板 basic。这次要创建 CrawlSpider，就需要使用第二个模板 crawl，创建命令如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapy genspider -t crawl china tech.china.com</span><br></pre></td></tr></table></figure><p>运行之后便会生成一个 CrawlSpider，其内容如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChinaSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'china'</span></span><br><span class="line">    allowed_domains = [<span class="string">'tech.china.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://tech.china.com/'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (Rule(LinkExtractor(allow=<span class="string">r'Items/'</span>), callback=<span class="string">'parse_item'</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        <span class="comment">#i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()</span></span><br><span class="line">        <span class="comment">#i['name'] = response.xpath('//div[@id="name"]').extract()</span></span><br><span class="line">        <span class="comment">#i['description'] = response.xpath('//div[@id="description"]').extract()</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>这次生成的 Spider 内容多了一个 rules 属性的定义。Rule 的第一个参数是 LinkExtractor，就是上文所说的 LxmlLinkExtractor，只是名称不同。同时，默认的回调函数也不再是 parse，而是 parse_item。</p><h4 id="定义-Rule"><a href="#定义-Rule" class="headerlink" title="定义 Rule"></a>定义 Rule</h4><p>要实现新闻的爬取，我们需要做的就是定义好 Rule，然后实现解析函数。下面我们就来一步步实现这个过程。 首先将 start_urls 修改为起始链接，代码如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">start_urls = [<span class="string">'http://tech.china.com/articles/'</span>]</span><br></pre></td></tr></table></figure><p>之后，Spider 爬取 start_urls 里面的每一个链接。所以这里第一个爬取的页面就是我们刚才所定义的链接。得到 Response 之后，Spider 就会根据每一个 Rule 来提取这个页面内的超链接，去生成进一步的 Request。接下来，我们就需要定义 Rule 来指定提取哪些链接。构造出一个 Rule ，代码如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">Rule(LinkExtractor(allow=<span class="string">'article/.*.html'</span>, </span><br><span class="line">  restrict_xpaths=<span class="string">'//div[@id="left_side"]//div[@class="con_item"]'</span>), callback=<span class="string">'parse_item'</span>)</span><br></pre></td></tr></table></figure><p>接下来，我们还要让当前页面实现分页功能，所以还需要提取下一页的链接。</p><p>下一页节点和其他分页链接区分度不高，要取出此链接我们可以直接用 XPath 的文本匹配方式，所以这里我们直接用 LinkExtractor 的 restrict_xpaths 属性来指定提取的链接即可。另外，我们不需要像新闻详情页一样去提取此分页链接对应的页面详情信息，也就是不需要生成 Item，所以不需要加 callback 参数。另外这下一页的页面如果请求成功了就需要继续像上述情况一样分析，所以它还需要加一个 follow 参数为 True，代表继续跟进匹配分析。其实，follow 参数也可以不加，因为当 callback 为空的时候，follow 默认为 True。此处 Rule 定义为如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">Rule(LinkExtractor(restrict_xpaths=<span class="string">'//div[@id="pageStyle"]//a[contains(., "下一页")]'</span>))</span><br></pre></td></tr></table></figure><p>所以现在 rules 就变成了：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">rules = (Rule(LinkExtractor(allow=<span class="string">'article/.*.html'</span>, </span><br><span class="line">  restrict_xpaths=<span class="string">'//div[@id="left_side"]//div[@class="con_item"]'</span>), callback=<span class="string">'parse_item'</span>),</span><br><span class="line">  Rule(LinkExtractor(restrict_xpaths=<span class="string">'//div[@id="pageStyle"]//a[contains(., "下一页")]'</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>现在已经实现页面的翻页和详情页的抓取了。</p><h4 id="解析页面"><a href="#解析页面" class="headerlink" title="解析页面"></a>解析页面</h4><p>首先定义一个 Item，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Field, Item</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NewsItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    title = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    text = Field()</span><br><span class="line">    datetime = Field()</span><br><span class="line">    source = Field()</span><br><span class="line">    website = Field()</span><br></pre></td></tr></table></figure><p>其中站点名称直接赋值为中华网。因为既然是通用爬虫，肯定还有很多爬虫也来爬取同样结构的其他站点的新闻内容，所以需要一个字段来区分一下站点名称。</p><p>像之前一样提取内容，就直接调用 response 变量的 xpath ()、css () 等方法即可。这里 parse_item () 方法的实现如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    item = NewsItem()</span><br><span class="line">    item[<span class="string">'title'</span>] = response.xpath(<span class="string">'//h1[@id="chan_newsTitle"]/text()'</span>).extract_first()</span><br><span class="line">    item[<span class="string">'url'</span>] = response.url</span><br><span class="line">    item[<span class="string">'text'</span>] = <span class="string">''</span>.join(response.xpath(<span class="string">'//div[@id="chan_newsDetail"]//text()'</span>).extract()).strip()</span><br><span class="line">    item[<span class="string">'datetime'</span>] = response.xpath(<span class="string">'//div[@id="chan_newsInfo"]/text()'</span>).re_first(<span class="string">'(d+-d+-d+sd+:d+:d+)'</span>)</span><br><span class="line">    item[<span class="string">'source'</span>] = response.xpath(<span class="string">'//div[@id="chan_newsInfo"]/text()'</span>).re_first(<span class="string">' 来源：(.*)'</span>).strip()</span><br><span class="line">    item[<span class="string">'website'</span>] = <span class="string">' 中华网 '</span></span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><p>用 Item Loader，通过 add_xpath ()、add_css ()、add_value () 等方式实现配置化提取。我们可以改写 parse_item ()，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    loader = ChinaLoader(item=NewsItem(), response=response)</span><br><span class="line">    loader.add_xpath(<span class="string">'title'</span>, <span class="string">'//h1[@id="chan_newsTitle"]/text()'</span>)</span><br><span class="line">    loader.add_value(<span class="string">'url'</span>, response.url)</span><br><span class="line">    loader.add_xpath(<span class="string">'text'</span>, <span class="string">'//div[@id="chan_newsDetail"]//text()'</span>)</span><br><span class="line">    loader.add_xpath(<span class="string">'datetime'</span>, <span class="string">'//div[@id="chan_newsInfo"]/text()'</span>, re=<span class="string">'(d+-d+-d+sd+:d+:d+)'</span>)</span><br><span class="line">    loader.add_xpath(<span class="string">'source'</span>, <span class="string">'//div[@id="chan_newsInfo"]/text()'</span>, re=<span class="string">' 来源：(.*)'</span>)</span><br><span class="line">    loader.add_value(<span class="string">'website'</span>, <span class="string">' 中华网 '</span>)</span><br><span class="line">    <span class="keyword">yield</span> loader.load_item()</span><br></pre></td></tr></table></figure><p>这里我们定义了一个 ItemLoader 的子类，名为 ChinaLoader，其实现如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst, Join, Compose</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NewsLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChinaLoader</span><span class="params">(NewsLoader)</span>:</span></span><br><span class="line">    text_out = Compose(Join(), <span class="keyword">lambda</span> s: s.strip())</span><br><span class="line">    source_out = Compose(Join(), <span class="keyword">lambda</span> s: s.strip())</span><br></pre></td></tr></table></figure><p>ChinaLoader 继承了 NewsLoader 类，其内定义了一个通用的 Out Processor 为 TakeFirst，这相当于之前所定义的 extract_first () 方法的功能。我们在 ChinaLoader 中定义了 text_out 和 source_out 字段。这里使用了一个 Compose Processor，它有两个参数：第一个参数 Join 也是一个 Processor，它可以把列表拼合成一个字符串；第二个参数是一个匿名函数，可以将字符串的头尾空白字符去掉。经过这一系列处理之后，我们就将列表形式的提取结果转化为去除头尾空白字符的字符串。 代码重新运行，提取效果是完全一样的。 至此，我们已经实现了爬虫的半通用化配置。</p><h4 id="通用配置抽取"><a href="#通用配置抽取" class="headerlink" title="通用配置抽取"></a>通用配置抽取</h4><p>所有的变量都可以抽取，如 name、allowed_domains、start_urls、rules 等。这些变量在 CrawlSpider 初始化的时候赋值即可。我们就可以新建一个通用的 Spider 来实现这个功能，命令如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapy genspider -t crawl universal universal</span><br></pre></td></tr></table></figure><p>接下来，我们将刚才所写的 Spider 内的属性抽离出来配置成一个 JSON，命名为 china.json，放到 configs 文件夹内，和 spiders 文件夹并列，代码如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"spider"</span>: <span class="string">"universal"</span>,</span><br><span class="line">  <span class="string">"website"</span>: <span class="string">"中华网科技"</span>,</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"新闻"</span>,</span><br><span class="line">  <span class="string">"index"</span>: <span class="string">"http://tech.china.com/"</span>,</span><br><span class="line">  <span class="string">"settings"</span>: &#123;<span class="string">"USER_AGENT"</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"start_urls"</span>: [<span class="string">"http://tech.china.com/articles/"</span>],</span><br><span class="line">  <span class="string">"allowed_domains"</span>: [<span class="string">"tech.china.com"</span>],</span><br><span class="line">  <span class="string">"rules"</span>: <span class="string">"china"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一个字段 spider 即 Spider 的名称，在这里是 universal。后面是站点的描述，比如站点名称、类型、首页等。随后的 settings 是该 Spider 特有的 settings 配置，如果要覆盖全局项目，settings.py 内的配置可以单独为其配置。随后是 Spider 的一些属性，如 start_urls、allowed_domains、rules 等。rules 也可以单独定义成一个 rules.py 文件，做成配置文件，实现 Rule 的分离，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> Rule</span><br><span class="line"></span><br><span class="line">rules = &#123;</span><br><span class="line">    <span class="string">'china'</span>: (Rule(LinkExtractor(allow=<span class="string">'article/.*.html'</span>, restrict_xpaths=<span class="string">'//div[@id="left_side"]//div[@class="con_item"]'</span>),</span><br><span class="line">             callback=<span class="string">'parse_item'</span>),</span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths=<span class="string">'//div[@id="pageStyle"]//a[contains(., "下一页")]'</span>))</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样我们将基本的配置抽取出来。如果要启动爬虫，只需要从该配置文件中读取然后动态加载到 Spider 中即可。所以我们需要定义一个读取该 JSON 文件的方法，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> realpath, dirname</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(name)</span>:</span></span><br><span class="line">    path = dirname(realpath(__file__)) + <span class="string">'/configs/'</span> + name + <span class="string">'.json'</span></span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> json.loads(f.read())</span><br></pre></td></tr></table></figure><p>定义了 get_config () 方法之后，我们只需要向其传入 JSON 配置文件的名称即可获取此 JSON 配置信息。随后我们定义入口文件 run.py，把它放在项目根目录下，它的作用是启动 Spider，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.spiders.universal <span class="keyword">import</span> UniversalSpider</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.utils <span class="keyword">import</span> get_config</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerProcess</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></span><br><span class="line">    name = sys.argv[<span class="number">1</span>]</span><br><span class="line">    custom_settings = get_config(name)</span><br><span class="line">    <span class="comment"># 爬取使用的 Spider 名称</span></span><br><span class="line">    spider = custom_settings.get(<span class="string">'spider'</span>, <span class="string">'universal'</span>)</span><br><span class="line">    project_settings = get_project_settings()</span><br><span class="line">    settings = dict(project_settings.copy())</span><br><span class="line">    <span class="comment"># 合并配置</span></span><br><span class="line">    settings.update(custom_settings.get(<span class="string">'settings'</span>))</span><br><span class="line">    process = CrawlerProcess(settings)</span><br><span class="line">    <span class="comment"># 启动爬虫</span></span><br><span class="line">    process.crawl(spider, **&#123;<span class="string">'name'</span>: name&#125;)</span><br><span class="line">    process.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure><p>运行入口为 run ()。首先获取命令行的参数并赋值为 name，name 就是 JSON 文件的名称，其实就是要爬取的目标网站的名称。我们首先利用 get_config () 方法，传入该名称读取刚才定义的配置文件。获取爬取使用的 spider 的名称、配置文件中的 settings 配置，然后将获取到的 settings 配置和项目全局的 settings 配置做了合并。新建一个 CrawlerProcess，传入爬取使用的配置。调用 crawl () 和 start () 方法即可启动爬取。 在 universal 中，我们新建一个 init() 方法，进行初始化配置，实现如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.utils <span class="keyword">import</span> get_config</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.rules <span class="keyword">import</span> rules</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UniversalSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'universal'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, *args, **kwargs)</span>:</span></span><br><span class="line">        config = get_config(name)</span><br><span class="line">        self.config = config</span><br><span class="line">        self.rules = rules.get(config.get(<span class="string">'rules'</span>))</span><br><span class="line">        self.start_urls = config.get(<span class="string">'start_urls'</span>)</span><br><span class="line">        self.allowed_domains = config.get(<span class="string">'allowed_domains'</span>)</span><br><span class="line">        super(UniversalSpider, self).__init__(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre></td></tr></table></figure><p>在 init() 方法中，start_urls、allowed_domains、rules 等属性被赋值。其中，rules 属性另外读取了 rules.py 的配置，这样就成功实现爬虫的基础配置。 接下来，执行如下命令运行爬虫：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">python run.py china</span><br></pre></td></tr></table></figure><p>解析部分同样需要实现可配置化，原来的解析函数如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    loader = ChinaLoader(item=NewsItem(), response=response)</span><br><span class="line">    loader.add_xpath(<span class="string">'title'</span>, <span class="string">'//h1[@id="chan_newsTitle"]/text()'</span>)</span><br><span class="line">    loader.add_value(<span class="string">'url'</span>, response.url)</span><br><span class="line">    loader.add_xpath(<span class="string">'text'</span>, <span class="string">'//div[@id="chan_newsDetail"]//text()'</span>)</span><br><span class="line">    loader.add_xpath(<span class="string">'datetime'</span>, <span class="string">'//div[@id="chan_newsInfo"]/text()'</span>, re=<span class="string">'(d+-d+-d+sd+:d+:d+)'</span>)</span><br><span class="line">    loader.add_xpath(<span class="string">'source'</span>, <span class="string">'//div[@id="chan_newsInfo"]/text()'</span>, re=<span class="string">' 来源：(.*)'</span>)</span><br><span class="line">    loader.add_value(<span class="string">'website'</span>, <span class="string">' 中华网 '</span>)</span><br><span class="line">    <span class="keyword">yield</span> loader.load_item()</span><br></pre></td></tr></table></figure><p>我们需要将这些配置也抽离出来。这里的变量主要有 Item Loader 类的选用、Item 类的选用、Item Loader 方法参数的定义，我们可以在 JSON 文件中添加如下 item 的配置：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="string">"item"</span>: &#123;</span><br><span class="line">  <span class="string">"class"</span>: <span class="string">"NewsItem"</span>,</span><br><span class="line">  <span class="string">"loader"</span>: <span class="string">"ChinaLoader"</span>,</span><br><span class="line">  <span class="string">"attrs"</span>: &#123;</span><br><span class="line">    <span class="string">"title"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//h1[@id='chan_newsTitle']/text()"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"url"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"attr"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"url"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"text"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsDetail']//text()"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"datetime"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsInfo']/text()"</span>],</span><br><span class="line">        <span class="string">"re"</span>: <span class="string">"(\\d+-\\d+-\\d+\\s\\d+:\\d+:\\d+)"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"source"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsInfo']/text()"</span>],</span><br><span class="line">        <span class="string">"re"</span>: <span class="string">"来源：(.*)"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"website"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"value"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"中华网"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里定义了 class 和 loader 属性，它们分别代表 Item 和 Item Loader 所使用的类。定义了 attrs 属性来定义每个字段的提取规则，例如，title 定义的每一项都包含一个 method 属性，它代表使用的提取方法，如 xpath 即代表调用 Item Loader 的 add_xpath () 方法。args 即参数，就是 add_xpath () 的第二个参数，即 XPath 表达式。针对 datetime 字段，我们还用了一次正则提取，所以这里还可以定义一个 re 参数来传递提取时所使用的正则表达式。 我们还要将这些配置之后动态加载到 parse_item () 方法里。最后，最重要的就是实现 parse_item () 方法，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">   item = self.config.get(<span class="string">'item'</span>)</span><br><span class="line">   <span class="keyword">if</span> item:</span><br><span class="line">       cls = eval(item.get(<span class="string">'class'</span>))()</span><br><span class="line">       loader = eval(item.get(<span class="string">'loader'</span>))(cls, response=response)</span><br><span class="line">       <span class="comment"># 动态获取属性配置</span></span><br><span class="line">       <span class="keyword">for</span> key, value <span class="keyword">in</span> item.get(<span class="string">'attrs'</span>).items():</span><br><span class="line">           <span class="keyword">for</span> extractor <span class="keyword">in</span> value:</span><br><span class="line">               <span class="keyword">if</span> extractor.get(<span class="string">'method'</span>) == <span class="string">'xpath'</span>:</span><br><span class="line">                   loader.add_xpath(key, *extractor.get(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.get(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.get(<span class="string">'method'</span>) == <span class="string">'css'</span>:</span><br><span class="line">                   loader.add_css(key, *extractor.get(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.get(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.get(<span class="string">'method'</span>) == <span class="string">'value'</span>:</span><br><span class="line">                   loader.add_value(key, *extractor.get(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.get(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.get(<span class="string">'method'</span>) == <span class="string">'attr'</span>:</span><br><span class="line">                   loader.add_value(key, getattr(response, *extractor.get(<span class="string">'args'</span>)))</span><br><span class="line">       <span class="keyword">yield</span> loader.load_item()</span><br></pre></td></tr></table></figure><p>再回过头看一下 start_urls 的配置。这里 start_urls 只可以配置具体的链接。如果这些链接有 100 个、1000 个，我们总不能将所有的链接全部列出来吧？在某些情况下，start_urls 也需要动态配置。我们将 start_urls 分成两种，一种是直接配置 URL 列表，一种是调用方法生成，它们分别定义为 static 和 dynamic 类型。 本例中的 start_urls 很明显是 static 类型的，所以 start_urls 配置改写如下所示：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"start_urls"</span>: &#123;</span><br><span class="line">  <span class="string">"type”: "</span>static<span class="string">",</span></span><br><span class="line"><span class="string">  "</span>value<span class="string">": ["</span>http://tech.china.com/articles/<span class="string">"] </span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>如果 start_urls 是动态生成的，我们可以调用方法传参数，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="string">"start_urls"</span>: &#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"dynamic"</span>,</span><br><span class="line">  <span class="string">"method"</span>: <span class="string">"china"</span>,</span><br><span class="line">  <span class="string">"args"</span>: [<span class="number">5</span>, <span class="number">10</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里 start_urls 定义为 dynamic 类型，指定方法为 urls_china ()，然后传入参数 5 和 10，来生成第 5 到 10 页的链接。这样我们只需要实现该方法即可，统一新建一个 urls.py 文件，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">china</span><span class="params">(start, end)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(start, end + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">yield</span> <span class="string">'http://tech.china.com/articles/index_'</span> + str(page) + <span class="string">'.html'</span></span><br></pre></td></tr></table></figure><p>接下来在 Spider 的 init() 方法中，start_urls 的配置改写如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapyuniversal <span class="keyword">import</span> urls</span><br><span class="line"></span><br><span class="line">start_urls = config.get(<span class="string">'start_urls'</span>)</span><br><span class="line"><span class="keyword">if</span> start_urls:</span><br><span class="line">    <span class="keyword">if</span> start_urls.get(<span class="string">'type'</span>) == <span class="string">'static'</span>:</span><br><span class="line">        self.start_urls = start_urls.get(<span class="string">'value'</span>)</span><br><span class="line">    <span class="keyword">elif</span> start_urls.get(<span class="string">'type'</span>) == <span class="string">'dynamic'</span>:</span><br><span class="line">        self.start_urls = list(eval(<span class="string">'urls.'</span> + start_urls.get(<span class="string">'method'</span>))(*start_urls.get(<span class="string">'args'</span>, [])))</span><br></pre></td></tr></table></figure><p>综上所述，整个项目的配置包括如下内容。</p><ul><li>spider，指定所使用的 Spider 的名称。</li><li>settings，可以专门为 Spider 定制配置信息，会覆盖项目级别的配置。</li><li>start_urls，指定爬虫爬取的起始链接。</li><li>allowed_domains，允许爬取的站点。</li><li>rules，站点的爬取规则。</li><li>item，数据的提取规则。</li></ul><p>我们实现了 Scrapy 的通用爬虫，每个站点只需要修改 JSON 文件即可实现自由配置。</p><h3 id="Scrapyrt的使用"><a href="#Scrapyrt的使用" class="headerlink" title="Scrapyrt的使用"></a>Scrapyrt的使用</h3><p>Scrapyrt 为 Scrapy 提供了一个调度的 HTTP 接口。不需要执行 Scrapy 命令，而是通过请求一个 HTTP 接口即可调度 Scrapy 任务。<br>安装：<code>pip install scrapyrt</code><br>启动服务：<code>scrapyrt [-p 9080]</code></p><h4 id="GET-请求"><a href="#GET-请求" class="headerlink" title="GET 请求"></a>GET 请求</h4><p>目前，GET 请求方式支持如下的参数。</p><ul><li>spider_name，Spider 名称，字符串类型，必传参数，如果传递的 Spider 名称不存在则会返回 404 错误。</li><li>url，爬取链接，字符串类型，如果起始链接没有定义的话就必须要传递，如果传递了该参数，Scrapy 会直接用该 URL 生成 Request，而直接忽略 start_requests () 方法和 start_urls 属性的定义。</li><li>callback，回调函数名称，字符串类型，可选参数，如果传递了就会使用此回调函数处理，否则会默认使用 Spider 内定义的回调函数。</li><li>max_requests，最大请求数量，数值类型，可选参数，它定义了 Scrapy 执行请求的 Request 的最大限制，如定义为 5，则最多只执行 5 次 Request 请求，其余的则会被忽略。</li><li>start_requests，是否要执行 start_request () 函数，布尔类型，可选参数，在 Scrapy 项目中如果定义了 start_requests () 方法，那么在项目启动时会默认调用该方法，但是在 Scrapyrt 就不一样了，它默认不执行 start_requests () 方法，如果要执行，需要将它设置为 true。</li></ul><p>示例：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">curl http://localhost:9080/crawl.json?spider_name=quotes&amp;url=http://quotes.toscrape.com/</span><br></pre></td></tr></table></figure><p>输出结果返回的是一个 JSON 格式的字符串，解析它的结构，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"status"</span>: <span class="string">"ok"</span>,</span><br><span class="line">  <span class="string">"items"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"text"</span>: <span class="string">"“The world as we have created it is a process of o..."</span>,</span><br><span class="line">      <span class="string">"author"</span>: <span class="string">"Albert Einstein"</span>,</span><br><span class="line">      <span class="string">"tags"</span>: [</span><br><span class="line">        <span class="string">"change"</span>,</span><br><span class="line">        <span class="string">"deep-thoughts"</span>,</span><br><span class="line">        <span class="string">"thinking"</span>,</span><br><span class="line">        <span class="string">"world"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    ...</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"text"</span>: <span class="string">"“... a mind needs books as a sword needs a whetsto..."</span>,</span><br><span class="line">      <span class="string">"author"</span>: <span class="string">"George R.R. Martin"</span>,</span><br><span class="line">      <span class="string">"tags"</span>: [</span><br><span class="line">        <span class="string">"books"</span>,</span><br><span class="line">        <span class="string">"mind"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"items_dropped"</span>: [],</span><br><span class="line">  <span class="string">"stats"</span>: &#123;</span><br><span class="line">    <span class="string">"downloader/request_bytes"</span>: <span class="number">2892</span>,</span><br><span class="line">    <span class="string">"downloader/request_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="string">"downloader/request_method_count/GET"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="string">"downloader/response_bytes"</span>: <span class="number">24812</span>,</span><br><span class="line">    <span class="string">"downloader/response_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="string">"downloader/response_status_count/200"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"downloader/response_status_count/404"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"dupefilter/filtered"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"finish_reason"</span>: <span class="string">"finished"</span>,</span><br><span class="line">    <span class="string">"finish_time"</span>: <span class="string">"2017-07-12 15:09:02"</span>,</span><br><span class="line">    <span class="string">"item_scraped_count"</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">"log_count/DEBUG"</span>: <span class="number">112</span>,</span><br><span class="line">    <span class="string">"log_count/INFO"</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">"memusage/max"</span>: <span class="number">52510720</span>,</span><br><span class="line">    <span class="string">"memusage/startup"</span>: <span class="number">52510720</span>,</span><br><span class="line">    <span class="string">"request_depth_max"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"response_received_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="string">"scheduler/dequeued"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"scheduler/dequeued/memory"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"scheduler/enqueued"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"scheduler/enqueued/memory"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"start_time"</span>: <span class="string">"2017-07-12 15:08:56"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"spider_name"</span>: <span class="string">"quotes"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里省略了 items 绝大部分。status 显示了爬取的状态，items 部分是 Scrapy 项目的爬取结果，items_dropped 是被忽略的 Item 列表，stats 是爬取结果的统计情况。此结果和直接运行 Scrapy 项目得到的统计是相同的。 这样一来，我们就通过 HTTP 接口调度 Scrapy 项目并获取爬取结果，如果 Scrapy 项目部署在服务器上，我们可以通过开启一个 Scrapyrt 服务实现任务的调度并直接取到爬取结果，这很方便。</p><h4 id="POST-请求"><a href="#POST-请求" class="headerlink" title="POST 请求"></a>POST 请求</h4><p>此处 Request Body 必须是一个合法的 JSON 配置，在 JSON 里面可以配置相应的参数，支持的配置参数更多。<br>目前，JSON 配置支持如下参数。</p><ul><li>spider_name：Spider 名称，字符串类型，必传参数。如果传递的 Spider 名称不存在，则返回 404 错误。</li><li>max_requests：最大请求数量，数值类型，可选参数。它定义了 Scrapy 执行请求的 Request 的最大限制，如定义为 5，则表示最多只执行 5 次 Request 请求，其余的则会被忽略。</li><li>request：Request 配置，JSON 对象，必传参数。通过该参数可以定义 Request 的各个参数，必须指定 url 字段来指定爬取链接，其他字段可选。<br>一个 JSON 配置实例，如下所示：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"request"</span>: &#123;</span><br><span class="line">        <span class="string">"url"</span>: <span class="string">"http://quotes.toscrape.com/"</span>,</span><br><span class="line">        <span class="string">"callback"</span>: <span class="string">"parse"</span>,</span><br><span class="line">        <span class="string">"dont_filter"</span>: <span class="string">"True"</span>,</span><br><span class="line">        <span class="string">"cookies"</span>: &#123;<span class="string">"foo"</span>: <span class="string">"bar"</span>&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"max_requests"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"spider_name"</span>: <span class="string">"quotes"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>执行如下命令传递该 Json 配置并发起 POST 请求：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">curl http://localhost:<span class="number">9080</span>/crawl.json -d <span class="string">'&#123;"request": &#123;"url": "http://quotes.toscrape.com/", "dont_filter": "True", "callback": "parse", "cookies": &#123;"foo": "bar"&#125;&#125;, "max_requests": 2, "spider_name": "quotes"&#125;'</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="Scrapy对接Docker"><a href="#Scrapy对接Docker" class="headerlink" title="Scrapy对接Docker"></a>Scrapy对接Docker</h3><p>把前文 Scrapy 的入门项目打包成一个 Docker 镜像。项目爬取的网址为：<a href="http://quotes.toscrape.com/，" target="_blank" rel="noopener">http://quotes.toscrape.com/，</a><br>本章 Scrapy 入门一节已经实现了 Scrapy 对此站点的爬取过程，项目代码为：<a href="https://github.com/Python3WebSpider/ScrapyTutorial。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyTutorial。</a></p><h4 id="创建-Dockerfile"><a href="#创建-Dockerfile" class="headerlink" title="创建 Dockerfile"></a>创建 Dockerfile</h4><p>首先在项目的根目录下新建一个 <code>requirements.txt</code> 文件，将整个项目依赖的 Python 环境包都列出来，可以执行如下命令生成该文件：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成本地全部依赖，适用于虚拟环境</span></span><br><span class="line">pip freeze &gt;requirements.txt</span><br></pre></td></tr></table></figure><p>或使用pipreqs生成<code>requirements.txt</code>：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pip install pipreqs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只生成该目录内的依赖</span></span><br><span class="line">pipreqs ./ --encoding=utf8</span><br></pre></td></tr></table></figure><p>在项目根目录下新建一个 Dockerfile 文件，文件不加任何后缀名，修改内容如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">FROM python:<span class="number">3.7</span></span><br><span class="line">ENV PATH /usr/local/bin:$PATH</span><br><span class="line">ADD . /code</span><br><span class="line">WORKDIR /code</span><br><span class="line">RUN pip3 install -r requirements.txt</span><br><span class="line">CMD scrapy crawl quotes</span><br></pre></td></tr></table></figure><ul><li>第一行的 FROM 代表使用的 Docker 基础镜像，在这里直接使用 python:3.7 的镜像，在此基础上运行 Scrapy 项目。</li><li>第二行 ENV 是环境变量设置，将 /usr/local/bin:$PATH 赋值给 PATH，即增加 /usr/local/bin 这个环境变量路径。</li><li>第三行 ADD 是将本地的代码放置到虚拟容器中。它有两个参数：第一个参数是.，代表本地当前路径；第二个参数是 /code，代表虚拟容器中的路径，也就是将本地项目所有内容放置到虚拟容器的 /code 目录下，以便于在虚拟容器中运行代码。</li><li>第四行 WORKDIR 是指定工作目录，这里将刚才添加的代码路径设成工作路径。这个路径下的目录结构和当前本地目录结构是相同的，所以我们可以直接执行库安装命令、爬虫运行命令等。</li><li>第五行 RUN 是执行某些命令来做一些环境准备工作。由于 Docker 虚拟容器内只有 Python 3 环境，而没有所需要的 Python 库，所以运行此命令来在虚拟容器中安装相应的 Python 库如 Scrapy，这样就可以在虚拟容器中执行 Scrapy 命令了。</li><li>第六行 CMD 是容器启动命令。在容器运行时，此命令会被执行。在这里我们直接用 scrapy crawl quotes 来启动爬虫。</li></ul><h4 id="修改-MongoDB-连接"><a href="#修改-MongoDB-连接" class="headerlink" title="修改 MongoDB 连接"></a>修改 MongoDB 连接</h4><p>如果继续用 localhost 是无法找到 MongoDB 的，因为在 Docker 虚拟容器里 localhost 实际指向容器本身的运行 IP，而容器内部并没有安装 MongoDB，所以爬虫无法连接 MongoDB。 这里的 MongoDB 地址可以有如下两种选择。</p><ul><li>如果只想在本机测试，我们可以将地址修改为宿主机的 IP，也就是容器外部的本机 IP，一般是一个局域网 IP，使用 ifconfig 命令即可查看。</li><li>如果要部署到远程主机运行，一般 MongoDB 都是可公网访问的地址，修改为此地址即可。</li></ul><p>在本节中，我们的目标是将项目打包成一个镜像，让其他远程主机也可运行这个项目。所以我们直接将此处 MongoDB 地址修改为某个公网可访问的远程数据库地址，修改 MONGO_URI 如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">MONGO_URI = <span class="string">'mongodb://admin:admin123@120.27.34.25:27017'</span></span><br></pre></td></tr></table></figure><h4 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h4><p>执行如下命令构建镜像：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker build -t quotes:latest .</span><br></pre></td></tr></table></figure><p>查看一下构建的镜像：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure><h4 id="运行-3"><a href="#运行-3" class="headerlink" title="运行"></a>运行</h4><p>在本地测试运行，执行如下命令：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker run quotes</span><br></pre></td></tr></table></figure><h4 id="推送至-Docker-Hub"><a href="#推送至-Docker-Hub" class="headerlink" title="推送至 Docker Hub"></a>推送至 Docker Hub</h4><p>构建完成之后，我们可以将镜像 Push 到 Docker 镜像托管平台，如 Docker Hub 或者私有的 Docker Registry 等，这样我们就可以从远程服务器下拉镜像并运行了。<br>以 Docker Hub 为例，如果项目包含一些私有的连接信息（如数据库），我们最好将 Repository 设为私有或者直接放到私有的 Docker Registry。<br>首先在 <a href="https://hub.docker.com" target="_blank" rel="noopener">https://hub.docker.com</a> 注册一个账号，新建一个 Repository，名为 quotes。比如，我的用户名为 germey，新建的 Repository 名为 quotes，那么此 Repository 的地址就可以用 germey/quotes 来表示。 为新建的镜像打一个标签，命令如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker tag quotes:latest germey/quotes:latest</span><br></pre></td></tr></table></figure><p>推送镜像到 Docker Hub 即可，命令如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker push germey/quotes</span><br></pre></td></tr></table></figure><p>如果想在其他的主机上运行这个镜像，主机上装好 Docker 后，可以直接执行如下命令：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker run germey/quotes</span><br></pre></td></tr></table></figure><h3 id="Scrapy爬取新浪微博"><a href="#Scrapy爬取新浪微博" class="headerlink" title="Scrapy爬取新浪微博"></a>Scrapy爬取新浪微博</h3><p>60845<br>scrapy crawl btheavens<br>scrapy crawl pianku</p><h2 id="Ch-13-分布式爬虫"><a href="#Ch-13-分布式爬虫" class="headerlink" title="Ch 13 分布式爬虫"></a>Ch 13 分布式爬虫</h2><h3 id="分布式爬虫原理"><a href="#分布式爬虫原理" class="headerlink" title="分布式爬虫原理"></a>分布式爬虫原理</h3><h4 id="分布式爬虫架构"><a href="#分布式爬虫架构" class="headerlink" title="分布式爬虫架构"></a>分布式爬虫架构</h4><p>Scheduler 可以扩展多个，Downloader 也可以扩展多个。而爬取队列 Queue 必须始终为一个，也就是所谓的共享爬取队列。这样才能保证 Scheduer 从队列里调度某个 Request 之后，其他 Scheduler 不会重复调度此 Request，就可以做到多个 Schduler 同步爬取。这就是分布式爬虫的基本雏形，简单调度架构如图所示。<br><img src="https://gitee.com/xxyrs/filehouse/raw/master/Pictures/20210208-150715-0510.png" alt=""></p><h4 id="维护爬取队列"><a href="#维护爬取队列" class="headerlink" title="维护爬取队列"></a>维护爬取队列</h4><p>那么这个队列用什么维护来好呢？首先需要考虑的就是性能问题，什么数据库存取效率高？自然想到基于内存存储的 Redis，而且 Redis 还支持多种数据结构，例如列表 List、集合 Set、有序集合 Sorted Set 等等，存取的操作也非常简单，所以在这里采用 Redis 来维护爬取队列。 这几种数据结构存储实际各有千秋，分析如下：</p><ul><li>列表数据结构有 lpush ()、lpop ()、rpush ()、rpop () 方法，所以我们可以用它来实现一个先进先出式爬取队列，也可以实现一个先进后出栈式爬取队列。</li><li>集合的元素是无序的且不重复的，这样我们可以非常方便地实现一个随机排序的不重复的爬取队列。</li><li>有序集合带有分数表示，而 Scrapy 的 Request 也有优先级的控制，所以用有集合我们可以实现一个带优先级调度的队列。</li></ul><p>这些不同的队列我们需要根据具体爬虫的需求灵活选择。</p><h4 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h4><p>Scrapy 有自动去重，它的去重使用了 Python 中的集合。这个集合记录了 Scrapy 中每个 Request 的指纹，这个指纹实际上就是 Request 的散列值。我们可以看看 Scrapy 的源代码，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_fingerprint</span><span class="params">(request, include_headers=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> include_headers:</span><br><span class="line">        include_headers = tuple(to_bytes(h.lower())</span><br><span class="line">                                 <span class="keyword">for</span> h <span class="keyword">in</span> sorted(include_headers))</span><br><span class="line">    cache = _fingerprint_cache.setdefault(request, &#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> include_headers <span class="keyword">not</span> <span class="keyword">in</span> cache:</span><br><span class="line">        fp = hashlib.sha1()</span><br><span class="line">        fp.update(to_bytes(request.method))</span><br><span class="line">        fp.update(to_bytes(canonicalize_url(request.url)))</span><br><span class="line">        fp.update(request.body <span class="keyword">or</span> <span class="string">b''</span>)</span><br><span class="line">        <span class="keyword">if</span> include_headers:</span><br><span class="line">            <span class="keyword">for</span> hdr <span class="keyword">in</span> include_headers:</span><br><span class="line">                <span class="keyword">if</span> hdr <span class="keyword">in</span> request.headers:</span><br><span class="line">                    fp.update(hdr)</span><br><span class="line">                    <span class="keyword">for</span> v <span class="keyword">in</span> request.headers.getlist(hdr):</span><br><span class="line">                        fp.update(v)</span><br><span class="line">        cache[include_headers] = fp.hexdigest()</span><br><span class="line">    <span class="keyword">return</span> cache[include_headers]</span><br></pre></td></tr></table></figure><p><code>request_fingerprint ()</code> 就是计算 Request 指纹的方法，其方法内部使用的是 <code>hashlib</code> 的 <code>sha1 ()</code> 方法。计算的字段包括 Request 的 Method、URL、Body、Headers 这几部分内容，这里只要有一点不同，那么计算的结果就不同。计算得到的结果是加密后的字符串，也就是指纹。每个 Request 都有独有的指纹，指纹就是一个字符串，判定字符串是否重复比判定 Request 对象是否重复容易得多，所以指纹可以作为判定 Request 是否重复的依据。 那么我们如何判定重复呢？Scrapy 是这样实现的，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.fingerprints = set()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></span><br><span class="line">    fp = self.request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> fp <span class="keyword">in</span> self.fingerprints:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    self.fingerprints.add(fp)</span><br></pre></td></tr></table></figure><p>在去重的类 RFPDupeFilter 中，有一个 <code>request_seen ()</code> 方法，这个方法有一个参数 request，它的作用就是检测该 Request 对象是否重复。这个方法调用 <code>request_fingerprint ()</code> 获取该 Request 的指纹，检测这个指纹是否存在于 fingerprints 变量中，而 fingerprints 是一个集合，集合的元素都是不重复的。如果指纹存在，那么就返回 True，说明该 Request 是重复的，否则这个指纹加入到集合中。如果下次还有相同的 Request 传递过来，指纹也是相同的，那么这时指纹就已经存在于集合中，Request 对象就会直接判定为重复。这样去重的目的就实现了。<br><strong>Scrapy 的去重过程就是，利用集合元素的不重复特性来实现 Request 的去重</strong>。 对于分布式爬虫来说，我们肯定不能再用每个爬虫各自的集合来去重了。因为这样还是每个主机单独维护自己的集合，不能做到共享。多台主机如果生成了相同的 Request，只能各自去重，各个主机之间就无法做到去重了。 那么要实现去重，这个指纹集合也需要是共享的，Redis 正好有集合的存储数据结构，我们可以<strong>利用 Redis 的集合作为指纹集合</strong>，那么这样去重集合也是利用 Redis 共享的。每台主机新生成 Request 之后，把该 Request 的指纹与集合比对，如果指纹已经存在，说明该 Request 是重复的，否则将 Request 的指纹加入到这个集合中即可。利用同样的原理不同的存储结构我们也实现了分布式 Reqeust 的去重。</p><h4 id="防止中断"><a href="#防止中断" class="headerlink" title="防止中断"></a>防止中断</h4><p>在 Scrapy 中，爬虫运行时的 Request 队列放在内存中。爬虫运行中断后，这个队列的空间就被释放，此队列就被销毁了。所以一旦爬虫运行中断，爬虫再次运行就相当于全新的爬取过程。<br>要做到中断后继续爬取，我们可以将队列中的 Request 保存起来，下次爬取直接读取保存数据即可获取上次爬取的队列。我们在 Scrapy 中指定一个爬取队列的存储路径即可，这个路径使用 JOB_DIR 变量来标识，我们可以用如下命令来实现：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapy crawl spider -s JOBDIR=crawls/spider</span><br></pre></td></tr></table></figure><p>更加详细的使用方法可以参见官方文档，链接为：<a href="https://doc.scrapy.org/en/latest/topics/jobs.html。" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/topics/jobs.html。</a> 在 Scrapy 中，我们实际是把爬取队列保存到本地，第二次爬取直接读取并恢复队列即可。那么在分布式架构中我们还用担心这个问题吗？不需要。因为爬取队列本身就是用数据库保存的，如果爬虫中断了，数据库中的 Request 依然是存在的，下次启动就会接着上次中断的地方继续爬取。 所以，当 Redis 的队列为空时，爬虫会重新爬取；当 Redis 的队列不为空时，爬虫便会接着上次中断之处继续爬取。</p><h3 id="Scrapy-Redis源码解析"><a href="#Scrapy-Redis源码解析" class="headerlink" title="Scrapy-Redis源码解析"></a>Scrapy-Redis源码解析</h3><p>Scrapy-Redis 库已经为我们提供了 Scrapy 分布式的队列、调度器、去重等功能，其 GitHub 地址为：<a href="https://github.com/rmax/scrapy-redis。" target="_blank" rel="noopener">https://github.com/rmax/scrapy-redis。</a></p><h4 id="获取源码"><a href="#获取源码" class="headerlink" title="获取源码"></a>获取源码</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/rmax/scrapy-redis.git</span><br></pre></td></tr></table></figure><p>核心源码在 scrapy-redis/src/scrapy_redis 目录下。</p><h4 id="爬取队列"><a href="#爬取队列" class="headerlink" title="爬取队列"></a>爬取队列</h4><p>源码文件为 queue.py，它有三个队列的实现，首先它实现了一个父类 Base，提供一些基本方法和属性，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider base queue class"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, spider, key, serializer=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> serializer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            serializer = picklecompat</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(serializer, <span class="string">'loads'</span>):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"serializer does not implement 'loads' function: % r"</span></span><br><span class="line">                            % serializer)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(serializer, <span class="string">'dumps'</span>):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"serializer '% s' does not implement 'dumps' function: % r"</span></span><br><span class="line">                            % serializer)</span><br><span class="line">        self.server = server</span><br><span class="line">        self.spider = spider</span><br><span class="line">        self.key = key % &#123;<span class="string">'spider'</span>: spider.name&#125;</span><br><span class="line">        self.serializer = serializer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_encode_request</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        obj = request_to_dict(request, self.spider)</span><br><span class="line">        <span class="keyword">return</span> self.serializer.dumps(obj)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_decode_request</span><span class="params">(self, encoded_request)</span>:</span></span><br><span class="line">        obj = self.serializer.loads(encoded_request)</span><br><span class="line">        <span class="keyword">return</span> request_from_dict(obj, self.spider)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Clear queue/stack"""</span></span><br><span class="line">        self.server.delete(self.key)</span><br></pre></td></tr></table></figure><p>首先看一下 <code>_encode_request ()</code> 和 <code>_decode_request ()</code> 方法，因为我们需要把一个 Request 对象存储到数据库中，但数据库无法直接存储对象，所以需要将 Request 序列化转成字符串再存储，而这两个方法就分别是序列化和反序列化的操作。</p><p>利用 pickle 库来实现，一般在调用 <code>push ()</code> 将 Request 存入数据库时会调用 <code>_encode_request ()</code> 方法进行序列化，在调用 <code>pop ()</code> 取出 Request 的时候会调用 <code>_decode_request ()</code> 进行反序列化。</p><p>在父类中 <code>__len()__</code>、<code>push ()</code> 和 <code>pop ()</code> 方法都是未实现的，会直接抛出 <code>NotImplementedError</code>，因此这个类是不能直接被使用的，所以必须要实现一个子类来重写这三个方法，而不同的子类就会有不同的实现，也就有着不同的功能。 那么接下来就需要定义一些子类来继承 Base 类，并重写这几个方法，那在源码中就有三个子类的实现，它们分别是 FifoQueue、PriorityQueue、LifoQueue，我们分别来看下它们的实现原理。</p><p>首先是 <code>FifoQueue</code>：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FifoQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider FIFO queue"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.llen(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        self.server.lpush(self.key, self._encode_request(request))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">if</span> timeout &gt; <span class="number">0</span>:</span><br><span class="line">            data = self.server.brpop(self.key, timeout)</span><br><span class="line">            <span class="keyword">if</span> isinstance(data, tuple):</span><br><span class="line">                data = data[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = self.server.rpop(self.key)</span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(data)</span><br></pre></td></tr></table></figure><p>这个类继承了 Base 类，并重写了 <code>len()</code>、<code>push ()</code>、<code>pop ()</code> 这三个方法，在这三个方法中都是对 server 对象的操作，而 server 对象就是一个 Redis 连接对象，我们可以直接调用其操作 Redis 的方法对数据库进行操作，可以看到这里的操作方法有 <code>llen ()</code>、<code>lpush ()</code>、<code>rpop ()</code> 等，代表此爬取队列是使用的 Redis 的列表，序列化后的 Request 会被存入列表中，就是列表的其中一个元素，</p><p><code>len()</code> 方法是获取列表的长度，<code>push ()</code> 方法中调用了 <code>lpush ()</code> 操作，这代表从列表左侧存入数据，<code>pop ()</code> 方法中调用了 <code>rpop ()</code> 操作，这代表从列表右侧取出数据。 所以 Request 在列表中的存取顺序是左侧进、右侧出，所以这是有序的进出，即先进先出，英文叫做 First Input First Output，也被简称作 Fifo，而此类的名称就叫做 FifoQueue。</p><p>另外还有一个与之相反的实现类，叫做 <code>LifoQueue</code>，实现如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LifoQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider LIFO queue."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the stack"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.llen(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        self.server.lpush(self.key, self._encode_request(request))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">if</span> timeout &gt; <span class="number">0</span>:</span><br><span class="line">            data = self.server.blpop(self.key, timeout)</span><br><span class="line">            <span class="keyword">if</span> isinstance(data, tuple):</span><br><span class="line">                data = data[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = self.server.lpop(self.key)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(data)</span><br></pre></td></tr></table></figure><p>与 FifoQueue 不同的就是它的 pop () 方法，在这里使用的是 lpop () 操作，也就是从左侧出，而 push () 方法依然是使用的 lpush () 操作，是从左侧入。那么这样达到的效果就是先进后出、后进先出，英文叫做 Last In First Out，简称为 Lifo，而此类名称就叫做 LifoQueue。同时这个存取方式类似栈的操作，所以其实也可以称作 StackQueue。</p><p>另外在源码中还有一个子类实现，叫做 <code>PriorityQueue</code>，顾名思义，它叫做优先级队列（<strong>此队列是默认使用的队列，也就是爬取队列默认是使用有序集合来存储的</strong>），实现如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PriorityQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider priority queue abstraction using redis' sorted set"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.zcard(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        data = self._encode_request(request)</span><br><span class="line">        score = -request.priority</span><br><span class="line">        self.server.execute_command(<span class="string">'ZADD'</span>, self.key, score, data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Pop a request</span></span><br><span class="line"><span class="string">        timeout not support in this queue class</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pipe = self.server.pipeline()</span><br><span class="line">        pipe.multi()</span><br><span class="line">        pipe.zrange(self.key, <span class="number">0</span>, <span class="number">0</span>).zremrangebyrank(self.key, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">        results, count = pipe.execute()</span><br><span class="line">        <span class="keyword">if</span> results:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(results[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>在这里我们可以看到 <code>len()</code>、<code>push ()</code>、<code>pop ()</code> 方法中使用了 server 对象的 <code>zcard ()</code>、<code>zadd ()</code>、<code>zrange ()</code> 操作，可以知道这里使用的存储结果是有序集合 <code>Sorted Set</code>，在这个集合中每个元素都可以设置一个分数，那么这个分数就代表优先级。</p><p>在 <code>len()</code> 方法里调用了 <code>zcard ()</code> 操作，返回的就是有序集合的大小，也就是爬取队列的长度，在 <code>push ()</code> 方法中调用了 <code>zadd ()</code> 操作，就是向集合中添加元素，这里的分数指定成 Request 的优先级的相反数，因为分数低的会排在集合的前面，所以这里高优先级的Request 就会存在集合的最前面。<code>pop ()</code> 方法是首先调用了 <code>zrange ()</code> 操作取出了集合的第一个元素，因为最高优先级的 Request 会存在集合最前面，所以第一个元素就是最高优先级的 Request，然后再调用 <code>zremrangebyrank ()</code> 操作将这个元素删除，这样就完成了取出并删除的操作。</p><h4 id="去重过滤"><a href="#去重过滤" class="headerlink" title="去重过滤"></a>去重过滤</h4><p>前面说过 Scrapy 的去重是利用集合来实现的，而在 Scrapy 分布式中的去重就需要利用共享的集合，那么这里使用的就是 Redis 中的集合数据结构。我们来看看去重类是怎样实现的，源码文件是 <code>dupefilter.py</code>，其内实现了一个 <code>RFPDupeFilter</code> 类，如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RFPDupeFilter</span><span class="params">(BaseDupeFilter)</span>:</span></span><br><span class="line">    <span class="string">"""Redis-based request duplicates filter.</span></span><br><span class="line"><span class="string">    This class can also be used with default Scrapy's scheduler.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger = logger</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, key, debug=False)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize the duplicates filter.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        server : redis.StrictRedis</span></span><br><span class="line"><span class="string">            The redis server instance.</span></span><br><span class="line"><span class="string">        key : str</span></span><br><span class="line"><span class="string">            Redis key Where to store fingerprints.</span></span><br><span class="line"><span class="string">        debug : bool, optional</span></span><br><span class="line"><span class="string">            Whether to log filtered requests.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.server = server</span><br><span class="line">        self.key = key</span><br><span class="line">        self.debug = debug</span><br><span class="line">        self.logdupes = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">        <span class="string">"""Returns an instance from given settings.</span></span><br><span class="line"><span class="string">        This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the</span></span><br><span class="line"><span class="string">        ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as</span></span><br><span class="line"><span class="string">        it needs to pass the spider name in the key.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        settings : scrapy.settings.Settings</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        RFPDupeFilter</span></span><br><span class="line"><span class="string">            A RFPDupeFilter instance.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        server = get_redis_from_settings(settings)</span><br><span class="line">        key = defaults.DUPEFILTER_KEY % &#123;<span class="string">'timestamp'</span>: int(time.time())&#125;</span><br><span class="line">        debug = settings.getbool(<span class="string">'DUPEFILTER_DEBUG'</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(server, key=key, debug=debug)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="string">"""Returns instance from crawler.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        crawler : scrapy.crawler.Crawler</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        RFPDupeFilter</span></span><br><span class="line"><span class="string">            Instance of RFPDupeFilter.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> cls.from_settings(crawler.settings)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Returns True if request was already seen.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        fp = self.request_fingerprint(request)</span><br><span class="line">        added = self.server.sadd(self.key, fp)</span><br><span class="line">        <span class="keyword">return</span> added == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_fingerprint</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a fingerprint for a given request.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> request_fingerprint(request)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, reason=<span class="string">''</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Delete data on close. Called by Scrapy's scheduler.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        reason : str, optional</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.clear()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Clears fingerprints data."""</span></span><br><span class="line">        self.server.delete(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">"""Logs given request.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string">        spider : scrapy.spiders.Spider</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.debug:</span><br><span class="line">            msg = <span class="string">"Filtered duplicate request: %(request) s"</span></span><br><span class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</span><br><span class="line">        <span class="keyword">elif</span> self.logdupes:</span><br><span class="line">            msg = (<span class="string">"Filtered duplicate request %(request) s"</span></span><br><span class="line">                   <span class="string">"- no more duplicates will be shown"</span></span><br><span class="line">                   <span class="string">"(see DUPEFILTER_DEBUG to show all duplicates)"</span>)</span><br><span class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</span><br><span class="line">            self.logdupes = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>这里同样实现了一个 <code>request_seen ()</code> 方法，和 Scrapy 中的 <code>request_seen ()</code> 方法实现极其类似。不过这里集合使用的是 server 对象的 <code>sadd ()</code> 操作，也就是集合不再是一个简单数据结构了，而是直接换成了数据库的存储方式。</p><p>鉴别重复的方式还是使用指纹，指纹同样是依靠 <code>request_fingerprint ()</code> 方法来获取的。获取指纹之后就直接向集合添加指纹，如果添加成功，说明这个指纹原本不存在于集合中，返回值 1。代码中最后的返回结果是判定添加结果是否为 0，如果刚才的返回值为 1，那这个判定结果就是 False，也就是不重复，否则判定为重复。 这样我们就成功利用 Redis 的集合完成了指纹的记录和重复的验证。</p><h4 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h4><p>Scrapy-Redis 还帮我们实现了配合 Queue、DupeFilter 使用的调度器 Scheduler，源文件名称是 <code>scheduler.py</code>。我们可以指定一些配置，如 <code>SCHEDULER_FLUSH_ON_START</code> 即是否在爬取开始的时候清空爬取队列，<code>SCHEDULER_PERSIST</code> 即是否在爬取结束后保持爬取队列不清除。我们可以在 settings.py 里自由配置，而此调度器很好地实现了对接。 接下来我们看看两个核心的存取方法，实现如下所示：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enqueue_request</span><span class="params">(self, request)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> request.dont_filter <span class="keyword">and</span> self.df.request_seen(request):</span><br><span class="line">        self.df.log(request, self.spider)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> self.stats:</span><br><span class="line">        self.stats.inc_value(<span class="string">'scheduler/enqueued/redis'</span>, spider=self.spider)</span><br><span class="line">    self.queue.push(request)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_request</span><span class="params">(self)</span>:</span></span><br><span class="line">    block_pop_timeout = self.idle_before_close</span><br><span class="line">    request = self.queue.pop(block_pop_timeout)</span><br><span class="line">    <span class="keyword">if</span> request <span class="keyword">and</span> self.stats:</span><br><span class="line">        self.stats.inc_value(<span class="string">'scheduler/dequeued/redis'</span>, spider=self.spider)</span><br><span class="line">    <span class="keyword">return</span> request</span><br></pre></td></tr></table></figure><p><code>enqueue_request ()</code> 可以向队列中添加 Request，核心操作就是调用 Queue 的 <code>push ()</code> 操作，还有一些统计和日志操作。<code>next_request ()</code> 就是从队列中取 Request，核心操作就是调用 Queue 的 <code>pop ()</code> 操作，此时如果队列中还有 Request，则 Request 会直接取出来，爬取继续，否则如果队列为空，爬取则会重新开始。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>爬取队列的实现，在这里提供了三种队列，使用了 Redis 的列表或有序集合来维护。</li><li>去重的实现，使用了 Redis 的集合来保存 Request 的指纹来提供重复过滤。</li><li>中断后重新爬取的实现，中断后 Redis 的队列没有清空，再次启动时调度器的 <code>next_request ()</code> 会从队列中取到下一个 Request，继续爬取。</li></ul><h3 id="Scrapy分布式实现"><a href="#Scrapy分布式实现" class="headerlink" title="Scrapy分布式实现"></a>Scrapy分布式实现</h3><p>代码地址为：<a href="https://github.com/Python3WebSpider/Weibo/tree/distributed" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Weibo/tree/distributed</a></p><h4 id="搭建-Redis-服务器"><a href="#搭建-Redis-服务器" class="headerlink" title="搭建 Redis 服务器"></a>搭建 Redis 服务器</h4><p>要实现分布式部署，多台主机需要共享爬取队列和去重集合，而这两部分内容都是存于 Redis 数据库中的，我们需要搭建一个可公网访问的 Redis 服务器。</p><h4 id="部署代理池和-Cookies-池"><a href="#部署代理池和-Cookies-池" class="headerlink" title="部署代理池和 Cookies 池"></a>部署代理池和 Cookies 池</h4><p>将二者放到可以被公网访问的服务器上运行，将代码上传到服务器，修改 Redis 的连接信息配置，用同样的方式运行代理池和 Cookies 池。 远程访问代理池和 Cookies 池提供的接口，来获取随机代理和 Cookies。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">PROXY_URL = <span class="string">'http://120.27.34.25:5555/random'</span></span><br><span class="line">COOKIES_URL = <span class="string">'http://120.27.34.25:5556/weibo/random'</span></span><br></pre></td></tr></table></figure><h4 id="配置-Scrapy-Redis"><a href="#配置-Scrapy-Redis" class="headerlink" title="配置 Scrapy-Redis"></a>配置 Scrapy-Redis</h4><p>修改 <code>settings.py</code> 配置文件。</p><ul><li>核心配置<br>将调度器的类和去重的类替换为 Scrapy-Redis 提供的类，在 settings.py 里面添加如下配置即可：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">SCHEDULER = <span class="string">"scrapy_redis.scheduler.Scheduler"</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span></span><br></pre></td></tr></table></figure></li><li>Redis 连接配置<br>通过连接字符串配置。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">REDIS_URL = <span class="string">'redis://:foobared@120.27.34.25:6379'</span></span><br></pre></td></tr></table></figure>分项单独配置。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">REDIS_HOST = <span class="string">'120.27.34.25'</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span></span><br><span class="line">REDIS_PASSWORD = <span class="string">'foobared'</span></span><br></pre></td></tr></table></figure>注意，如果配置了 REDIS_URL，那么 Scrapy-Redis 将优先使用 REDIS_URL 连接，会覆盖上面的三项配置。</li><li>配置调度队列<br>此项配置是可选的，默认使用 PriorityQueue。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">SCHEDULER_QUEUE_CLASS = <span class="string">'scrapy_redis.queue.PriorityQueue'</span></span><br><span class="line">SCHEDULER_QUEUE_CLASS = <span class="string">'scrapy_redis.queue.FifoQueue'</span></span><br><span class="line">SCHEDULER_QUEUE_CLASS = <span class="string">'scrapy_redis.queue.LifoQueue'</span></span><br></pre></td></tr></table></figure></li><li>配置持久化<br>此配置是可选的，默认是 False。Scrapy-Redis 默认会在爬取全部完成后清空爬取队列和去重指纹集合。 如果不想自动清空爬取队列和去重指纹集合，可以增加如下配置：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">SCHEDULER_PERSIST = <span class="literal">True</span></span><br></pre></td></tr></table></figure>如果强制中断爬虫的运行，爬取队列和去重指纹集合是不会自动清空的。</li><li>配置重爬<br>此配置是可选的，默认是 False。如果配置了持久化或者强制中断了爬虫，那么爬取队列和指纹集合不会被清空，爬虫重新启动之后就会接着上次爬取。如果想重新爬取，我们可以配置重爬的选项：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">SCHEDULER_FLUSH_ON_START = <span class="literal">True</span></span><br></pre></td></tr></table></figure></li><li>Pipeline 配置<br>此配置是可选的，默认不启动 Pipeline。Scrapy-Redis 实现了一个存储到 Redis 的 Item Pipeline，启用了这个 Pipeline 的话，爬虫会把生成的 Item 存储到 Redis 数据库中。在数据量比较大的情况下，一般不会这么做。因为 Redis 是基于内存的，我们利用的是它处理速度快的特性，用它来做存储未免太浪费了，配置如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;<span class="string">'scrapy_redis.pipelines.RedisPipeline'</span>: <span class="number">300</span>&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="配置存储目标"><a href="#配置存储目标" class="headerlink" title="配置存储目标"></a>配置存储目标</h4><p>在服务器上搭建一个 MongoDB 服务，或者直接购买 MongoDB 数据存储服务。 这里使用的就是服务器上搭建的的 MongoDB 服务，IP 仍然为 120.27.34.25，用户名为 admin，密码为 admin123。 修改配置 MONGO_URI 为如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">MONGO_URI = <span class="string">'mongodb://admin:admin123@120.27.34.25:27017'</span></span><br></pre></td></tr></table></figure><h4 id="运行-4"><a href="#运行-4" class="headerlink" title="运行"></a>运行</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapy crawl weibocn</span><br></pre></td></tr></table></figure><h3 id="Bloom-Filter的对接"><a href="#Bloom-Filter的对接" class="headerlink" title="Bloom Filter的对接"></a>Bloom Filter的对接</h3><p>首先回顾一下 Scrapy-Redis 的去重机制。</p><p>Scrapy-Redis 将 Request 的指纹存储到了 Redis 集合中，每个指纹的长度为 40，例如 27adcc2e8979cdee0c9cecbbe8bf8ff51edefb61 就是一个指纹，它的每一位都是 16 进制数。</p><p>我们计算一下用这种方式耗费的存储空间。每个十六进制数占用 4 b，1 个指纹用 40 个十六进制数表示，占用空间为 20 B，1 万个指纹即占用空间 200 KB，1 亿个指纹占用 2 GB。</p><p>当爬取数量达到上亿级别时，Redis 的占用的内存就会变得很大，而且这仅仅是指纹的存储。Redis 还存储了爬取队列，内存占用会进一步提高，更别说有多个 Scrapy 项目同时爬取的情况了。当爬取达到亿级别规模时，Scrapy-Redis 提供的集合去重已经不能满足我们的要求。所以我们需要使用一个更加节省内存的去重算法 Bloom Filter。</p><h4 id="了解-BloomFilter"><a href="#了解-BloomFilter" class="headerlink" title="了解 BloomFilter"></a>了解 BloomFilter</h4><p>Bloom Filter，中文名称叫作布隆过滤器，是 1970 年由 Bloom 提出的，它可以被用来检测一个元素是否在一个集合中。Bloom Filter 的空间利用效率很高，使用它可以大大节省存储空间。</p><p>Bloom Filter 使用位数组表示一个待检测集合，并可以快速地通过概率算法判断一个元素是否存在于这个集合中。利用这个算法可以实现去重效果。</p><h4 id="BloomFilter-的算法"><a href="#BloomFilter-的算法" class="headerlink" title="BloomFilter 的算法"></a>BloomFilter 的算法</h4><h4 id="对接-Scrapy-Redis"><a href="#对接-Scrapy-Redis" class="headerlink" title="对接 Scrapy-Redis"></a>对接 Scrapy-Redis</h4><p>本节代码地址为：<a href="https://github.com/Python3WebSpider/ScrapyRedisBloomFilter。" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyRedisBloomFilter。</a></p><p>实现 BloomFilter 时，我们首先要保证不能破坏 Scrapy-Redis 分布式爬取的运行架构，所以我们需要修改 Scrapy-Redis 的源码，将它的去重类替换掉。同时 BloomFilter 的实现需要借助于一个位数组，所以既然当前架构还是依赖于 Redis 的，那么正好位数组的维护直接使用 Redis 就好了。 首先我们实现一个基本的哈希算法，可以实现将一个值经过哈希运算后映射到一个 m 位位数组的某一位上，代码实现如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashMap</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, m, seed)</span>:</span></span><br><span class="line">        self.m = m</span><br><span class="line">        self.seed = seed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Hash Algorithm</span></span><br><span class="line"><span class="string">        :param value: Value</span></span><br><span class="line"><span class="string">        :return: Hash Value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ret = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(value)):</span><br><span class="line">            ret += self.seed * ret + ord(value[i])</span><br><span class="line">        <span class="keyword">return</span> (self.m - <span class="number">1</span>) &amp; ret</span><br></pre></td></tr></table></figure><p>在这里新建了一个 HashMap 类，构造函数传入两个值，一个是 m 位数组的位数，另一个是种子值 seed，不同的哈希函数需要有不同的 seed，这样可以保证不同的哈希函数的结果不会碰撞。 在 hash () 方法的实现中，value 是要被处理的内容，在这里我们遍历了该字符的每一位并利用 ord () 方法取到了它的 ASCII 码值，然后混淆 seed 进行迭代求和运算，最终会得到一个数值。这个数值的结果就由 value 和 seed 唯一确定，然后我们再将它和 m 进行按位与运算，即可获取到 m 位数组的映射结果，这样我们就实现了一个由字符串和 seed 来确定的哈希函数。当 m 固定时，只要 seed 值相同，就代表是同一个哈希函数，相同的 value 必然会映射到相同的位置。所以如果我们想要构造几个不同的哈希函数，只需要改变其 seed 就好了。</p><p>以上便是一个简易的哈希函数的实现。 接下来我们再实现 BloomFilter，BloomFilter 里面需要用到 k 个哈希函数，所以在这里我们需要对这几个哈希函数指定相同的 m 值和不同的 seed 值，在这里构造如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">BLOOMFILTER_HASH_NUMBER = <span class="number">6</span></span><br><span class="line">BLOOMFILTER_BIT = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BloomFilter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, key, bit=BLOOMFILTER_BIT, hash_number=BLOOMFILTER_HASH_NUMBER)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize BloomFilter</span></span><br><span class="line"><span class="string">        :param server: Redis Server</span></span><br><span class="line"><span class="string">        :param key: BloomFilter Key</span></span><br><span class="line"><span class="string">        :param bit: m = 2 ^ bit</span></span><br><span class="line"><span class="string">        :param hash_number: the number of hash function</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># default to 1 &lt;&lt; 30 = 10,7374,1824 = 2^30 = 128MB, max filter 2^30/hash_number = 1,7895,6970 fingerprints</span></span><br><span class="line">        self.m = <span class="number">1</span> &lt;&lt; bit</span><br><span class="line">        self.seeds = range(hash_number)</span><br><span class="line">        self.maps = [HashMap(self.m, seed) <span class="keyword">for</span> seed <span class="keyword">in</span> self.seeds]</span><br><span class="line">        self.server = server</span><br><span class="line">        self.key = key</span><br></pre></td></tr></table></figure><p>由于我们需要亿级别的数据的去重，即前文介绍的算法中的 n 为 1 亿以上，哈希函数的个数 k 大约取 10 左右的量级，而 m&gt;kn，所以这里 m 值大约保底在 10 亿，由于这个数值比较大，所以这里用移位操作来实现，传入位数 bit，定义 30，然后做一个移位操作 1 &lt;&lt;30，相当于 2 的 30 次方，等于 1073741824，量级也是恰好在 10 亿左右，由于是位数组，所以这个位数组占用的大小就是 2^30b=128MB，而本文开头我们计算过 Scrapy-Redis 集合去重的占用空间大约在 2G 左右，可见 BloomFilter 的空间利用效率之高。 随后我们再传入哈希函数的个数，用它来生成几个不同的 seed，用不同的 seed 来定义不同的哈希函数，这样我们就可以构造一个哈希函数列表，遍历 seed，构造带有不同 seed 值的 HashMap 对象，保存成变量 maps 供后续使用。 另外 server 就是 Redis 连接对象，key 就是这个 m 位数组的名称。</p><p>接下来我们就要实现比较关键的两个方法了，一个是判定元素是否重复的方法 exists ()，另一个是添加元素到集合中的方法 insert ()，实现如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span><span class="params">(self, value)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    if value exists</span></span><br><span class="line"><span class="string">    :param value:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> value:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    exist = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> map <span class="keyword">in</span> self.maps:</span><br><span class="line">        offset = map.hash(value)</span><br><span class="line">        exist = exist &amp; self.server.getbit(self.key, offset)</span><br><span class="line">    <span class="keyword">return</span> exist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, value)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    add value to bloom</span></span><br><span class="line"><span class="string">    :param value:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> self.maps:</span><br><span class="line">        offset = f.hash(value)</span><br><span class="line">        self.server.setbit(self.key, offset, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>首先我们先看下 insert () 方法，BloomFilter 算法中会逐个调用哈希函数对放入集合中的元素进行运算得到在 m 位位数组中的映射位置，然后将位数组对应的位置置 1，所以这里在代码中我们遍历了初始化好的哈希函数，然后调用其 hash () 方法算出映射位置 offset，再利用 Redis 的 setbit () 方法将该位置 1。 在 exists () 方法中我们就需要实现判定是否重复的逻辑了，方法参数 value 即为待判断的元素，在这里我们首先定义了一个变量 exist，然后遍历了所有哈希函数对 value 进行哈希运算，得到映射位置，然后我们用 getbit () 方法取得该映射位置的结果，依次进行与运算。这样只有每次 getbit () 得到的结果都为 1 时，最后的 exist 才为 True，即代表 value 属于这个集合。如果其中只要有一次 getbit () 得到的结果为 0，即 m 位数组中有对应的 0 位，那么最终的结果 exist 就为 False，即代表 value 不属于这个集合。这样此方法最后的返回结果就是判定重复与否的结果了。 到现在为止 BloomFilter 的实现就已经完成了，我们可以用一个实例来测试一下，代码如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">conn = StrictRedis(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, password=<span class="string">'foobared'</span>)</span><br><span class="line">bf = BloomFilter(conn, <span class="string">'testbf'</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">bf.insert(<span class="string">'Hello'</span>)</span><br><span class="line">bf.insert(<span class="string">'World'</span>)</span><br><span class="line">result = bf.exists(<span class="string">'Hello'</span>)</span><br><span class="line">print(bool(result))</span><br><span class="line">result = bf.exists(<span class="string">'Python'</span>)</span><br><span class="line">print(bool(result))</span><br></pre></td></tr></table></figure><p>在这里我们首先定义了一个 Redis 连接对象，然后传递给 BloomFilter，为了避免内存占用过大这里传的位数 bit 比较小，设置为 5，哈希函数的个数设置为 6。 首先我们调用 insert () 方法插入了 Hello 和 World 两个字符串，随后判断了一下 Hello 和 Python 这两个字符串是否存在，最后输出它的结果，运行结果如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><p>接下来我们需要继续修改 Scrapy-Redis 的源码，将它的 dupefilter 逻辑替换为 BloomFilter 的逻辑，在这里主要是修改 RFPDupeFilter 类的 request_seen () 方法，实现如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></span><br><span class="line">    fp = self.request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> self.bf.exists(fp):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    self.bf.insert(fp)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>首先还是利用 request_fingerprint () 方法获取了 Request 的指纹，然后调用 BloomFilter 的 exists () 方法判定了该指纹是否存在，如果存在，则证明该 Request 是重复的，返回 True，否则调用 BloomFilter 的 insert () 方法将该指纹添加并返回 False，这样就成功利用 BloomFilter 替换了 Scrapy-Redis 的集合去重。 对于 BloomFilter 的初始化定义，我们可以将 init() 方法修改为如下内容：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, key, debug, bit, hash_number)</span>:</span></span><br><span class="line">    self.server = server</span><br><span class="line">    self.key = key</span><br><span class="line">    self.debug = debug</span><br><span class="line">    self.bit = bit</span><br><span class="line">    self.hash_number = hash_number</span><br><span class="line">    self.logdupes = <span class="literal">True</span></span><br><span class="line">    self.bf = BloomFilter(server, self.key, bit, hash_number)</span><br></pre></td></tr></table></figure><p>其中 bit 和 hash_number 需要使用 from_settings () 方法传递，修改如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">    server = get_redis_from_settings(settings)</span><br><span class="line">    key = defaults.DUPEFILTER_KEY % &#123;<span class="string">'timestamp'</span>: int(time.time())&#125;</span><br><span class="line">    debug = settings.getbool(<span class="string">'DUPEFILTER_DEBUG'</span>, DUPEFILTER_DEBUG)</span><br><span class="line">    bit = settings.getint(<span class="string">'BLOOMFILTER_BIT'</span>, BLOOMFILTER_BIT)</span><br><span class="line">    hash_number = settings.getint(<span class="string">'BLOOMFILTER_HASH_NUMBER'</span>, BLOOMFILTER_HASH_NUMBER)</span><br><span class="line">    <span class="keyword">return</span> cls(server, key=key, debug=debug, bit=bit, hash_number=hash_number)</span><br></pre></td></tr></table></figure><p>其中常量的定义 DUPEFILTER_DEBUG 和 BLOOMFILTER_BIT 统一定义在 defaults.py 中，默认如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">BLOOMFILTER_HASH_NUMBER = <span class="number">6</span></span><br><span class="line">BLOOMFILTER_BIT = <span class="number">30</span></span><br></pre></td></tr></table></figure><p>到此为止我们就成功实现了 BloomFilter 和 Scrapy-Redis 的对接。</p><h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><p>为了方便使用，本节的代码已经打包成了一个 Python 包并发布到了 PyPi，链接为：<br><a href="https://pypi.python.org/pypi/scrapy-redis-bloomfilter" target="_blank" rel="noopener">https://pypi.python.org/pypi/scrapy-redis-bloomfilter</a><br>因此我们以后如果想使用 ScrapyRedisBloomFilter 直接使用就好了，不需要再自己实现一遍。 我们可以直接使用 Pip 来安装，命令如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pip install scrapy-redis-bloomfilter</span><br></pre></td></tr></table></figure><p>使用的方法和 Scrapy-Redis 基本相似，在这里说明几个关键配置：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 去重类，要使用 BloomFilter 请替换 DUPEFILTER_CLASS</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">"scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter"</span></span><br><span class="line"><span class="comment"># 哈希函数的个数，默认为 6，可以自行修改</span></span><br><span class="line">BLOOMFILTER_HASH_NUMBER = <span class="number">6</span></span><br><span class="line"><span class="comment"># BloomFilter 的 bit 参数，默认 30，占用 128MB 空间，去重量级 1 亿</span></span><br><span class="line">BLOOMFILTER_BIT = <span class="number">30</span></span><br></pre></td></tr></table></figure><p>DUPEFILTER_CLASS 是去重类，如果要使用 BloomFilter 需要将 DUPEFILTER_CLASS 修改为该包的去重类。 BLOOMFILTER_HASH_NUMBER 是 BloomFilter 使用的哈希函数的个数，默认为 6，可以根据去重量级自行修改。 BLOOMFILTER_BIT 即前文所介绍的 BloomFilter 类的 bit 参数，它决定了位数组的位数，如果 BLOOMFILTER_BIT 为 30，那么位数组位数为 2 的 30 次方，将占用 Redis 128MB 的存储空间，去重量级在 1 亿左右，即对应爬取量级 1 亿左右。如果爬取量级在 10 亿、20 亿甚至 100 亿，请务必将此参数对应调高。</p><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>在源代码中附有一个测试项目，放在 tests 文件夹，该项目使用了 Scrapy-RedisBloomFilter 来去重，Spider 的实现如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request, Spider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'test'</span></span><br><span class="line">    base_url = <span class="string">'https://www.baidu.com/s?wd='</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            url = self.base_url + str(i)</span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Here contains 10 duplicated Requests    </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>): </span><br><span class="line">            url = self.base_url + str(i)</span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.debug(<span class="string">'Response of '</span> + response.url)</span><br></pre></td></tr></table></figure><p>在 start_requests () 方法中首先循环 10 次，构造参数为 0-9 的 URL，然后重新循环了 100 次，构造了参数为 0-99 的 URL，那么这里就会包含 10 个重复的 Request，我们运行项目测试一下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapy crawl test</span><br></pre></td></tr></table></figure><p>可以看到最后统计的第一行的结果：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="string">'bloomfilter/filtered'</span>: <span class="number">10</span>,</span><br></pre></td></tr></table></figure><p>这就是 BloomFilter 过滤后的统计结果，可以看到它的过滤个数为 10 个，也就是它成功将重复的 10 个 Reqeust 识别出来了，测试通过。</p><h2 id="Ch-14-分布式爬虫的部署"><a href="#Ch-14-分布式爬虫的部署" class="headerlink" title="Ch 14 分布式爬虫的部署"></a>Ch 14 分布式爬虫的部署</h2><h3 id="Scrapyd分布式部署"><a href="#Scrapyd分布式部署" class="headerlink" title="Scrapyd分布式部署"></a>Scrapyd分布式部署</h3><h4 id="了解-Scrapyd"><a href="#了解-Scrapyd" class="headerlink" title="了解 Scrapyd"></a>了解 Scrapyd</h4><p>Scrapyd 是一个运行 Scrapy 爬虫的服务程序，它提供一系列 HTTP 接口来帮助我们部署、启动、停止、删除爬虫程序。Scrapyd 支持版本管理，同时还可以管理多个爬虫任务，利用它我们可以非常方便地完成 Scrapy 爬虫项目的部署任务调度。</p><h4 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pip install scrapyd</span><br></pre></td></tr></table></figure><h4 id="Scrapyd-的功能"><a href="#Scrapyd-的功能" class="headerlink" title="Scrapyd 的功能"></a>Scrapyd 的功能</h4><ul><li>daemonstatus.json<br>这个接口负责查看 Scrapyd 当前的服务和任务状态，我们可以用 curl 命令来请求这个接口，命令如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">curl http://<span class="number">139.217</span><span class="number">.26</span><span class="number">.30</span>:<span class="number">6800</span>/daemonstatus.json</span><br></pre></td></tr></table></figure>结果：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"status"</span>: <span class="string">"ok"</span>, <span class="string">"finished"</span>: <span class="number">90</span>, <span class="string">"running"</span>: <span class="number">9</span>, <span class="string">"node_name"</span>: <span class="string">"datacrawl-vm"</span>, <span class="string">"pending"</span>: <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure>返回结果是 Json 字符串，status 是当前运行状态， finished 代表当前已经完成的 Scrapy 任务，running 代表正在运行的 Scrapy 任务，pending 代表等待被调度的 Scrapyd 任务，node_name 就是主机的名称。</li><li>addversion.json<br>这个接口主要是用来部署 Scrapy 项目用的，在部署的时候我们需要首先将项目打包成 Egg 文件，然后传入项目名称和部署版本。 我们可以用如下的方式实现项目部署：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">curl http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/addversion.json -F project=wenbo -F version=first -F egg=@weibo.egg</span><br></pre></td></tr></table></figure>在这里 -F 即代表添加一个参数，同时我们还需要将项目打包成 Egg 文件放到本地。</li><li>schedule.json<br>这个接口负责调度已部署好的 Scrapy 项目运行。 我们可以用如下接口实现任务调度：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">curl http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/schedule.json -d project=weibo -d spider=weibocn</span><br></pre></td></tr></table></figure>在这里需要传入两个参数，project 即 Scrapy 项目名称，spider 即 Spider 名称。 返回结果如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"status"</span>: <span class="string">"ok"</span>, <span class="string">"jobid"</span>: <span class="string">"6487ec79947edab326d6db28a2d86511e8247444"</span>&#125;</span><br></pre></td></tr></table></figure>status 代表 Scrapy 项目启动情况，jobid 代表当前正在运行的爬取任务代号。</li><li>cancel.json<br>这个接口可以用来取消某个爬取任务，如果这个任务是 pending 状态，那么它将会被移除，如果这个任务是 running 状态，那么它将会被终止。 我们可以用下面的命令来取消任务的运行：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">curl http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/cancel.json -d project=weibo -d job=<span class="number">6487</span>ec79947edab326d6db28a2d86511e8247444</span><br></pre></td></tr></table></figure>在这里需要传入两个参数，project 即项目名称，job 即爬取任务代号。返回结果如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"status"</span>: <span class="string">"ok"</span>, <span class="string">"prevstate"</span>: <span class="string">"running"</span>&#125;</span><br></pre></td></tr></table></figure>status 代表请求执行情况，prevstate 代表之前的运行状态。</li><li>listprojects.json<br>这个接口用来列出部署到 Scrapyd 服务上的所有项目描述。 我们可以用下面的命令来获取 Scrapyd 服务器上的所有项目描述：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">curl http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/listprojects.json</span><br></pre></td></tr></table></figure></li><li>listversions.json<br>这个接口用来获取某个项目的所有版本号，版本号是按序排列的，最后一个条目是最新的版本号。 我们可以用如下命令来获取项目的版本号：<figure class="highlight"><table><tr><td class="code"><pre><span class="line">curl http://120.27.34.25:6800/listversions.json?project=weibo</span><br></pre></td></tr></table></figure></li><li>listspiders.json<br>这个接口用来获取某个项目最新的一个版本的所有 Spider 名称。 我们可以用如下命令来获取项目的 Spider 名称：<figure class="highlight"><table><tr><td class="code"><pre><span class="line">curl http://120.27.34.25:6800/listspiders.json?project=weibo</span><br></pre></td></tr></table></figure></li><li>listjobs.json<br>这个接口用来获取某个项目当前运行的所有任务详情。 我们可以用如下命令来获取所有任务详情：<figure class="highlight"><table><tr><td class="code"><pre><span class="line">curl http://120.27.34.25:6800/listjobs.json?project=weibo</span><br></pre></td></tr></table></figure>在这里需要一个参数 project，就是项目的名称。 返回结果如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"status"</span>: <span class="string">"ok"</span>,</span><br><span class="line"><span class="string">"pending"</span>: [&#123;<span class="string">"id"</span>: <span class="string">"78391cc0fcaf11e1b0090800272a6d06"</span>, <span class="string">"spider"</span>: <span class="string">"weibocn"</span>&#125;],</span><br><span class="line"><span class="string">"running"</span>: [&#123;<span class="string">"id"</span>: <span class="string">"422e608f9f28cef127b3d5ef93fe9399"</span>, <span class="string">"spider"</span>: <span class="string">"weibocn"</span>, <span class="string">"start_time"</span>: <span class="string">"2017-07-12 10:14:03.594664"</span>&#125;],</span><br><span class="line"><span class="string">"finished"</span>: [&#123;<span class="string">"id"</span>: <span class="string">"2f16646cfcaf11e1b0090800272a6d06"</span>, <span class="string">"spider"</span>: <span class="string">"weibocn"</span>, <span class="string">"start_time"</span>: <span class="string">"2017-07-12 10:14:03.594664"</span>, <span class="string">"end_time"</span>: <span class="string">"2017-07-12 10:24:03.594664"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>status 代表请求执行情况，pendings 代表当前正在等待的任务，running 代表当前正在运行的任务，finished 代表已经完成的任务。</li><li>delversion.json<br>这个接口用来删除项目的某个版本。 我们可以用如下命令来删除项目版本：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">curl http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/delversion.json -d project=weibo -d version=v1</span><br></pre></td></tr></table></figure></li><li>delproject.json<br>这个接口用来删除某个项目。 我们可以用如下命令来删除某个项目：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">curl http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/delproject.json -d project=weibo</span><br></pre></td></tr></table></figure></li></ul><h4 id="ScrapydAPI-的使用"><a href="#ScrapydAPI-的使用" class="headerlink" title="ScrapydAPI 的使用"></a>ScrapydAPI 的使用</h4><p>安装：<code>pip install python-scrapyd-api</code><br>ScrapydAPI 库对这些接口又做了一层封装。下面我们来看下 ScrapydAPI 的使用方法，其实核心原理和 HTTP 接口请求方式并无二致，只不过用 Python 封装后使用更加便捷。 我们可以用如下方式建立一个 ScrapydAPI 对象：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapyd_api <span class="keyword">import</span> ScrapydAPI</span><br><span class="line">scrapyd = ScrapydAPI(<span class="string">'http://120.27.34.25:6800'</span>)</span><br></pre></td></tr></table></figure><p>然后就可以调用它的方法来实现对应接口的操作了，例如部署的操作可以使用如下方式：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">egg = open(<span class="string">'weibo.egg'</span>, <span class="string">'rb'</span>)</span><br><span class="line">scrapyd.add_version(<span class="string">'weibo'</span>, <span class="string">'v1'</span>, egg)</span><br></pre></td></tr></table></figure><p>这样我们就可以将项目打包为 Egg 文件，然后把本地打包的的 Egg 项目部署到远程 Scrapyd 了。 另外 ScrapydAPI 还实现了所有 Scrapyd 提供的 API 接口，名称都是相同的，参数也是相同的。 例如我们可以调用 list_projects () 方法即可列出 Scrapyd 中所有已部署的项目：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapyd.list_projects()</span><br><span class="line">[<span class="string">'weibo'</span>, <span class="string">'zhihu'</span>]</span><br></pre></td></tr></table></figure><p>更加详细的操作可以参考其官方文档：<a href="http://python-scrapyd-api.readthedocs.io/" target="_blank" rel="noopener">http://python-scrapyd-api.readthedocs.io/</a></p><h3 id="Scrapy-Client使用"><a href="#Scrapy-Client使用" class="headerlink" title="Scrapy-Client使用"></a>Scrapy-Client使用</h3><p>Scrapyd-Client用来完成部署过程。</p><h4 id="安装-2"><a href="#安装-2" class="headerlink" title="安装"></a>安装</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pip install scrapyd-client</span><br></pre></td></tr></table></figure><h4 id="Scrapyd-Client-的功能"><a href="#Scrapyd-Client-的功能" class="headerlink" title="Scrapyd-Client 的功能"></a>Scrapyd-Client 的功能</h4><p>Scrapyd-Client 为了方便 Scrapy 项目的部署，提供两个功能：</p><ul><li>将项目打包成 Egg 文件。</li><li>将打包生成的 Egg 文件通过 addversion.json 接口部署到 Scrapyd 上。</li></ul><p>也就是说，Scrapyd-Client 帮我们把部署全部实现了，我们不需要再去关心 Egg 文件是怎样生成的，也不需要再去读 Egg 文件并请求接口上传了，这一切的操作只需要执行一个命令即可一键部署。</p><h4 id="Scrapyd-Client-部署"><a href="#Scrapyd-Client-部署" class="headerlink" title="Scrapyd-Client 部署"></a>Scrapyd-Client 部署</h4><p>要部署 Scrapy 项目，我们首先需要修改一下项目的配置文件，例如我们之前写的 Scrapy 微博爬虫项目，在项目的第一层会有一个 scrapy.cfg 文件，在这里我们需要配置一下 deploy 部分，例如我们要将项目部署到 120.27.34.25 的 Scrapyd 上，修改内容如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default = weibo.settings</span><br><span class="line"></span><br><span class="line">[deploy]</span><br><span class="line"><span class="comment">#url = http://localhost:6800/</span></span><br><span class="line">url = http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/</span><br><span class="line">project = weibo</span><br></pre></td></tr></table></figure><p>这样我们再在 scrapy.cfg 文件所在路径执行如下命令：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapyd-deploy</span><br></pre></td></tr></table></figure><p>我们也可以指定项目版本，如果不指定的话默认为当前时间戳，指定的话通过 version 参数传递即可，例如：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapyd-deploy --version <span class="number">201707131455</span></span><br></pre></td></tr></table></figure><p>值得注意的是在 Python3 的 Scrapyd 1.2.0 版本中我们不要指定版本号为带字母的字符串，需要为纯数字，否则可能会出现报错。 另外如果我们有多台主机，我们可以配置各台主机的别名，例如可以修改配置文件为：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[deploy:vm1]</span><br><span class="line">url = http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.24</span>:<span class="number">6800</span>/</span><br><span class="line">project = weibo</span><br><span class="line"></span><br><span class="line">[deploy:vm2]</span><br><span class="line">url = http://<span class="number">139.217</span><span class="number">.26</span><span class="number">.30</span>:<span class="number">6800</span>/</span><br><span class="line">project = weibo</span><br></pre></td></tr></table></figure><p>如果我们想将项目部署到 IP 为 139.217.26.30 的 vm2 主机，我们只需要执行如下命令：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">scrapyd-deploy vm2</span><br></pre></td></tr></table></figure><p>这样我们就可以将项目部署到名称为 vm2 的主机上了。 如此一来，如果我们有多台主机，我们只需要在 scrapy.cfg 文件中配置好各台主机的 Scrapyd 地址，然后调用 scrapyd-deploy 命令加主机名称即可实现部署，非常方便。</p><p>如果 Scrapyd 设置了访问限制的话，我们可以在配置文件中加入用户名和密码的配置，同时端口修改一下，修改成 Nginx 代理端口，如在第一章我们使用的是 6801，那么这里就需要改成 6801，修改如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[deploy:vm1]</span><br><span class="line">url = http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.24</span>:<span class="number">6801</span>/</span><br><span class="line">project = weibo</span><br><span class="line">username = admin</span><br><span class="line">password = admin</span><br><span class="line"></span><br><span class="line">[deploy:vm2]</span><br><span class="line">url = http://<span class="number">139.217</span><span class="number">.26</span><span class="number">.30</span>:<span class="number">6801</span>/</span><br><span class="line">project = weibo</span><br><span class="line">username = germey</span><br><span class="line">password = germey</span><br></pre></td></tr></table></figure><h3 id="Scrapyd对接Docker"><a href="#Scrapyd对接Docker" class="headerlink" title="Scrapyd对接Docker"></a>Scrapyd对接Docker</h3><p>需要解决一个痛点，那就是 Python 环境配置问题和版本冲突解决问题。如果我们将 Scrapyd 直接打包成一个 Docker 镜像，那么在服务器上只需要执行 Docker 命令就可以启动 Scrapyd 服务，这样就不用再关心 Python 环境问题，也不需要担心版本冲突问题。 接下来，我们就将 Scrapyd 打包制作成一个 Docker 镜像。</p><h4 id="对接-Docker"><a href="#对接-Docker" class="headerlink" title="对接 Docker"></a>对接 Docker</h4><p>首先新建一个项目，然后新建一个 scrapyd.conf，即 Scrapyd 的配置文件，内容如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">[scrapyd]</span><br><span class="line">eggs_dir    = eggs</span><br><span class="line">logs_dir    = logs</span><br><span class="line">items_dir   =</span><br><span class="line">jobs_to_keep = <span class="number">5</span></span><br><span class="line">dbs_dir     = dbs</span><br><span class="line">max_proc    = <span class="number">0</span></span><br><span class="line">max_proc_per_cpu = <span class="number">10</span></span><br><span class="line">finished_to_keep = <span class="number">100</span></span><br><span class="line">poll_interval = <span class="number">5.0</span></span><br><span class="line">bind_address = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">http_port   = <span class="number">6800</span></span><br><span class="line">debug       = off</span><br><span class="line">runner      = scrapyd.runner</span><br><span class="line">application = scrapyd.app.application</span><br><span class="line">launcher    = scrapyd.launcher.Launcher</span><br><span class="line">webroot     = scrapyd.website.Root</span><br><span class="line"></span><br><span class="line">[services]</span><br><span class="line">schedule.json     = scrapyd.webservice.Schedule</span><br><span class="line">cancel.json       = scrapyd.webservice.Cancel</span><br><span class="line">addversion.json   = scrapyd.webservice.AddVersion</span><br><span class="line">listprojects.json = scrapyd.webservice.ListProjects</span><br><span class="line">listversions.json = scrapyd.webservice.ListVersions</span><br><span class="line">listspiders.json  = scrapyd.webservice.ListSpiders</span><br><span class="line">delproject.json   = scrapyd.webservice.DeleteProject</span><br><span class="line">delversion.json   = scrapyd.webservice.DeleteVersion</span><br><span class="line">listjobs.json     = scrapyd.webservice.ListJobs</span><br><span class="line">daemonstatus.json = scrapyd.webservice.DaemonStatus</span><br></pre></td></tr></table></figure><p>在这里实际上是修改自官方文档的配置文件：<br><a href="https://scrapyd.readthedocs.io/en/stable/config.html#example-configuration-file，" target="_blank" rel="noopener">https://scrapyd.readthedocs.io/en/stable/config.html#example-configuration-file，</a><br>其中修改的地方有两个：</p><ul><li>max_proc_per_cpu = 10，原本是 4，即 CPU 单核最多运行 4 个 Scrapy 任务，也就是说 1 核的主机最多同时只能运行 4 个 Scrapy 任务，在这里设置上限为 10，也可以自行设置。</li><li>bind_address = 0.0.0.0，原本是 127.0.0.1，不能公开访问，在这里修改为 0.0.0.0 即可解除此限制。</li></ul><p>接下来新建一个 requirements.txt ，将一些 Scrapy 项目常用的库都列进去，内容如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">requests</span><br><span class="line">selenium</span><br><span class="line">aiohttp</span><br><span class="line">beautifulsoup4</span><br><span class="line">pyquery</span><br><span class="line">pymysql</span><br><span class="line">redis</span><br><span class="line">pymongo</span><br><span class="line">flask</span><br><span class="line">django</span><br><span class="line">scrapy</span><br><span class="line">scrapyd</span><br><span class="line">scrapyd-client</span><br><span class="line">scrapy-redis</span><br><span class="line">scrapy-splash</span><br></pre></td></tr></table></figure><p>如果我们运行的 Scrapy 项目还有其他的库需要用到可以自行添加到此文件中。 最后我们新建一个 Dockerfile，内容如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">FROM python:<span class="number">3.7</span></span><br><span class="line">ADD . /code</span><br><span class="line">WORKDIR /code</span><br><span class="line">COPY ./scrapyd.conf /etc/scrapyd/</span><br><span class="line">EXPOSE <span class="number">6800</span></span><br><span class="line">RUN pip3 install -r requirements.txt</span><br><span class="line">CMD scrapyd</span><br></pre></td></tr></table></figure><ul><li>第一行 FROM 是指在 python:3.6 这个镜像上构建，也就是说在构建时就已经有了 Python 3.6 的环境。</li><li>第二行 ADD 是将本地的代码放置到虚拟容器中，它有两个参数，第一个参数是 . ，即代表本地当前路径，/code 代表虚拟容器中的路径，也就是将本地项目所有内容放置到虚拟容器的 /code 目录下。</li><li>第三行 WORKDIR 是指定工作目录，在这里将刚才我们添加的代码路径设成工作路径，在这个路径下的目录结构和我们当前本地目录结构是相同的，所以可以直接执行库安装命令等。</li><li>第四行 COPY 是将当前目录下的 scrapyd.conf 文件拷贝到虚拟容器的 /etc/scrapyd/ 目录下，Scrapyd 在运行的时候会默认读取这个配置。</li><li>第五行 EXPOSE 是声明运行时容器提供服务端口，注意这里只是一个声明，在运行时不一定就会在此端口开启服务。这样的声明一是告诉使用者这个镜像服务的运行端口，以方便配置映射。另一个用处则是在运行时使用随机端口映射时，会自动随机映射 EXPOSE 的端口。</li><li>第六行 RUN 是执行某些命令，一般做一些环境准备工作，由于 Docker 虚拟容器内只有 Python3 环境，而没有我们所需要的一些 Python 库，所以在这里我们运行此命令来在虚拟容器中安装相应的 Python 库，这样项目部署到 Scrapyd 中便可以正常运行了。</li><li>第七行 CMD 是容器启动命令，在容器运行时，会直接执行此命令，在这里我们直接用 scrapyd 来启动 Scrapyd 服务。</li></ul><p>到现在基本的工作就完成了，运行如下命令进行构建：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker build -t scrapyd:latest .</span><br></pre></td></tr></table></figure><p>构建成功后即可运行测试：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker run -d -p <span class="number">6800</span>:<span class="number">6800</span> scrapyd</span><br></pre></td></tr></table></figure><p>将此镜像上传到 Docker Hub，例如我的 Docker Hub 用户名为 germey，新建了一个名为 scrapyd 的项目，首先可以打一个标签：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker tag scrapyd:latest germey/scrapyd:latest</span><br></pre></td></tr></table></figure><p>这里请自行替换成你的项目名称。 然后 Push 即可：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker push germey/scrapyd:latest</span><br></pre></td></tr></table></figure><p>之后我们在其他主机运行此命令即可启动 Scrapyd 服务：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">docker run -d -p <span class="number">6800</span>:<span class="number">6800</span> germey/scrapyd</span><br></pre></td></tr></table></figure><h3 id="Scrapyd批量部署"><a href="#Scrapyd批量部署" class="headerlink" title="Scrapyd批量部署"></a>Scrapyd批量部署</h3><p>我们在上一节实现了 Scrapyd 和 Docker 的对接，这样每台主机就不用再安装 Python 环境和安装 Scrapyd 了，直接执行一句 Docker 命令运行 Scrapyd 服务即可。但是这种做法有个前提，那就是每台主机都安装 Docker，然后再去运行 Scrapyd 服务。如果我们需要部署 10 台主机的话，工作量确实不小。</p><p>一种方案是，一台主机已经安装好各种开发环境，我们取到它的镜像，然后用镜像来批量复制多台主机，批量部署就可以轻松实现了。</p><p>另一种方案是，我们在新建主机的时候直接指定一个运行脚本，脚本里写好配置各种环境的命令，指定其在新建主机的时候自动执行，那么主机创建之后所有的环境就按照自定义的命令配置好了，这样也可以很方便地实现批量部署。</p><p>目前很多服务商都提供云主机服务，如阿里云、腾讯云、Azure、Amazon 等，不同的服务商提供了不同的批量部署云主机的方式。例如，腾讯云提供了创建自定义镜像的服务，在新建主机的时候使用自定义镜像创建新的主机即可，这样就可以批量生成多个相同的环境。Azure 提供了模板部署的服务，我们可以在模板中指定新建主机时执行的配置环境的命令，这样在主机创建之后环境就配置完成了。 本节我们就来看看这两种批量部署的方式，来实现 Docker 和 Scrapyd 服务的批量部署。</p><h4 id="镜像部署"><a href="#镜像部署" class="headerlink" title="镜像部署"></a>镜像部署</h4><h4 id="模板部署"><a href="#模板部署" class="headerlink" title="模板部署"></a>模板部署</h4><h3 id="Gerapy分布式管理"><a href="#Gerapy分布式管理" class="headerlink" title="Gerapy分布式管理"></a>Gerapy分布式管理</h3><p>我们可以通过 Scrapyd-Client 将 Scrapy 项目部署到 Scrapyd 上，并且可以通过 Scrapyd API 来控制 Scrapy 的运行。那么，我们是否可以做到更优化？方法是否可以更方便可控？ 我们重新分析一下当前可以优化的问题。</p><p>使用 Scrapyd-Client 部署时，需要在配置文件中配置好各台主机的地址，然后利用命令行执行部署过程。如果我们省去各台主机的地址配置，将命令行对接图形界面，只需要点击按钮即可实现批量部署，这样就更方便了。</p><p>使用 Scrapyd API 可以控制 Scrapy 任务的启动、终止等工作，但很多操作还是需要代码来实现，同时获取爬取日志还比较烦琐。如果我们有一个图形界面，只需要点击按钮即可启动和终止爬虫任务，同时还可以实时查看爬取日志报告，那这将大大节省我们的时间和精力。</p><p>所以我们的终极目标是如下内容。</p><ul><li>更方便地控制爬虫运行</li><li>更直观地查看爬虫状态</li><li>更实时地查看爬取结果</li><li>更简单地实现项目部署</li><li>更统一地实现主机管理</li></ul><p>而这所有的工作均可通过 Gerapy 来实现。 Gerapy 是一个基于 Scrapyd、Scrapyd API、Django、Vue.js 搭建的分布式爬虫管理框架。接下来将简单介绍它的使用方法。</p><p>Gerapy 的 GitHub 地址：<a href="https://github.com/Gerapy。" target="_blank" rel="noopener">https://github.com/Gerapy。</a></p><h4 id="安装-3"><a href="#安装-3" class="headerlink" title="安装"></a>安装</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pip install gerapy</span><br></pre></td></tr></table></figure><h4 id="使用说明-2"><a href="#使用说明-2" class="headerlink" title="使用说明"></a>使用说明</h4><p>首先可以利用 gerapy 命令新建一个项目，命令如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">gerapy init</span><br></pre></td></tr></table></figure><p>这样会在当前目录下生成一个 gerapy 文件夹，然后进入 gerapy 文件夹，会发现一个空的 projects 文件夹，我们后文会提及。 这时先对数据库进行初始化：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">gerapy migrate</span><br></pre></td></tr></table></figure><p>这样即会生成一个 SQLite 数据库，数据库中会用于保存各个主机配置信息、部署版本等。 接下来启动 Gerapy 服务，命令如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">gerapy runserver</span><br></pre></td></tr></table></figure><p>这样即可在默认 8000 端口上开启 Gerapy 服务，我们浏览器打开：<a href="http://localhost:8000" target="_blank" rel="noopener">http://localhost:8000</a> 即可进入 Gerapy 的管理页面，在这里提供了主机管理和项目管理的功能。 主机管理中，我们可以将各台主机的 Scrapyd 运行地址和端口添加，并加以名称标记，添加之后便会出现在主机列表中，Gerapy 会监控各台主机的运行状况并以不同的状态标识</p><h2 id="Splash中的CrawlSpider模块源码解析"><a href="#Splash中的CrawlSpider模块源码解析" class="headerlink" title="Splash中的CrawlSpider模块源码解析"></a>Splash中的CrawlSpider模块源码解析</h2><p>地址：<a href="https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider" target="_blank" rel="noopener">https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider</a></p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">This modules implements the CrawlSpider which is the recommended spider to use</span></span><br><span class="line"><span class="string">for scraping typical web sites that requires crawling pages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">See documentation in docs/topics/spiders.rst</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Sequence</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request, HtmlResponse</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> Spider</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.spider <span class="keyword">import</span> iterate_spider_output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_identity</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_identity_process_request</span><span class="params">(request, response)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_method</span><span class="params">(method, spider)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> callable(method):</span><br><span class="line">        <span class="keyword">return</span> method</span><br><span class="line">    <span class="keyword">elif</span> isinstance(method, str):</span><br><span class="line">        <span class="keyword">return</span> getattr(spider, method, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_default_link_extractor = LinkExtractor()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rule</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        link_extractor=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        callback=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        cb_kwargs=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        follow=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        process_links=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        process_request=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        errback=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        self.link_extractor = link_extractor <span class="keyword">or</span> _default_link_extractor</span><br><span class="line">        self.callback = callback</span><br><span class="line">        self.errback = errback</span><br><span class="line">        self.cb_kwargs = cb_kwargs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        self.process_links = process_links <span class="keyword">or</span> _identity</span><br><span class="line">        self.process_request = process_request <span class="keyword">or</span> _identity_process_request</span><br><span class="line">        self.follow = follow <span class="keyword">if</span> follow <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="keyword">not</span> callback</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compile</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.callback = _get_method(self.callback, spider)</span><br><span class="line">        self.errback = _get_method(self.errback, spider)</span><br><span class="line">        self.process_links = _get_method(self.process_links, spider)</span><br><span class="line">        self.process_request = _get_method(self.process_request, spider)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrawlSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line"></span><br><span class="line">    rules: Sequence[Rule] = ()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *a, **kw)</span>:</span></span><br><span class="line">        super().__init__(*a, **kw)</span><br><span class="line">        self._compile_rules()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_parse</span><span class="params">(self, response, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._parse_response(</span><br><span class="line">            response=response,</span><br><span class="line">            callback=self.parse_start_url,</span><br><span class="line">            cb_kwargs=kwargs,</span><br><span class="line">            follow=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_start_url</span><span class="params">(self, response, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_results</span><span class="params">(self, response, results)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_request</span><span class="params">(self, rule_index, link)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Request(</span><br><span class="line">            url=link.url,</span><br><span class="line">            callback=self._callback,</span><br><span class="line">            errback=self._errback,</span><br><span class="line">            meta=dict(rule=rule_index, link_text=link.text),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        seen = set()</span><br><span class="line">        <span class="keyword">for</span> rule_index, rule <span class="keyword">in</span> enumerate(self._rules):</span><br><span class="line">            links = [lnk <span class="keyword">for</span> lnk <span class="keyword">in</span> rule.link_extractor.extract_links(response)</span><br><span class="line">                     <span class="keyword">if</span> lnk <span class="keyword">not</span> <span class="keyword">in</span> seen]</span><br><span class="line">            <span class="keyword">for</span> link <span class="keyword">in</span> rule.process_links(links):</span><br><span class="line">                seen.add(link)</span><br><span class="line">                request = self._build_request(rule_index, link)</span><br><span class="line">                <span class="keyword">yield</span> rule.process_request(request, response)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_callback</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        rule = self._rules[response.meta[<span class="string">'rule'</span>]]</span><br><span class="line">        <span class="keyword">return</span> self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_errback</span><span class="params">(self, failure)</span>:</span></span><br><span class="line">        rule = self._rules[failure.request.meta[<span class="string">'rule'</span>]]</span><br><span class="line">        <span class="keyword">return</span> self._handle_failure(failure, rule.errback)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(self, response, callback, cb_kwargs, follow=True)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> callback:</span><br><span class="line">            cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</span><br><span class="line">            cb_res = self.process_results(response, cb_res)</span><br><span class="line">            <span class="keyword">for</span> request_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</span><br><span class="line">                <span class="keyword">yield</span> request_or_item</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> follow <span class="keyword">and</span> self._follow_links:</span><br><span class="line">            <span class="keyword">for</span> request_or_item <span class="keyword">in</span> self._requests_to_follow(response):</span><br><span class="line">                <span class="keyword">yield</span> request_or_item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_handle_failure</span><span class="params">(self, failure, errback)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> errback:</span><br><span class="line">            results = errback(failure) <span class="keyword">or</span> ()</span><br><span class="line">            <span class="keyword">for</span> request_or_item <span class="keyword">in</span> iterate_spider_output(results):</span><br><span class="line">                <span class="keyword">yield</span> request_or_item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._rules = []</span><br><span class="line">        <span class="keyword">for</span> rule <span class="keyword">in</span> self.rules:</span><br><span class="line">            self._rules.append(copy.copy(rule))</span><br><span class="line">            self._rules[<span class="number">-1</span>]._compile(self)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler, *args, **kwargs)</span>:</span></span><br><span class="line">        spider = super().from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        spider._follow_links = crawler.settings.getbool(<span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> spider</span><br></pre></td></tr></table></figure><h3 id="Rule类"><a href="#Rule类" class="headerlink" title="Rule类"></a>Rule类</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">spiders</span>.<span class="title">Rule</span><span class="params">(link_extractor=None, callback=None, cb_kwargs=None, </span></span></span><br><span class="line"><span class="class"><span class="params">  follow=None, process_links=None, process_request=None, errback=None)</span></span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rule</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        link_extractor=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        callback=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        cb_kwargs=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        follow=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        process_links=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        process_request=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        errback=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        self.link_extractor = link_extractor <span class="keyword">or</span> _default_link_extractor</span><br><span class="line">        self.callback = callback</span><br><span class="line">        self.errback = errback</span><br><span class="line">        self.cb_kwargs = cb_kwargs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        self.process_links = process_links <span class="keyword">or</span> _identity</span><br><span class="line">        self.process_request = process_request <span class="keyword">or</span> _identity_process_request</span><br><span class="line">        self.follow = follow <span class="keyword">if</span> follow <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="keyword">not</span> callback</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compile</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.callback = _get_method(self.callback, spider)</span><br><span class="line">        self.errback = _get_method(self.errback, spider)</span><br><span class="line">        self.process_links = _get_method(self.process_links, spider)</span><br><span class="line">        self.process_request = _get_method(self.process_request, spider)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Spider</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>常用命令</title>
    <url>/post/Tips/common-commands/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>包括Hexo、Django等<a id="more"></a></p><h2 id="Django常用命令"><a href="#Django常用命令" class="headerlink" title="Django常用命令"></a>Django常用命令</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python manage.py makemigrations</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python manage.py migrate</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python manage.py createsuperuser</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python manage.py collectstatic</span><br></pre></td></tr></table></figure><h2 id="Hexo常用命令"><a href="#Hexo常用命令" class="headerlink" title="Hexo常用命令"></a>Hexo常用命令</h2><p>文档：<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/</a></p><h3 id="新建文章："><a href="#新建文章：" class="headerlink" title="新建文章："></a>新建文章：</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">hexo new page --path _posts/Hexo/hexo-commands <span class="string">"Hexo常用命令"</span></span><br></pre></td></tr></table></figure><h3 id="生成静态文件和上传："><a href="#生成静态文件和上传：" class="headerlink" title="生成静态文件和上传："></a>生成静态文件和上传：</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">hexo g -d</span><br></pre></td></tr></table></figure><h3 id="本地运行："><a href="#本地运行：" class="headerlink" title="本地运行："></a>本地运行：</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">hexo s --debug</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tips</category>
      </categories>
  </entry>
  <entry>
    <title>《Python深度学习》笔记</title>
    <url>/post/DeepLearning/deep-learning-notes/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>《Python深度学习》笔记<a id="more"></a><br><a href="#jump">Windows下的环境搭建</a>，<br>英文电子版<a href="https://livebook.manning.com/book/deep-learning-with-python/" target="_blank" rel="noopener">https://livebook.manning.com/book/deep-learning-with-python/</a><br>Keras中文文档：<a href="https://keras.io/zh/" target="_blank" rel="noopener">https://keras.io/zh/</a></p><h2 id="Chapter-1-基本概念"><a href="#Chapter-1-基本概念" class="headerlink" title="Chapter 1 基本概念"></a>Chapter 1 基本概念</h2><ul><li>首先理清人工智能、机器学习和深度学习的概念和关系，<br>人工智能&gt;机器学习&gt;深度学习<br>深度学习是机器学习的子集，而机器学习则是人工智能的子集</li></ul><h3 id="1-1-人工智能"><a href="#1-1-人工智能" class="headerlink" title="1.1 人工智能"></a>1.1 人工智能</h3><p><strong>简洁定义</strong>：将通常由人类完成的智力任务自动化。<br><strong>发展</strong>：<em>符号主义人工智能</em>（symbolic AI）→<em>机器学习</em>（machine learning）<br><strong>符号主义人工智能</strong>：编写足够多的明确规则来处理知识，用于解决定义明确的逻辑问题，如下国际象棋，但对于图像分类、语音识别等难以给出明确规则的复杂、模糊的问题将无法解决。</p><h3 id="1-2-机器学习"><a href="#1-2-机器学习" class="headerlink" title="1.2 机器学习"></a>1.2 机器学习</h3><p><img src="https://i.loli.net/2020/05/16/iweKFoO9k862ufD.png" alt="机器学习与经典程序设计区别"></p><ul><li>机器学习将某个任务相关的示例输入机器学习系统，系统从中找到统计结构，最终找到规则将任务自动化，其与经典程序设计不同如上图。</li><li>机器学习的三个要素如下：</li><li>（1）输入数据点。</li><li>（2）预期输出的示例。</li><li>（3）衡量算法效果好坏的方法。</li><li>机器学习和深度学习的核心问题在于<strong>有意义地变换数据</strong> ，即学习输入数据的有用<strong>表示</strong>（表征数据或将数据编码），如彩色图像可以编码为RGB（红-绿-蓝）格式或HSV（色相-饱和度-明度）格式，在应对不同任务时，不同的表示方式将会产生很大的差异，所以机器学习模型的目的就是为输入数据寻找合适的表示。</li><li><strong>定义</strong>：在预先定义好的可能性空间中（假设空间），利用反馈信号的指引来寻找输入数据的有用表示。</li></ul><h3 id="1-3-深度学习"><a href="#1-3-深度学习" class="headerlink" title="1.3 深度学习"></a>1.3 深度学习</h3><ul><li><p>深度学习强调从连续的<strong>层</strong>中进行学习，这些层对应越来越有意义的<strong>表示</strong>。</p></li><li><p>模型包含的层数成为模型的<strong>深度</strong>，这些分层总是通过<strong>神经网络</strong>（neural network）模型学习得到。</p></li><li><p><strong>定义</strong>：学习数据表示的多级方法，相当于多级信息蒸馏操作。</p></li><li><p><strong>深度学习工作原理</strong>：</p></li><li><p>神经网络每层对输入数据所做的具体操作保存在该层的<strong>权重</strong>（weight）中，其本质是一串数字，每层实现的变换由其权重来<strong>参数化</strong>（parameterize），<strong>学习</strong>即为每层找到一组权重值，使得该网络能够将每个示例输入与其目标正确地一一对应。</p></li><li><p>输入网络预测值和真实目标值通过<strong>损失函数</strong>计算一个距离值，利用这个距离值作为反馈信号通过<strong>优化器</strong>实现<strong>反向传播算法</strong>来对权重值进行微调，以降低当前示例的损失值，随着示例的增多，损失值逐渐降低，输出值与目标值尽可能接近。如下图。<br><img src="https://i.loli.net/2020/05/16/zomZXBxty67UqEN.png" alt="深度学习工作原理图"></p></li><li><p>首先了解特征工程的概念，即手动提取出数据有用的表示，即手动为数据设计好的表示层。而深度学习可将这个步骤完全自动化，一次学习所有特征。</p></li><li><p>深度学习的两个基本特征：第一，通过渐进的逐层的方式形成越来越复杂的表示；第二，对中间这些渐进的表示层同一时间共同进行学习，而不是依次连续学习（贪婪学习），每一层的变化都需要同时考虑上下两层的需要。</p></li></ul><h3 id="1-3-机器学习简史"><a href="#1-3-机器学习简史" class="headerlink" title="1.3 机器学习简史"></a>1.3 机器学习简史</h3><h4 id="1-3-1-概率建模"><a href="#1-3-1-概率建模" class="headerlink" title="1.3.1 概率建模"></a>1.3.1 概率建模</h4><ul><li>概率建模（probabilistic modeling）是统计学原理在数据分析中的应用。</li><li>两种分类方法：朴素贝叶斯算法和logistic回归（logistic regression，简称logreg）</li></ul><h4 id="1-3-2-早期神经网络"><a href="#1-3-2-早期神经网络" class="headerlink" title="1.3.2 早期神经网络"></a>1.3.2 早期神经网络</h4><h4 id="1-3-3-核方法"><a href="#1-3-3-核方法" class="headerlink" title="1.3.3 核方法"></a>1.3.3 核方法</h4><ul><li>核方法（kenel method）：另一种机器学习方法，为一组分类算法，支持向量机（SVM，support vector machine）最为出名。</li><li>SVM通过找出两个不同类别的两组数据点间的良好决策边界（decision boundary）来解决分类问题。</li><li>SVM分两步来寻找决策边界：<ul><li>将数据映射到一个新的高维表示，此时决策边界可以用一个超平面来表示（二维数据超平面为一条直线）。</li><li>间隔最大化（maximizing the margin）：尽量让超平面与每个类别最接近的数据点之间的距离最大化，从而计算出良好决策边界。</li></ul></li><li>将数据映射到一个新的高维表示，需要用到核技巧，其基本思想为：利用核函数（人为选择）在新空间中计算点对之间的距离，从而找到良好的决策超平面。</li><li>SVM是一种浅层方法，在简单的分类问题上表现出良好的性能，但在大型数据集以及图像分类等感知问题上效果不好。</li></ul><h4 id="1-3-4-决策树、随机森林与梯度提升机"><a href="#1-3-4-决策树、随机森林与梯度提升机" class="headerlink" title="1.3.4 决策树、随机森林与梯度提升机"></a>1.3.4 决策树、随机森林与梯度提升机</h4><ul><li>决策树（decision tree）是类似于流程图的结构，可以对输入数据点进行分类或根据给定输入来预测输出值。</li><li>随机森林（random forest）算法：构建许多决策树，然后将其输出集成在一起，适用于各种浅层学习的非感知类机器学习任务。</li><li>梯度提升机（gradient boosting machine）：使用梯度提升方法，通过迭代训练新模型来专门解决之前模型的弱点，从而改进任何机器学习模型的效果。</li></ul><h2 id="Chapter-2-神经网络的数学基础"><a href="#Chapter-2-神经网络的数学基础" class="headerlink" title="Chapter 2 神经网络的数学基础"></a>Chapter 2 神经网络的数学基础</h2><h3 id="2-1-张量（tensor）"><a href="#2-1-张量（tensor）" class="headerlink" title="2.1 张量（tensor）"></a>2.1 张量（tensor）</h3><ul><li>定义：又叫多维Numpy数组，作为机器学习的基本数据结构，是一个数据容器，包含的数据几乎总是数值数据，如矩阵是二维张量，张量是矩阵向任意维度的推广[ 张量的<strong>维度</strong>（dimension）通常叫做<strong>轴</strong>（axis）]</li></ul><h3 id="2-2-标量（scalar）"><a href="#2-2-标量（scalar）" class="headerlink" title="2.2 标量（scalar）"></a>2.2 标量（scalar）</h3><ul><li>定义：仅包含一个数字的张量，又叫标量张量、零维张量、0D张量。在Numpy数组中，一个float32或float64的数字就是一个标量。</li><li>标量张量有0个轴（ndim == 0），轴的个数又叫<strong>阶</strong>（rank）。如下代码查看一个标量张量的轴的个数：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array(<span class="number">12</span>)</span><br><span class="line">print(x.ndim)   <span class="comment">#轴的个数</span></span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="2-3-向量（vector）"><a href="#2-3-向量（vector）" class="headerlink" title="2.3 向量（vector）"></a>2.3 向量（vector）</h3><ul><li>定义：数字组成的数组，又叫一维张量或1D张量，只有一个轴，如下：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>])</span><br><span class="line">print(x.ndim)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>该向量有5个元素，为<strong>5D向量</strong>，有1个轴，沿着轴有5个维度。而<strong>5D张量</strong>有5个轴，沿着某个轴可能有任意个维度。</li></ul><h3 id="2-4-矩阵（matrix）"><a href="#2-4-矩阵（matrix）" class="headerlink" title="2.4 矩阵（matrix）"></a>2.4 矩阵（matrix）</h3><p>定义：多个向量组成的数组，又叫二维张量或2D张量，有2个轴，如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>]])</span><br><span class="line">print(x.ndim)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><p>第一个轴上的元素叫行，[12, 3, 22, 121, 4]是x的第一行，第二个轴上的元素叫列，[12, 12, 12]是x的第一列。</p><h3 id="2-5-3D张量与更高维张量"><a href="#2-5-3D张量与更高维张量" class="headerlink" title="2.5 3D张量与更高维张量"></a>2.5 3D张量与更高维张量</h3><p>定义：多个矩阵组成的数组，有3个轴，如下：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[[<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>]],</span><br><span class="line">            [[<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>],</span><br><span class="line">             [<span class="number">12</span>, <span class="number">3</span>, <span class="number">22</span>, <span class="number">121</span>, <span class="number">4</span>]]])</span><br><span class="line">print(x.ndim)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><p>深度学习一般处理0D到4D张量，处理视频数据可能会遇到5D张量。</p><h3 id="2-6-张量的关键属性"><a href="#2-6-张量的关键属性" class="headerlink" title="2.6 张量的关键属性"></a>2.6 张量的关键属性</h3><ul><li><strong>轴的个数</strong>（阶、维度）：Python库中为ndim。</li><li><strong>形状</strong>：张量沿某个轴的维度大小（元素个数），如前面的3D张量的形状为（3， 3， 5）。</li><li><strong>数据类型</strong>：张量所包含数据的类型，Python库中为dtype，如float32、float64、unit8等，极少数情况有字符（char）张量，Numpy等大多数库都不存在字符串张量，因为<span style="border-bottom:2px solid red">张量存储在预先分配的连续内存段中</span>，而字符串长度可变，无法用这种方式存储。</li></ul><p>代码示例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line">print(train_images.shape)   <span class="comment">#形状</span></span><br><span class="line">print(train_images.ndim)    <span class="comment">#轴的个数</span></span><br><span class="line">print(train_images.dtype)   <span class="comment">#数据类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#该3D张量中的第四个数字</span></span><br><span class="line">digit = train_images[<span class="number">4</span>]</span><br><span class="line">plt.imshow(digit, cmap=plt.cm.binary)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="number">3</span></span><br><span class="line">uint8</span><br><span class="line"></span><br><span class="line"><span class="comment">#由上可见，train_images是一个由8位整数组成的3维张量，即60000个28*28整数矩阵组成的数组，</span></span><br><span class="line"><span class="comment">#每个矩阵是一张灰度图像。</span></span><br></pre></td></tr></table></figure><p><img src="https://s1.ax1x.com/2020/05/18/Yf6u1H.png" alt="该3D张量中的第四个数字"></p><h3 id="2-7-张量切片"><a href="#2-7-张量切片" class="headerlink" title="2.7 张量切片"></a>2.7 张量切片</h3><p>类似于Python的切片操作，可用于裁剪图像，示例:</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#选择第10~100个数字（不包括100），将其放在形为（90，28，28）的数组中</span></span><br><span class="line">my_slice = train_images[<span class="number">10</span>:<span class="number">100</span>]     </span><br><span class="line">my_slice = train_images[<span class="number">10</span>:<span class="number">100</span>, <span class="number">0</span>:<span class="number">28</span>, <span class="number">0</span>:<span class="number">28</span>]   <span class="comment">#两者等价</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#可沿着张量轴在任意两个索引间进行选择，也可使用负索引</span></span><br><span class="line">my_slice = train_images[, <span class="number">14</span>:, <span class="number">14</span>:]       <span class="comment">#右下角14像素*14像素</span></span><br><span class="line">my_slice = train_images[, <span class="number">7</span>:<span class="number">-7</span>, <span class="number">7</span>:<span class="number">-7</span>]       <span class="comment">#中心14像素*14像素</span></span><br></pre></td></tr></table></figure><h3 id="2-8-数据批量"><a href="#2-8-数据批量" class="headerlink" title="2.8 数据批量"></a>2.8 数据批量</h3><ul><li>所有数据张量的第一个轴称为<strong>样本轴</strong>或<strong>0轴</strong>（samples axis）。</li><li>深度学习模型不会同时处理整个数据集，而是会将数据集拆分成小批量，形如<code>batch = train_images[:128]</code>，批量大小为128。</li><li>对于这种批量张量，第一个轴为<strong>批量轴</strong>（batch axis）或<strong>批量维度</strong>（batch dimension）。</li></ul><h3 id="2-9-现实世界中的数据张量"><a href="#2-9-现实世界中的数据张量" class="headerlink" title="2.9 现实世界中的数据张量"></a>2.9 现实世界中的数据张量</h3><ul><li><strong>向量数据</strong>：2D 张量，形状为 (samples, features) 。</li><li><strong>时间序列数据或序列数据</strong>：3D 张量，形状为 (samples, timesteps, features) 。</li><li><strong>图像</strong>：4D张量，形状为 (samples, height, width, channels) 或 (samples, channels,<br>height, width) 。</li><li><strong>视频</strong>：5D张量，形状为 (samples, frames, height, width, channels) 或 (samples,<br>frames, channels, height, width)</li></ul><h4 id="2-9-1-向量数据"><a href="#2-9-1-向量数据" class="headerlink" title="2.9.1 向量数据"></a>2.9.1 向量数据</h4><ul><li>常见的数据。每个数据点被编码为一个向量，因此一个数据批量就被编码为 2D 张量（即向量组成的数组），其中第一个轴是<strong>样本轴</strong>，第二个轴是<strong>特征轴</strong> <span style="border-bottom:2px solid red">(samples, features)</span>。如</li><li>人口统计数据集，其中包括每个人的年龄、邮编和收入。每个人可以表示为包含 3 个值的向量，而整个数据集包含 100 000 个人，因此可以存储在形状为 (100000, 3) 的 2D张量中。</li></ul><h4 id="2-9-2-时间序列数据或序列数据"><a href="#2-9-2-时间序列数据或序列数据" class="headerlink" title="2.9.2 时间序列数据或序列数据"></a>2.9.2 时间序列数据或序列数据</h4><ul><li>当时间对于数据很重要时，将数据存储在带有时间轴的3D张量中。每个样本被编码为一个向量序列（即2D张量），因此一个数据批量就被编码为一个3D张量 <span style="border-bottom:2px solid red">(samples, timesteps, features)</span>。如图：<br><img src="https://s1.ax1x.com/2020/05/18/YhClMF.png" alt=""></li><li>依据惯例，时间轴始终是第二个轴。</li><li>例如股票价格数据集：<br>要构建一个股票价格数据集：每一分钟，我们将股票的当前价格，前一分钟的最高价格和前一分钟的最低价格保存下来，那么就被编码为一个3D向量。整个交易日就编码为一个有390个3D向量的2D张量（390，3）。250天的数据就编码为3D张量（250，390，3）。</li></ul><h4 id="2-9-3-图像数据"><a href="#2-9-3-图像数据" class="headerlink" title="2.9.3 图像数据"></a>2.9.3 图像数据</h4><ul><li>图像通常具有三个维度：<strong>高度</strong>、<strong>宽度</strong>和<strong>颜色深度</strong>。虽然灰度图像（比如 MNIST 数字图像）只有一个颜色通道，因此可以保存在 2D 张量中，但按照惯例，图像张量始终都是 3D 张量，灰度图像的彩色通道只有一维。因此，如果图像大小为 256×256，那么 128 张灰度图像组成的批量可以保存在一个形状为 (128, 256, 256, 1) 的张量中，而 128 张彩色图像组成的批量则可以保存在一个形状为 (128, 256, 256, 3) 的张量中。<br><img src="https://s1.ax1x.com/2020/05/18/YhipjS.png" alt=""></li><li>图像张量的形状有两种约定：通道在后（channels-last）的约定（在 TensorFlow 中使用）和通道在前（channels-first）的约定（在 Theano 中使用）。</li><li>Google 的 TensorFlow 机器学习框架将颜色深度轴放在最后：<span style="border-bottom:2px solid red"> (samples, height, width, color_depth)</span>,例如(128, 256, 256, 3) 。</li><li>Theano将图像深度轴放在批量轴之后：<span style="border-bottom:2px solid red">(samples, color_depth, height, width)</span>，例如(128, 3, 256, 256)。</li><li>Keras 框架同时支持这两种格式。</li></ul><h4 id="2-9-4-视频数据"><a href="#2-9-4-视频数据" class="headerlink" title="2.9.4 视频数据"></a>2.9.4 视频数据</h4><ul><li>视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧，每一帧都是一张彩色图像。由于每一帧都可以保存在一个形状为 (height, width, color_depth) 的 3D 张量中，因此一系列帧可以保存在一个形状为 (frames, height, width,color_depth) 的 4D 张量中，而不同视频组成的批量则可以保存在一个 5D 张量中，其形状为<span style="border-bottom:2px solid red">(samples, frames, height, width, color_depth) </span>。</li><li>例如，一个以每秒 4 帧采样的 60 秒 YouTube 视频片段，视频尺寸为 144×256，这个视频共有 240 帧。4 个这样的视频片段组成的批量将保存在形状为 (4, 240, 144, 256, 3)的张量中。总共有 106 168 320 个值！如果张量的数据类型（ dtype ）是 float32 ，每个值都是32 位，那么这个张量共有 405MB。现实生活中遇到的视频要小得多，因为它们不以float32 格式存储，而且通常被大大压缩，比如 MPEG 格式。</li></ul><h3 id="2-10-张量运算——神经网络的“齿轮”"><a href="#2-10-张量运算——神经网络的“齿轮”" class="headerlink" title="2.10 张量运算——神经网络的“齿轮”"></a>2.10 张量运算——神经网络的“齿轮”</h3><p>本节和下一节主要是高等数学和线性代数在神经网络中的运用，下面只简单提一下这些数学知识在神经网络中的相关概念。</p><h4 id="2-10-1-逐元素运算（element-wise）"><a href="#2-10-1-逐元素运算（element-wise）" class="headerlink" title="2.10.1 逐元素运算（element-wise）"></a>2.10.1 逐元素运算（element-wise）</h4><ul><li>运算独立地应用于张量中的每个元素，适合大规模并行实现。如：</li><li>relu运算：relu(x) == max(x, 0)</li><li>四则运算（前提是运算对象形状相同）</li><li>在Numpy中可直接进行逐元素运算，由Numpy内置函数交给基础线性代数子程序（BLAS，basic linear algebra subprograms）运算。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">z = x + y</span><br><span class="line">z = np.maximum(x, <span class="number">0.</span>)</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-10-2-广播（broadcast）"><a href="#2-10-2-广播（broadcast）" class="headerlink" title="2.10.2 广播（broadcast）"></a>2.10.2 广播（broadcast）</h4><ul><li>两个形状不同地张量相加，较小地张量会被广播，以匹配较大的张量。如：</li><li>x形状为（32，10），y形状为（10，），则x+y会为y添加空的第一个轴（广播轴）→（1，10），再沿新轴重复32次→（32，10）。</li><li>但以上过程不会在运算中实际发生，只是想象的思维模型。</li></ul><h4 id="2-10-3-张量点积（tensor-product）"><a href="#2-10-3-张量点积（tensor-product）" class="headerlink" title="2.10.3 张量点积（tensor product）"></a>2.10.3 张量点积（tensor product）</h4><ul><li>类似于线性代数中矩阵的乘法，Keras和Numpy中使用<code>numpy.dot(x, y)</code>实现。</li><li>形如(a, b, c, d).(d, e)-&gt;(a, b, c, e)，<img src="https://s1.ax1x.com/2020/05/19/Y4DYO1.jpg" alt=""></li></ul><h4 id="2-10-4-张量变形（tensor-reshaping）"><a href="#2-10-4-张量变形（tensor-reshaping）" class="headerlink" title="2.10.4 张量变形（tensor reshaping）"></a>2.10.4 张量变形（tensor reshaping）</h4><ul><li>改变张量的行列，但元素总数不变。<code>x.reshape((2,6))</code></li><li>行列互换称为<strong>转置</strong>（transposition）<code>numpy.transpose(x)</code></li></ul><h4 id="2-10-5-张量运算的几何解释"><a href="#2-10-5-张量运算的几何解释" class="headerlink" title="2.10.5 张量运算的几何解释"></a>2.10.5 张量运算的几何解释</h4><ul><li>类似高数中的向量变换以及更高维的延申。</li><li>机器学习的内容就是，为复杂的、高度折叠的数据流形式找到简洁地表示。</li></ul><h3 id="2-11-神经网络的“引擎”——基于梯度的优化"><a href="#2-11-神经网络的“引擎”——基于梯度的优化" class="headerlink" title="2.11 神经网络的“引擎”——基于梯度的优化"></a>2.11 神经网络的“引擎”——基于梯度的优化</h3><ul><li><p>对每个神经层用下述方法对输入数据进行变换。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">output = relu(dot(W, input) + b)</span><br></pre></td></tr></table></figure><ul><li>在这个表达式中，W 和 b 都是张量，均为该层的属性。它们被称为该层的<strong>权重</strong>（weight）或<strong>可训练参数</strong>（trainable parameter），分别对应 kernel 和 bias 属性。这些权重包含网络从观察训练数据中学到的信息。</li><li>一开始，这些权重矩阵取较小的随机值，这一步叫作<strong>随机初始化</strong>（random initialization）。当然，W 和 b 都是随机的，<code>relu(dot(W, input) + b)</code> 肯定不会得到任何有用的表示。虽然得到的表示是没有意义的，但这是一个起点。下一步则是根据反馈信号逐渐调节这些权重。这个逐渐调节的过程叫作<strong>训练</strong>，也就是机器学习中的<strong>学习</strong>。</li><li>上述过程发生在一个<strong>训练循环</strong>（training loop）内，其具体过程如下。必要时一直重复这些步骤。<ul><li>抽取训练样本x 和对应目标y 组成的数据批量。</li><li>在x 上运行网络［这一步叫作前向传播（forward pass）］，得到预测值y_pred。</li><li>计算网络在这批数据上的损失，用于衡量y_pred 和y 之间的距离。</li><li>更新网络的所有权重，使网络在这批数据上的损失略微下降。</li></ul></li><li>最终得到的网络在训练数据上的损失非常小，即预测值y_pred 和预期目标y 之间的距离非常小。</li></ul></li></ul><p><span style="border-bottom:2px solid red">详情参见：<a href="https://www.ituring.com.cn/book/tupubarticle/23177" target="_blank" rel="noopener">https://www.ituring.com.cn/book/tupubarticle/23177</a></span></p><h2 id="Chapter-3-神经网络入门"><a href="#Chapter-3-神经网络入门" class="headerlink" title="Chapter 3 神经网络入门"></a>Chapter 3 神经网络入门</h2><ul><li>这里再次引用第一章的深度学习工作原理图。<br><img src="https://i.loli.net/2020/05/16/zomZXBxty67UqEN.png" alt="深度学习工作原理图"></li></ul><h3 id="3-1-层：深度学习的基础组件"><a href="#3-1-层：深度学习的基础组件" class="headerlink" title="3.1 层：深度学习的基础组件"></a>3.1 层：深度学习的基础组件</h3><ul><li><strong>层</strong>是神经网络的基本数据结构，层的状态即层的<strong>权重</strong>，权重是利用<strong>随机梯度下降</strong>学到的一个或多个张量。以下是几种不同的层及其应用场景。</li><li><strong>密集连接层</strong>（densely connected layer）：又叫<strong>全连接层</strong>（fully connected layer）和<strong>密集层</strong>（dense layer），用于处理保存简单向量的2D张量，形状为 (samples, features)，对应Keras的Dense类。</li><li><strong>循环层</strong>（recurrent layer）：用于处理保存序列数据的3D张量，形状为 (samples, timesteps, features)，对应Keras的LSTM层。</li><li><strong>二维卷积层</strong>：用于处理保存图像数据的4D张量，形状为 (samples, height, width, channels)，对应Keras的Conv2D。</li></ul><h3 id="3-2-模型：层构成的网络"><a href="#3-2-模型：层构成的网络" class="headerlink" title="3.2 模型：层构成的网络"></a>3.2 模型：层构成的网络</h3><ul><li>构建深度学习模型就是将相互兼容的多个层拼接在一起，以建立有用的数据变换流程。深度学习模型是层构成的有向无环图。</li><li>这里的层兼容性（layer compatibility）指每一层只接受特定形状的输入张量并返回特定形状的输出张量。</li><li>一些常见的网络拓扑结构如下:</li><li>（1）线性网络</li><li>（2）双分支（two-branch）网络</li><li>（3）多头（multihead）网络</li><li>（4）Inception模块</li></ul><h3 id="3-3-损失函数与优化器：配置学习过程的关键"><a href="#3-3-损失函数与优化器：配置学习过程的关键" class="headerlink" title="3.3 损失函数与优化器：配置学习过程的关键"></a>3.3 损失函数与优化器：配置学习过程的关键</h3><ul><li><p>损失函数(目标函数)：训练过程中将其最小化，能够衡量当前任务是否已成功完成。</p></li><li><p>优化器：决定如何根据损失函数对网络进行更新，执行随机梯度下降（SGD： stochastic gradient descent）的某个变体。</p></li><li><p>注意：具有多个输出的神经网络可能有多个损失函数，但梯度下降过程必须基于单个标量损失值。因此要将所有损失函数取平均变为一个标量值。</p></li></ul><h3 id="3-4-Keras开发流程"><a href="#3-4-Keras开发流程" class="headerlink" title="3.4 Keras开发流程"></a>3.4 Keras开发流程</h3><ul><li>定义训练数据：输入张量和目标张量。</li><li>定义层组成的网络（或模型），将输入映射到目标。</li><li>配置学习过程：选择损失函数，优化器和需要监控的指标。</li><li>调用模型的fit()方法迭代训练数据。</li></ul><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models,layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用Sequential类定义模型</span></span><br><span class="line"><span class="comment"># model = models.Sequential()</span></span><br><span class="line"><span class="comment"># model.add(layers.Dense(32, activation='relu', input_shape=(784,)))</span></span><br><span class="line"><span class="comment"># model.add(layers.Dense(10, activation='softmax'))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#函数式API定义模型</span></span><br><span class="line">input_tensor = layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">x = layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(input_tensor)</span><br><span class="line">output_tensor = layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)(x)</span><br><span class="line">model = models.Model(inputs=input_tensor, outputs=output_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment">#编译</span></span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),       <span class="comment">#优化器</span></span><br><span class="line">              loss=<span class="string">'mse'</span>,                                   <span class="comment">#损失函数</span></span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])                         <span class="comment">#评估函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#迭代训练数据</span></span><br><span class="line">model.fit(input_tensor, target_tensor, batch_size=<span class="number">128</span>, epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="3-5-电影评论分类：二分类问题"><a href="#3-5-电影评论分类：二分类问题" class="headerlink" title="3.5 电影评论分类：二分类问题"></a>3.5 电影评论分类：二分类问题</h3><p>注意：</p><ul><li>隐藏单元越多（更高维的表示空间），网络越能学习更复杂的表示，但这会使网络的计算代价更大，并且可能导致学习到不好的模式（这种模式可以提高训练数据的性能，但不能提高测试数据的性能）。</li><li>sigmoid函数将任意值压缩到[0, 1]区间内。</li><li>relu（rectified linear unit，整流线性单元）函数，将所有负值归零。</li><li>激活函数：也叫非线性，为了得到更丰富的假设空间，充分利用多层表示的优势。</li><li>对于二分类这种最后输出概率值的问题，损失函数最优解为binary_crossentropy（二元交叉熵）。</li></ul><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#二分类问题</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models, layers</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 1 加载数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#train_data和train_labels为评论组成的列表，而每条评论为单词索引组成的列表</span></span><br><span class="line"><span class="comment">#test_data和test_labels为0、1组成的列表，代表负面和正面</span></span><br><span class="line"><span class="comment">#num_words=10000表示仅保留训练数据中前10000个最常出现的单词</span></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># word_index = imdb.get_word_index()                                   #将单词映射为整数索引的字典</span></span><br><span class="line"><span class="comment"># reverse_word_index = dict(                                           #键值颠倒，整数索引映射为单词</span></span><br><span class="line"><span class="comment">#     [(value, key) for (key, value) in word_index.items()])</span></span><br><span class="line"><span class="comment"># decoded_review = ' '.join(                                           #评论解码，索引减去3，</span></span><br><span class="line"><span class="comment">#     [reverse_word_index.get(i - 3, '?') for i in train_data[0]])     #因为0、1、2为保留索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 2 处理数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#对列表进行one-hot编码，转化为0、1组成的向量。</span></span><br><span class="line"><span class="comment">#将整数序列编码为二进制矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))                    <span class="comment">#创建一个零矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)                              <span class="comment">#训练数据向量化</span></span><br><span class="line">x_test = vectorize_sequences(test_data)                                <span class="comment">#测试数据向量化</span></span><br><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)                   <span class="comment">#训练标签向量化</span></span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)                     <span class="comment">#测试标签向量化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#留出10000个样本作验证集</span></span><br><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line">y_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 3 定义模型/构建网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#采用3层全连接层，16为隐藏单元个数，即维度，activation为激活函数</span></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))    <span class="comment">#relu（rectified linear unit，整流线性单元）函数，将所有负值归零。</span></span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))  <span class="comment">#sigmoid函数将任意值压缩到[0, 1]区间内,输出一个0，1范围内的概率值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 4 编译模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置优化器，损失函数，评估函数</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'acc'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 5 训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用512个样本组成的小批量，对所有样本进行20次迭代，</span></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图</span></span><br><span class="line"></span><br><span class="line">history_dict = history.history</span><br><span class="line">loss_values = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">val_loss_values = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss_values) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss_values, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss_values, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.clf()</span><br><span class="line">acc_values = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">val_acc_values = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc_values, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc_values, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><table><thead><tr><th><img src="https://s1.ax1x.com/2020/05/20/YTstRf.png" alt="训练损失和验证损失"></th><th align="center"><img src="https://s1.ax1x.com/2020/05/20/YTsgzT.png" alt="训练精度和验证精度"></th></tr></thead></table><ul><li>由上图可见，每轮训练损失在降低，训练精度在上升，符合预期，但验证损失和验证精度并非如此，模型在训练数据上表现更好，但不一定在从未见过的数据上表现更好，这种现象成为<strong>过拟合</strong>（overfit），详见Chapter 4。</li><li>下面是一种简单的训练方法：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">results = model.evaluate(x_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>results</span><br><span class="line">[<span class="number">0.29506705965518953</span>, <span class="number">0.884119987487793</span>]    <span class="comment">#返回loss和acc</span></span><br></pre></td></tr></table></figure></li><li>用训练好的网络进行预测，得到评论为正面的可能性大小。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>model.predict(x_test)</span><br><span class="line">array([[ <span class="number">0.98006207</span>]</span><br><span class="line">       [ <span class="number">0.99758697</span>]</span><br><span class="line">       [ <span class="number">0.99975556</span>]</span><br><span class="line">       ...,</span><br><span class="line">       [ <span class="number">0.82167041</span>]</span><br><span class="line">       [ <span class="number">0.02885115</span>]</span><br><span class="line">       [ <span class="number">0.65371346</span>]], dtype=float32)</span><br></pre></td></tr></table></figure></li></ul><h3 id="3-6-新闻分类：多分类问题"><a href="#3-6-新闻分类：多分类问题" class="headerlink" title="3.6 新闻分类：多分类问题"></a>3.6 新闻分类：多分类问题</h3><ul><li>单标签、多分类（single-label，multiclass classification）：每个数据点只能划分到一个类别。</li><li>多标签、多分类（multilabel，multiclass classification）：每个数据点能划分到多个类别。</li></ul><p>注意：</p><ul><li>编码数据中将标签向量化的两种方法：转化为整数张量或进行one-hot编码。注意两种方法所使用的<strong>损失函数</strong>可能会有所差别。<ul><li>通过分类编码（也称为one-hot编码），使用categorical_crossentropy作为损失函数对标签进行编码。</li><li>将标签编码为整数，使用sparse_categorical_crossentropy作为损失函数对标签进行编码。</li></ul></li><li>如果要在N个类别中对数据点进行分类，则网络最后一层应为大小为N的Dense层。</li><li>在单标签，多类分类问题中，网络应以softmax激活结束，这样可以输出在N个类别上的概率分布。</li><li><strong>分类交叉熵</strong>几乎总是针对多分类问题使用的损失函数，它使网络输出的概率分布与目标的真实分布之间的距离最小化。</li><li>如果需要将数据划分为大量类别，则应避免使用较小的中间层，而在网络中导致信息瓶颈（永久地丢失信息）。</li></ul><p>本例使用路透社数据集。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多分类问题</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models, layers</span><br><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(</span><br><span class="line">    num_words=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br><span class="line"><span class="comment">#编码标签的两种方法</span></span><br><span class="line"><span class="comment">#分类编码</span></span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将标签转化为整数张量，对应需改变损失函数为sparse_categorical_crossentropy</span></span><br><span class="line"><span class="comment"># y_train = np.array(train_labels)</span></span><br><span class="line"><span class="comment"># y_test = np.array(test_labels)</span></span><br><span class="line"></span><br><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"><span class="comment">#单标签，多类分类问题中，网络应以softmax激活结束，这样可以输出在N个类别上的概率分布。</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,      <span class="comment">#分类交叉熵</span></span><br><span class="line">              metrics=[<span class="string">'acc'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><table><thead><tr><th><img src="https://s1.ax1x.com/2020/05/21/YbAsqs.png" alt="训练损失和验证损失"></th><th align="center"><img src="https://s1.ax1x.com/2020/05/21/YbA6Zn.png" alt="训练精度和验证精度"></th></tr></thead></table><p>由上图可见，网络在训练9轮后开始过拟合，重新进行训练并进行评估。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">9</span>,</span><br><span class="line">          batch_size=<span class="number">512</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br><span class="line">results = model.evaluate(x_test, one_hot_test_labels)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">[<span class="number">0.9783820241875449</span>, <span class="number">0.7925200462341309</span>]    <span class="comment">#返回loss和acc</span></span><br></pre></td></tr></table></figure><p>在新数据上进行评估。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">predictions = model.predict(x_test)</span><br><span class="line">print(predictions[<span class="number">0</span>].shape)         <span class="comment">#(46,)</span></span><br><span class="line">print(np.sum(predictions[<span class="number">0</span>]))       <span class="comment">#1.0</span></span><br><span class="line">print(np.argmax(predictions[<span class="number">0</span>]))    <span class="comment">#概率最大的类别</span></span><br><span class="line"></span><br><span class="line">word_index = reuters.get_word_index()                                   <span class="comment">#将单词映射为整数索引的字典</span></span><br><span class="line">reverse_word_index = dict(                                           <span class="comment">#键值颠倒，整数索引映射为单词</span></span><br><span class="line">    [(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_data = <span class="string">' '</span>.join(                                           <span class="comment">#评论解码，索引减去3，</span></span><br><span class="line">    [reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> test_data[<span class="number">0</span>]])     <span class="comment">#因为0、1、2为保留索引</span></span><br><span class="line">print(decoded_data)</span><br><span class="line">print(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h3 id="3-7-预测房价：回归问题"><a href="#3-7-预测房价：回归问题" class="headerlink" title="3.7 预测房价：回归问题"></a>3.7 预测房价：回归问题</h3><p>前面两种分类问题目标是预测输入数据点所对应的单一离散的标签，而回归问题预测一个连续值而不是离散的标签，如根据气象数据预测明天的气温。<br>本例采用波士顿房价数据集，每个样本有多个数据特征，如犯罪率、每个住宅的平均房间数等。目标是房屋价格的中位数，单位千美元。<br>注意：</p><ul><li>本例的每个特征几乎都有不同的取值范围，应当对每个特征值做标准化，如减去特征平均值再除以标准差。</li><li>回归问题常用的损失函数为均方误差（MSE，mean squared error），预测值与目标值之差的平方。</li><li>回归问题使用的评估指标（metrics）为平均绝对误差（MAE，mean absolute error）预测值与目标值之差的绝对值。</li><li>若可用数据很少，使用K折验证可以可靠地评估模型。</li><li>若可用的训练数据很少，最好使用隐藏层较少（1到2个）的网络，避免严重的过拟合</li></ul><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#回归问题</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models, layers</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment">#特征值标准化</span></span><br><span class="line">mean = train_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">train_data -= mean</span><br><span class="line">std = train_data.std(axis=<span class="number">0</span>)</span><br><span class="line">train_data /= std</span><br><span class="line"></span><br><span class="line">mean1 = test_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">test_data -= mean1</span><br><span class="line">std1 = test_data.std(axis=<span class="number">0</span>)</span><br><span class="line">test_data /= std1</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为要将同一个模型多次实例化，故用一个函数来构建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                           input_shape=(train_data.shape[<span class="number">1</span>],)))</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))                                      <span class="comment">#线性层，标量回归</span></span><br><span class="line">    model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'mse'</span>, metrics=[<span class="string">'mae'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment">#K折验证</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line"><span class="comment"># num_epochs = 100</span></span><br><span class="line"><span class="comment"># all_scores = []</span></span><br><span class="line">num_epochs = <span class="number">500</span></span><br><span class="line">all_mae_histories = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">'processing fold #'</span>, i)</span><br><span class="line">    <span class="comment">#准备验证数据：第k个分区的数据</span></span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#准备训练数据：其他所有分区的数据</span></span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[:i * num_val_samples],</span><br><span class="line">         train_data[(i + <span class="number">1</span>) * num_val_samples:]],</span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[:i * num_val_samples],</span><br><span class="line">         train_targets[(i + <span class="number">1</span>) * num_val_samples:]],</span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    model = build_model()</span><br><span class="line">    <span class="comment"># model.fit(partial_train_data, partial_train_targets,</span></span><br><span class="line">    <span class="comment">#           epochs=num_epochs, batch_size=1, verbose=0)</span></span><br><span class="line">    <span class="comment"># val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)</span></span><br><span class="line">    <span class="comment"># all_scores.append(val_mae)</span></span><br><span class="line">    <span class="comment"># mean_scores = np.mean(all_scores)</span></span><br><span class="line"></span><br><span class="line">    history = model.fit(partial_train_data, partial_train_targets,</span><br><span class="line">                        validation_data=(val_data, val_targets),</span><br><span class="line">                        epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">    print(history.history.keys())</span><br><span class="line">    mae_history = history.history[<span class="string">'val_mae'</span>]</span><br><span class="line">    all_mae_histories.append(mae_history)</span><br><span class="line"></span><br><span class="line">average_mae_history = [</span><br><span class="line">    np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> all_mae_histories]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)]</span><br><span class="line"></span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(average_mae_history) + <span class="number">1</span>), average_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#将每个数据点替换为前面数据点的指数移动平均值，以得到光滑曲线。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">  smoothed_points = []</span><br><span class="line">  <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">    <span class="keyword">if</span> smoothed_points:</span><br><span class="line">      previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">      smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      smoothed_points.append(point)</span><br><span class="line">  <span class="keyword">return</span> smoothed_points</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除前10个数据点</span></span><br><span class="line">smooth_mae_history = smooth_curve(average_mae_history[<span class="number">10</span>:])</span><br><span class="line"></span><br><span class="line">plt.clf()</span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(smooth_mae_history) + <span class="number">1</span>), smooth_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><table><thead><tr><th><img src="https://s1.ax1x.com/2020/05/21/YqbxS0.png" alt="每轮的验证MAE"></th><th align="center"><img src="https://s1.ax1x.com/2020/05/21/YqqSyT.png" alt="重绘后的每轮的验证MAE"></th></tr></thead></table><p>由上图可知，验证MAE在80轮后不再显著降低，之后开始过拟合，调整参数在训练数据上训练参数。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model = build_model()</span><br><span class="line">model.fit(train_data, train_targets,</span><br><span class="line">          epochs=<span class="number">80</span>, batch_size=<span class="number">16</span>, verbose=<span class="number">0</span>)</span><br><span class="line">test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_mae_score</span><br><span class="line"><span class="number">2.6759588718414307</span></span><br></pre></td></tr></table></figure><h2 id="Chapter-4-机器学习基础"><a href="#Chapter-4-机器学习基础" class="headerlink" title="Chapter 4 机器学习基础"></a>Chapter 4 机器学习基础</h2><h3 id="4-1-机器学习的四个分支"><a href="#4-1-机器学习的四个分支" class="headerlink" title="4.1 机器学习的四个分支"></a>4.1 机器学习的四个分支</h3><h4 id="4-1-1-监督学习"><a href="#4-1-1-监督学习" class="headerlink" title="4.1.1 监督学习"></a>4.1.1 监督学习</h4><p>最常见的机器学习类型，给定一组样本（通常由人工标注），学习将输入数据映射到已知目标。主要包括分类和回归问题，如光学字符识别、语音识别、图像分类以及语言翻译。另外包括一些主要的变体如下：</p><ul><li>分类：对数据进行分类 。</li><li>回归：对数据进行拟合 。</li><li>序列生成：给定一张图像，预测描述图像的文字。</li><li>语法树预测：给定一个句子，预测其分解生成的语法树 。</li><li>目标检测：给定一张图像，在图中特定目标的周围画一个边界框。可表示为分类（给定多个边界框，对框内目标进行分类）或分类与回归联合问题（用向量回归来预测边界框的坐标）。</li><li>图像分割：给定一张图像，在特定物体上画一个像素级的掩模（mask）。</li></ul><h4 id="4-1-2-无监督学习"><a href="#4-1-2-无监督学习" class="headerlink" title="4.1.2 无监督学习"></a>4.1.2 无监督学习</h4><p>无监督学习是指在没有目标的情况下寻找输入数据的有趣变换，其目的在于数据可视化、数据压缩、数据去噪或更好地理解数据中的相关性。<br>具体的方法：</p><ul><li>降维（dimensionality reduction）</li><li>聚类（clustering）</li></ul><h4 id="4-1-3-自监督学习"><a href="#4-1-3-自监督学习" class="headerlink" title="4.1.3 自监督学习"></a>4.1.3 自监督学习</h4><p>自监督学习是没有人工标注的标签的监督学习，可以将它看作没有人工参与的监督学习。<br>主要的例子：</p><ul><li>自编码器：其生成的目标就是未经修改的输入。</li><li>给定视频中过去的帧来预测下一帧，或者给定文本中前面的词来预测下一个词。</li></ul><p>上述两个例子也属于时序监督学习：用未来的输入数据作为监督。</p><h4 id="4-1-4-强化学习"><a href="#4-1-4-强化学习" class="headerlink" title="4.1.4 强化学习"></a>4.1.4 强化学习</h4><h3 id="4-2-机器学习通用工作流程"><a href="#4-2-机器学习通用工作流程" class="headerlink" title="4.2 机器学习通用工作流程"></a>4.2 机器学习通用工作流程</h3><h4 id="4-2-1-定义问题，收集数据集"><a href="#4-2-1-定义问题，收集数据集" class="headerlink" title="4.2.1 定义问题，收集数据集"></a>4.2.1 定义问题，收集数据集</h4><h4 id="4-2-2-选择衡量成功的指标"><a href="#4-2-2-选择衡量成功的指标" class="headerlink" title="4.2.2 选择衡量成功的指标"></a>4.2.2 选择衡量成功的指标</h4><h4 id="4-2-3-确定评估方法"><a href="#4-2-3-确定评估方法" class="headerlink" title="4.2.3 确定评估方法"></a>4.2.3 确定评估方法</h4><h4 id="4-2-4-准备数据"><a href="#4-2-4-准备数据" class="headerlink" title="4.2.4 准备数据"></a>4.2.4 准备数据</h4><h4 id="4-2-5-优化模型"><a href="#4-2-5-优化模型" class="headerlink" title="4.2.5 优化模型"></a>4.2.5 优化模型</h4><h4 id="4-2-6-模型正则化与调节超参数"><a href="#4-2-6-模型正则化与调节超参数" class="headerlink" title="4.2.6 模型正则化与调节超参数"></a>4.2.6 模型正则化与调节超参数</h4><h2 id="Chapter-5-卷积神经网络"><a href="#Chapter-5-卷积神经网络" class="headerlink" title="Chapter 5 卷积神经网络"></a>Chapter 5 卷积神经网络</h2><p>一个简单的卷积神经网络示例。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#卷积神经网络</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models, layers</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">train_images = train_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">test_images = test_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># model.summary()</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(train_images, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test_acc</span><br><span class="line"><span class="number">0.9919000267982483</span></span><br></pre></td></tr></table></figure><h2 id="Windows下的环境搭建"><a href="#Windows下的环境搭建" class="headerlink" title="Windows下的环境搭建"></a><span id="jump">Windows下的环境搭建</span></h2><p>由于懒得装双系统，虚拟机先不说配置不够，搭起来感觉坑很多，遂直接在Windows系统下搭建。</p><h3 id="安装CUDA和cuDNN"><a href="#安装CUDA和cuDNN" class="headerlink" title="安装CUDA和cuDNN"></a>安装CUDA和cuDNN</h3><blockquote><p>CUDA（Compute Unified Device Architecture）：NVIDIA用于自家GPU的并行计算框架，本质是一个工具包（ToolKit）。</p></blockquote><blockquote><p>cuDNN（CUDA Deep Neural Network library）：是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。用GPU训练模型，cuDNN不是必须的，但是一般会采用这个加速库。</p></blockquote><ol><li><p>查看显卡支持的CUDA版本<br>NVIDIA控制面板&gt;帮助&gt;系统信息&gt;组件，如图支持10.2版本<br><img src="https://s1.ax1x.com/2020/05/17/Y20vQK.png" alt=""></p></li><li><p>安装对应版本的CUDA</p><blockquote><p>下载地址：<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-downloads</a></p></blockquote><blockquote><p>安装完成后打开cmd，输入命令：<code>nvcc -V</code>，如图</p><img src="https://s1.ax1x.com/2020/05/17/Y2yrUe.png" style="margin-left:0" alt=""></blockquote></li><li><p>编译CUDA（暂时未编译）</p></li><li><p>安装cuDNN</p><blockquote><p>下载地址：<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a></p></blockquote><blockquote><p>这里需要注册账户才可以下载，可以看到支持的对应CUDA版本<br>将解压后的文件复制到CUDA安装路径中，这里是默认路径<br><code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1</code></p></blockquote></li></ol><h3 id="安装TensorFlow和Keras"><a href="#安装TensorFlow和Keras" class="headerlink" title="安装TensorFlow和Keras"></a>安装TensorFlow和Keras</h3><p>由于本机已经安装过Python3.7，管理多版本Python有点麻烦，故直接<strong>使用pip安装</strong>而不是Anaconda<br>在用pip安装前，<strong>确保pypi源更换为国内镜像</strong>，否则速度超级慢</p><blockquote><p>方法为在C盘的user目录下新建一个pip文件夹，如：C:\Users\xx\pip，在pip文件夹内新建一个pip.ini文件，内容如下：<br></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn</span><br></pre></td></tr></table></figure><p></p></blockquote><blockquote><p>常用国内镜像：<br></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple&#x2F;   # 清华大学</span><br><span class="line">https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;     # 阿里云</span><br><span class="line">https:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;             # 豆瓣</span><br><span class="line">https:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple&#x2F;    # 中国科学技术大学</span><br><span class="line">https:&#x2F;&#x2F;pypi.hustunique.com&#x2F;                # 华中科技大学</span><br></pre></td></tr></table></figure><p></p></blockquote><ol><li>安装Tensorflow<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">pip install tensorflow==<span class="number">2.1</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">pip install tensorflow-gpu==<span class="number">2.1</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>安装完成后在python环境中运行<figure class="highlight elm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">tf</span><span class="selector-class">.test</span><span class="selector-class">.is_gpu_available</span>()</span><br></pre></td></tr></table></figure>可以查看到GPU信息即安装成功。<br>Tensorflow作为Keras的后端,TensorFlow与CUDA和cuDNN各版本对应如下：<br><img src="https://s1.ax1x.com/2020/05/17/Y24DV1.png" alt=""></li><li>安装Keras<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">pip <span class="keyword">install</span> keras</span><br></pre></td></tr></table></figure></li></ol><h3 id="安装其他组件"><a href="#安装其他组件" class="headerlink" title="安装其他组件"></a>安装其他组件</h3><h4 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h4><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">python -m pip <span class="keyword">install</span> matplotlib</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pythonic Coding</title>
    <url>/post/Python/effective-python/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>《Effective Python》学习笔记<a id="more"></a></p><h2 id="程序风格"><a href="#程序风格" class="headerlink" title="程序风格"></a><center>程序风格</center></h2><h3 id="遵循PEP8风格指南"><a href="#遵循PEP8风格指南" class="headerlink" title="遵循PEP8风格指南"></a>遵循PEP8风格指南</h3><p><a href="https://www.python.org/dev/peps/pep-0008" target="_blank" rel="noopener">《Python Enhancement Proposal #8》</a>，简称PEP8，是针对Python代码格式编订的风格指南。</p><h3 id="bytes、str与unicode区别"><a href="#bytes、str与unicode区别" class="headerlink" title="bytes、str与unicode区别"></a>bytes、str与unicode区别</h3><h4 id="首先区分Python3与Python2的两种表示字符序列的类型"><a href="#首先区分Python3与Python2的两种表示字符序列的类型" class="headerlink" title="首先区分Python3与Python2的两种表示字符序列的类型"></a>首先区分Python3与Python2的两种表示字符序列的类型</h4><ul><li>Python3，bytes与str，前者的实例包含原始的8位值，即原始的字节，包含8个二进制位；后者的实例包含Unicode字符</li><li>Python2，str与unicode，前者的实例包含原始的8位值；后者的实例包含Unicode字符</li></ul><h4 id="二进制数据与Unicode字符相互转换"><a href="#二进制数据与Unicode字符相互转换" class="headerlink" title="二进制数据与Unicode字符相互转换"></a>二进制数据与Unicode字符相互转换</h4><p>常见编码方式为UTF-8<br>Unicode字符 → 二进制数据，encode()方法<br>二进制数据 → Unicode字符，decode()方法</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_bytes</span><span class="params">(bytes_or_str)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(bytes_or_str, str):</span><br><span class="line">        value = bytes_or_str.encode(<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        value = bytes_or_str</span><br><span class="line">    <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_str</span><span class="params">(bytes_or_str)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(bytes_or_str, bytes):</span><br><span class="line">        value = bytes_or_str.decode(<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        value = bytes_or_str</span><br><span class="line">    <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure><p>注：</p><ul><li>Python程序中，编码和解码操作放在程序外围，核心部分使用Unicode字符类型</li><li>在只处理7位ASCII时，Python2的str和unicode类型的实例可以等价，而Python3中bytes与str的实例绝对不等价</li><li>Python3中，使用内置open()函数获取文件句柄，该句柄默认采用UTF-8格式来操作文件，问题在于Python3给open()函数添加了名为encoding的新参数，其默认值为’utf-8’，要求必须传入包含Unicode字符的str实例，而不接受包含二进制数据的bytes实例<br>总结为必须使用二进制写入模式open(path, ‘wb’)来开启待操作文件</li></ul><h3 id="用辅助函数取代复杂表达式"><a href="#用辅助函数取代复杂表达式" class="headerlink" title="用辅助函数取代复杂表达式"></a>用辅助函数取代复杂表达式</h3><p>如，从字典中查询并返回得到的第一个整数值:<br><code>red = my_values.get(&#39;red&#39;, [&#39;&#39;])[0] or 0</code><br>未查询到或值为0为空统一返回0，该表达式不易理解，若要频繁使用，将其总结为辅助函数：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_first_int</span><span class="params">(values, key, default=<span class="number">0</span>)</span>:</span></span><br><span class="line">    found =values.get(key, [<span class="string">''</span>])</span><br><span class="line">    <span class="keyword">if</span> found[<span class="number">0</span>]:</span><br><span class="line">        found = int(found[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        found = default</span><br><span class="line">    <span class="keyword">return</span> found</span><br><span class="line"></span><br><span class="line">red = get_first_int(my_values, <span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><h3 id="切片操作"><a href="#切片操作" class="headerlink" title="切片操作"></a>切片操作</h3><h4 id="基本写法"><a href="#基本写法" class="headerlink" title="基本写法"></a>基本写法</h4><p>somelist[start:end]，其中start所指元素涵盖在切割后的范围内，end所指元素不包括在切割结果之中。例：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#start从0开始，end倒数从-1开始</span></span><br><span class="line">a[:]                                    <span class="comment">#[1,2,3,4,5,6]</span></span><br><span class="line">a[:<span class="number">3</span>]                                   <span class="comment">#[1,2,3]</span></span><br><span class="line">a[<span class="number">2</span>:]                                   <span class="comment">#[3,4,5,6]</span></span><br><span class="line">a[<span class="number">2</span>:<span class="number">5</span>]                                  <span class="comment">#[3,4,5]</span></span><br><span class="line">a[<span class="number">-3</span>:<span class="number">-1</span>]                                <span class="comment">#[4,5]</span></span><br></pre></td></tr></table></figure><p>切割列表时，start和end越界不会出问题，利用该特性可以限定输入序列的最大长度。</p><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">first_nine_items</span> = a[:<span class="number">9</span>]</span><br><span class="line"><span class="attr">last_nine_items</span> = a[<span class="number">9</span>:]</span><br></pre></td></tr></table></figure><p>切片后不影响原列表，对list赋值，若使用切片操作，会把原列表处在相关范围内的值替换为新值，即便长度不同也可以替换。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a[<span class="number">2</span>:<span class="number">5</span>] = [<span class="number">1</span>,<span class="number">1</span>]                          <span class="comment">#[1,2,1,1,6]</span></span><br><span class="line">a[:] = [<span class="number">1</span>,<span class="number">1</span>]                            <span class="comment">#[1,1]</span></span><br></pre></td></tr></table></figure><h4 id="步进式切割"><a href="#步进式切割" class="headerlink" title="步进式切割"></a>步进式切割</h4><p>somelist[start:end:stride]</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">odds = a[::<span class="number">2</span>]</span><br><span class="line">evens = a[<span class="number">1</span>::<span class="number">2</span>]</span><br><span class="line">b = <span class="string">b'abc'</span></span><br><span class="line">reverse = b[::<span class="number">-1</span>]                       <span class="comment">#负值为反向步进</span></span><br></pre></td></tr></table></figure><p>注：</p><ul><li>负步长只对字节串和ASCII字符有效，对已编码成UTF-8字节串的Unicode字符无效</li><li>尽量使用stride为正数，且不带start和end索引</li><li>同一切片操作内，不要同时指定start、end和stride，考虑将其拆解为一条步进切割，一条范围切割</li></ul><h3 id="列表与迭代"><a href="#列表与迭代" class="headerlink" title="列表与迭代"></a>列表与迭代</h3><h4 id="用列表推导取代map和filter"><a href="#用列表推导取代map和filter" class="headerlink" title="用列表推导取代map和filter"></a>用列表推导取代map和filter</h4><p><strong>列表推导</strong>(list comprehension)，根据一份列表来制作另外一份。<br><strong>字典</strong>(dict)与<strong>集</strong>(set)也支持推导表达式</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用map，创建lambda函数，结合filter</span></span><br><span class="line">squares = map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>, a)</span><br><span class="line">even_squares = map(<span class="keyword">lambda</span> x: x**<span class="number">2</span>, filter(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>, a))</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用列表推导</span></span><br><span class="line">squares = [x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> a]</span><br><span class="line">even_squares = [x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> a <span class="keyword">if</span> x % <span class="number">2</span> == <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="列表推导内含的表达式不宜超过两个"><a href="#列表推导内含的表达式不宜超过两个" class="headerlink" title="列表推导内含的表达式不宜超过两个"></a>列表推导内含的表达式不宜超过两个</h4><p>列表推导支持多重循环</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">matrix = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#矩阵简化为一维列表</span></span><br><span class="line">flat = [x <span class="keyword">for</span> row <span class="keyword">in</span> matrix <span class="keyword">for</span> x <span class="keyword">in</span> row]                  <span class="comment">#[1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line"></span><br><span class="line">squared = [[x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> row] <span class="keyword">for</span> row <span class="keyword">in</span> matrix]          <span class="comment">#[[1, 4, 9], [16, 25, 36], [49, 64, 81]]</span></span><br></pre></td></tr></table></figure><p>每一级循环也支持多重条件</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#处在同一循环级别中的多项条件， 彼此之间默认形成and表达式</span></span><br><span class="line">b = [x <span class="keyword">for</span> x <span class="keyword">in</span> a <span class="keyword">if</span> x &gt; <span class="number">4</span> <span class="keyword">if</span> x % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line">b = [x <span class="keyword">for</span> x <span class="keyword">in</span> a <span class="keyword">if</span> x &gt; <span class="number">4</span> <span class="keyword">and</span> x % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#从矩阵中取出本身能被3整除，且其所在行所有元素之和大于等于10的元素</span></span><br><span class="line">filtered = [[x <span class="keyword">for</span> x <span class="keyword">in</span> row <span class="keyword">if</span> x % <span class="number">3</span> == <span class="number">0</span>] </span><br><span class="line">            <span class="keyword">for</span> row <span class="keyword">in</span> matrix <span class="keyword">if</span> sum(row) &gt;= <span class="number">10</span>]</span><br></pre></td></tr></table></figure><h4 id="用生成器表达式改写数据量较大的列表推导"><a href="#用生成器表达式改写数据量较大的列表推导" class="headerlink" title="用生成器表达式改写数据量较大的列表推导"></a>用生成器表达式改写数据量较大的列表推导</h4><p>首先，列表推导的缺点是：<br>在推导过程中，对于输入序列中的每个值，可能都要创建一个仅含一个元素的新列表，若输入数据量较大，会消耗大量内存。<br>如，读取一份文件并返回每行的字符数，采用列表推导</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">value = [len(x) <span class="keyword">for</span> x <span class="keyword">in</span> open(file)]</span><br></pre></td></tr></table></figure><p><strong>生成器表达式</strong>(generator expression)：<br>对列表推导和生成器的一种<strong>泛化</strong>(generalization)，生成器表达式运行时，不会呈现整个输出序列，而是估值为<strong>迭代器</strong>(iterator)，该迭代器每次根据生成器表达式产生一项数据。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#生成器表达式，立刻返回一个迭代器</span></span><br><span class="line">it = (len(x) <span class="keyword">for</span> x <span class="keyword">in</span> open(file))</span><br><span class="line">next(it)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成器表达式可以互相组合</span></span><br><span class="line">roots = ((x, x**<span class="number">0.5</span>) <span class="keyword">for</span> x <span class="keyword">in</span> it)</span><br></pre></td></tr></table></figure><h4 id="用enumerate取代range"><a href="#用enumerate取代range" class="headerlink" title="用enumerate取代range"></a>用enumerate取代range</h4><p>enumerate可以把各种迭代器包装为生成器，可以在遍历迭代器时获得每个元素的索引，在同时需要下标和值的时候使用。如range</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">list_num = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(list_num)):</span><br><span class="line">    num = list_num[i]</span><br><span class="line">    print(<span class="string">'%d:%s'</span>, %(i+<span class="number">1</span>, num))</span><br></pre></td></tr></table></figure><p>使用enumerate</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">list_num = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br><span class="line"><span class="keyword">for</span> i, num <span class="keyword">in</span> enumerator(list_num, <span class="number">1</span>):                  <span class="comment">#起始下标指定为1</span></span><br><span class="line">    print(<span class="string">'%d:%s'</span>, %(i, num))</span><br></pre></td></tr></table></figure><h4 id="用zip函数同时遍历两个迭代器"><a href="#用zip函数同时遍历两个迭代器" class="headerlink" title="用zip函数同时遍历两个迭代器"></a>用zip函数同时遍历两个迭代器</h4><p>Python3中的zip函数，能将两个及以上的迭代器封装为生成器，在遍历过程中逐次产生元组。<br>Python2中，直接产生所有元组，并一次性返回整份列表<br>若提供的迭代器长度不等，zip会提前自动终止</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">names = [<span class="string">'adad'</span>, <span class="string">'bob'</span>, <span class="string">'alen'</span>]</span><br><span class="line">letters = [len(x) <span class="keyword">for</span> x <span class="keyword">in</span> names]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, letter <span class="keyword">in</span> zip(names, letters):</span><br><span class="line">    print(name+str(letter))</span><br></pre></td></tr></table></figure><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a><center>函数</center></h2><p>函数的问题主要体现在<strong>参数</strong>、<strong>作用域</strong>、<strong>返回值</strong>三个方面</p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h3 id="作用域"><a href="#作用域" class="headerlink" title="作用域"></a>作用域</h3><h4 id="在闭包里使用外围作用域中的变量"><a href="#在闭包里使用外围作用域中的变量" class="headerlink" title="在闭包里使用外围作用域中的变量"></a>在闭包里使用外围作用域中的变量</h4><p>注意：</p><ul><li>Python支持闭包（closure）：闭包是定义在某个作用域中的函数，可以引用那个作用域中的变量。</li><li>Python的函数时一级对象，可以直接引用函数、将函数赋给变量或者将函数作为参数传递。</li></ul><p>例如，对一份数字列表进行排序，要求出现的特定数字在其他数字排序之前。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sort_priority</span><span class="params">(values, groups)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> x <span class="keyword">in</span> group:</span><br><span class="line">            <span class="keyword">return</span> (<span class="number">0</span>, x)</span><br><span class="line">        <span class="keyword">return</span>(<span class="number">1</span>, x)</span><br><span class="line">    values.sort(key=helper)</span><br><span class="line"></span><br><span class="line">numbers = [<span class="number">18</span>, <span class="number">9</span>, <span class="number">89</span>, <span class="number">66</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">23</span>]</span><br><span class="line">group = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">66</span>]</span><br><span class="line">sort_priority(numbers, group)</span><br><span class="line">print(numbers)</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">66</span>, <span class="number">9</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">89</span>]</span><br></pre></td></tr></table></figure><p>稍作修改为</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sort_priority2</span><span class="params">(values, groups)</span>:</span></span><br><span class="line">    found = <span class="literal">False</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> x <span class="keyword">in</span> group:</span><br><span class="line">            found = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> (<span class="number">0</span>, x)</span><br><span class="line">        <span class="keyword">return</span>(<span class="number">1</span>, x)</span><br><span class="line">    values.sort(key=helper)</span><br><span class="line">    <span class="keyword">return</span> found</span><br><span class="line"></span><br><span class="line">numbers = [<span class="number">18</span>, <span class="number">9</span>, <span class="number">89</span>, <span class="number">66</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">23</span>]</span><br><span class="line">group = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">66</span>]</span><br><span class="line">found = sort_priority2(numbers, group)</span><br><span class="line">print(found)</span><br><span class="line">print(numbers)</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">66</span>, <span class="number">9</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">89</span>]</span><br></pre></td></tr></table></figure><p>found的输出结果与预期的True不符，这里我们首先要了解，在表达式中引用变量，Python解释器遵循以下顺序遍历各作用域：</p><ul><li>当前函数作用域</li><li>任何外围作用域（如包含当前函数的其他函数）</li><li>包含当前代码的那个模块的作用域（全局作用域，global scope）</li><li>内置作用域（包含str及len等函数的那个作用域）</li><li>未定义过名称相符变量，抛出NameError异常</li></ul><p>sort_priority2函数中将found赋值为True是在闭包函数helper内进行的，实则是在闭包函数的作用域中定义了一个found变量并赋值为True，与其外围函数sort_priority2作用域中定义的found不同，最后返回的是sort_priority2中赋值为False的found变量。<br>因此，我们需要获取闭包内的数据，可使用nonlocal语句，不支持Python2</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sort_priority2</span><span class="params">(values, groups)</span>:</span></span><br><span class="line">    found = <span class="literal">False</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">nonlocal</span> found                  <span class="comment">#声明该found为闭包外围作用域中的found</span></span><br><span class="line">        <span class="keyword">if</span> x <span class="keyword">in</span> group:</span><br><span class="line">            found = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> (<span class="number">0</span>, x)</span><br><span class="line">        <span class="keyword">return</span>(<span class="number">1</span>, x)</span><br><span class="line">    values.sort(key=helper)</span><br><span class="line">    <span class="keyword">return</span> found</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line"><span class="literal">True</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">66</span>, <span class="number">9</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">89</span>]</span><br></pre></td></tr></table></figure><p>而实际开发中，nonlocal容易遭到滥用，且副作用难以追踪，难以理解，不适用于较长较复杂的函数。有以下两个解决办法：</p><ol><li>将相关状态封装为辅助类<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sorter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, group)</span>:</span></span><br><span class="line">        self.group = group</span><br><span class="line">        self.found = <span class="literal">False</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> x <span class="keyword">in</span> self.group:</span><br><span class="line">            self.found = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> (<span class="number">0</span>, x)</span><br><span class="line">        <span class="keyword">return</span>(<span class="number">1</span>, x)</span><br><span class="line"></span><br><span class="line">numbers = [<span class="number">18</span>, <span class="number">9</span>, <span class="number">89</span>, <span class="number">66</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">23</span>]</span><br><span class="line">group = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">66</span>]</span><br><span class="line">sorter = Sorter(group)</span><br><span class="line">numbers.sort(key=sorter)</span><br><span class="line">print(sorter.found)</span><br></pre></td></tr></table></figure></li><li>用Python的作用域规则，Python2可用<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sort_priority2</span><span class="params">(values, groups)</span>:</span></span><br><span class="line">    found = [<span class="literal">False</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> x <span class="keyword">in</span> group:</span><br><span class="line">            found[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> (<span class="number">0</span>, x)</span><br><span class="line">        <span class="keyword">return</span>(<span class="number">1</span>, x)</span><br><span class="line">    values.sort(key=helper)</span><br><span class="line">    <span class="keyword">return</span> found[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h3><h4 id="尽量用异常表示特殊情况，而非返回None"><a href="#尽量用异常表示特殊情况，而非返回None" class="headerlink" title="尽量用异常表示特殊情况，而非返回None"></a>尽量用异常表示特殊情况，而非返回None</h4>例如，两数相除的情况。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divide</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> a/b</span><br><span class="line">    <span class="keyword">except</span> ZeroDivisionError:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">result = divide(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">if</span> result <span class="keyword">is</span> <span class="literal">None</span>:                              <span class="comment">#这种情况没有问题</span></span><br><span class="line">    print(<span class="string">'Invalid inputs'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> result:                                  <span class="comment">#错误情况</span></span><br><span class="line">    print(<span class="string">'Invalid inputs'</span>)</span><br></pre></td></tr></table></figure>当分子为0时not result结果为True，结果应为0，却显示Invalid inputs。用异常来表示这种情况：<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divide</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> a/b</span><br><span class="line">    <span class="keyword">except</span> ZeroDivisionError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid inputs'</span>) <span class="keyword">from</span> e</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = divide(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">except</span> ValueError:</span><br><span class="line">    print(<span class="string">'Invalid inputs'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Result is %.1f'</span> % result)</span><br></pre></td></tr></table></figure><h4 id="用生成器改写直接返回列表的函数"><a href="#用生成器改写直接返回列表的函数" class="headerlink" title="用生成器改写直接返回列表的函数"></a>用生成器改写直接返回列表的函数</h4>若函数产生一系列结果，最简单的方法是返回一个包含所有结果的列表，此方法主要有两个缺点：</li></ol><ul><li>代码拥挤，不清晰</li><li>返回前将所有结果放在列表里，若输入量非常大，会导致内存耗尽</li></ul><p>例如以下函数返回字符串中每个单词首字母的位置。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_words</span><span class="params">(text)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">if</span> text:</span><br><span class="line">        result.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> index, letter <span class="keyword">in</span> enumerate(text):</span><br><span class="line">        <span class="keyword">if</span> letter == <span class="string">' '</span>:</span><br><span class="line">            result.append(index+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">text = <span class="string">'seven years ago'</span></span><br><span class="line">result = index_words(text)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">[<span class="number">0</span>, <span class="number">6</span>, <span class="number">12</span>]</span><br></pre></td></tr></table></figure><p>用生成器改写：</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_words</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> text:</span><br><span class="line">        <span class="keyword">yield</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> index, letter <span class="keyword">in</span> enumerate(text):</span><br><span class="line">        <span class="keyword">if</span> letter == <span class="string">' '</span>:</span><br><span class="line">            <span class="keyword">yield</span> index+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">text = <span class="string">'seven years ago'</span></span><br><span class="line">result = list(index_words(text))</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Python自动化控制鼠标和键盘</title>
    <url>/post/Python/auto-gui-with-python/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>文档：<a href="https://pyautogui.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://pyautogui.readthedocs.io/en/latest/</a><a id="more"></a></p><h2 id="安装pyautogui模块"><a href="#安装pyautogui模块" class="headerlink" title="安装pyautogui模块"></a>安装pyautogui模块</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pip install pyautogui</span><br></pre></td></tr></table></figure><h2 id="鼠标操作模拟"><a href="#鼠标操作模拟" class="headerlink" title="鼠标操作模拟"></a>鼠标操作模拟</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyautogui</span><br><span class="line"></span><br><span class="line">width, height = pyautogui.size()            <span class="comment">#返回屏幕宽高像素数的元组</span></span><br><span class="line">pyautogui.position()                        <span class="comment">#返回鼠标当前位置元组</span></span><br><span class="line"></span><br><span class="line">pyautogui.moveTo(<span class="number">100</span>, <span class="number">200</span>, duration=<span class="number">1.5</span>)    <span class="comment">#绝对位置移动，参数为x,y,time可选秒数</span></span><br><span class="line">pyautogui.moveRel(<span class="number">100</span>, <span class="number">-1</span>, duration=<span class="number">1</span>)      <span class="comment">#相对位置移动。参数为右,左,时间可选秒数</span></span><br><span class="line"></span><br><span class="line">pyautogui.click(<span class="number">100</span>, <span class="number">200</span>, button=<span class="string">'middle'</span>)    <span class="comment">#完整的单击.默认鼠标左键单击当前位置</span></span><br><span class="line">pyautogui.mouseDown(button=left)            <span class="comment">#按下鼠标按键</span></span><br><span class="line">pyautogui.mouseUp(<span class="number">100</span>, <span class="number">200</span>)                 <span class="comment">#松开鼠标按键</span></span><br><span class="line">pyautogui.doubleClick()                     <span class="comment">#双击鼠标左键</span></span><br><span class="line">pyautogui.middleClick()                     <span class="comment">#单击鼠标中键</span></span><br><span class="line"></span><br><span class="line">pyautogui.dragTo(<span class="number">100</span>, <span class="number">200</span>, duration=<span class="number">1.5</span>)    <span class="comment">#绝对位置拖动，参数为x,y,time可选秒数</span></span><br><span class="line">pyautogui.dragRel(<span class="number">100</span>, <span class="number">-1</span>, button=<span class="string">'middle'</span>) <span class="comment">#相对位置拖动。参数为右,左,button可选默认左键</span></span><br><span class="line"></span><br><span class="line">pyautogui.scroll(<span class="number">-200</span>)                      <span class="comment">#鼠标滚轮上下滚动</span></span><br></pre></td></tr></table></figure><h2 id="处理屏幕"><a href="#处理屏幕" class="headerlink" title="处理屏幕"></a>处理屏幕</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyautogui</span><br><span class="line"></span><br><span class="line">im = pyautogui.screenshot()                 <span class="comment">#返回包含屏幕快照的Image对象</span></span><br><span class="line">im.getpixel((x,y))                          <span class="comment">#返回坐标处像素颜色的RGB元组</span></span><br><span class="line">pyautogui.pixelMatchesColor(x, y, (R, G, B))<span class="comment">#比较(x,y)处与RGB的颜色是否相同</span></span><br><span class="line"></span><br><span class="line">pyautogui.locateOnScreen(<span class="string">'screenshot.png'</span>)  <span class="comment">#参数或为region=(0,0, 300, 400)</span></span><br><span class="line"><span class="comment">#返回screenshot.png匹配当前屏幕成功的图像左边的x坐标，顶边的y坐标，宽度和高度元组</span></span><br><span class="line">pyautogui.center((<span class="number">0</span>,<span class="number">0</span>, <span class="number">300</span>, <span class="number">400</span>))           <span class="comment">#返回中心坐标</span></span><br><span class="line">pyautogui.click((<span class="number">150</span>, <span class="number">200</span>))</span><br></pre></td></tr></table></figure><h2 id="键盘操作模拟"><a href="#键盘操作模拟" class="headerlink" title="键盘操作模拟"></a>键盘操作模拟</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyautogui</span><br><span class="line"></span><br><span class="line">pyautogui.typewrite(<span class="string">'test'</span>,<span class="number">0.25</span>)            <span class="comment">#发送单个字符表示的虚拟按键,间隔0.25秒</span></span><br><span class="line">pyautogui.typewrite([<span class="string">'a'</span>, <span class="string">'b'</span>])               <span class="comment">#键名作为参数</span></span><br><span class="line"></span><br><span class="line">pyautogui.keyDown(<span class="string">'ctrl'</span>)</span><br><span class="line">pyautogui.press(<span class="string">'c'</span>)</span><br><span class="line">pyautogui.keyUp(<span class="string">'ctrl'</span>)</span><br><span class="line"><span class="comment">#等价于</span></span><br><span class="line">pyautogui.hotkey(<span class="string">'ctrl'</span>, <span class="string">'c'</span>)</span><br></pre></td></tr></table></figure><p>键名:<a href="https://pyautogui.readthedocs.io/en/latest/keyboard.html#keyboard-keys" target="_blank" rel="noopener">https://pyautogui.readthedocs.io/en/latest/keyboard.html#keyboard-keys</a></p>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Auto-GUI</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Python处理Excel，CSV和JSON</title>
    <url>/post/Python/excel-csv-json-with-python/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>使用Python处理Excel，CSV文件和JSON文件<a id="more"></a></p><h2 id="python处理excel"><a href="#python处理excel" class="headerlink" title="python处理excel"></a>python处理excel</h2><p>文档：<a href="https://openpyxl.readthedocs.io/en/stable/" target="_blank" rel="noopener">https://openpyxl.readthedocs.io/en/stable/</a></p><h3 id="安装openpyxl模块"><a href="#安装openpyxl模块" class="headerlink" title="安装openpyxl模块"></a>安装openpyxl模块</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">pip <span class="keyword">install</span> openpyxl</span><br></pre></td></tr></table></figure><h3 id="读取excel"><a href="#读取excel" class="headerlink" title="读取excel"></a>读取excel</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> openpyxl</span><br><span class="line"></span><br><span class="line">wb = openpyxl.load_workbook(<span class="string">'C:\\Users\\test.xlsx'</span>)     <span class="comment">#返回Workbook对象</span></span><br><span class="line">wb.sheetnames                                           <span class="comment">#返回所有工作表名称的字符串列表</span></span><br><span class="line"></span><br><span class="line">sheet = wb[<span class="string">'Sheet'</span>]                                     <span class="comment">#wb.get_sheet_by_name('Sheet')</span></span><br><span class="line">sheet1 = wb.active</span><br><span class="line"></span><br><span class="line">sheet[<span class="string">'A'</span>]                                              <span class="comment">#返回第一列</span></span><br><span class="line">sheet[<span class="string">'A1'</span>].value                                       <span class="comment">#.row .column .coordinate(r+c)</span></span><br><span class="line">sheet.cell(row=<span class="number">1</span>, column=<span class="number">1</span>).value</span><br><span class="line">sheet.max_row                                           <span class="comment">#返回一个整数.max_column</span></span><br><span class="line"></span><br><span class="line">openpyxl.cell.column_index_from_string()                <span class="comment">#列字母转换为数字，A→1</span></span><br><span class="line">openpyxl.cell.get_column_letter()                       <span class="comment">#数字转换为列字母</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#切片，遍历</span></span><br><span class="line"><span class="keyword">for</span> rowObj <span class="keyword">in</span> sheet[<span class="string">'A1'</span>:<span class="string">'D14'</span>]:                        <span class="comment">#每个元组代表一行，按行遍历整个区域</span></span><br><span class="line">    <span class="keyword">for</span> cellObj <span class="keyword">in</span> rowObj:                              <span class="comment">#遍历行中的每个单元格</span></span><br><span class="line">        print(cellObj.coordinate, cellObj.value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> cellObj <span class="keyword">in</span> list(sheet.rows)[<span class="number">0</span>]:                     <span class="comment">#按行按列遍历,或者 sheet[1] | sheet['A']</span></span><br><span class="line">    print(cellObj.value)</span><br></pre></td></tr></table></figure><h3 id="写入excel"><a href="#写入excel" class="headerlink" title="写入excel"></a>写入excel</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> openpyxl</span><br><span class="line"></span><br><span class="line">wb = openpyxl.Workbook()                                <span class="comment">#新建一个空的Workbook对象</span></span><br><span class="line">sheet = wb.active</span><br><span class="line">sheet.title = <span class="string">'test'</span></span><br><span class="line">sheet[<span class="string">'A1'</span>] = <span class="string">'test'</span>                                    <span class="comment">#将值写入单元格</span></span><br><span class="line"></span><br><span class="line">wb.create_sheet(index=<span class="number">0</span>, title=<span class="string">'ttest'</span>)                 <span class="comment">#新建一个工作表，返回Worksheet对象</span></span><br><span class="line">wb.remove(wb[<span class="string">'Sheet'</span>])                                  <span class="comment">#接受一个Worksheet对象作为其参数</span></span><br><span class="line">wb.save(<span class="string">'C:\\Users\\test.xlsx'</span>)                         <span class="comment">#保存变更，不同名或不同位置会创建副本</span></span><br></pre></td></tr></table></figure><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><h4 id="更新excel表"><a href="#更新excel表" class="headerlink" title="更新excel表"></a>更新excel表</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> openpyxl</span><br><span class="line"></span><br><span class="line">update_data = &#123;<span class="string">'apple'</span>:<span class="number">5</span>,</span><br><span class="line">               <span class="string">'lemon'</span>:<span class="number">1</span>,</span><br><span class="line">               <span class="string">'orange'</span>:<span class="number">2</span>&#125;                              <span class="comment">#待更新数据保存在字典中</span></span><br><span class="line">wb = openpyxl.load_workbook(<span class="string">'C:\\Users\\test.xlsx'</span>)</span><br><span class="line">sheet = wb[<span class="string">'Sheet'</span>]</span><br><span class="line"><span class="keyword">for</span> rowNum <span class="keyword">in</span> range(<span class="number">2</span>, sheet.max_row+<span class="number">1</span>):</span><br><span class="line">    updatedName = sheet.cell(row=rowNum, column=<span class="number">1</span>).value</span><br><span class="line">    <span class="keyword">if</span> updatedName <span class="keyword">in</span> update_data:</span><br><span class="line">        sheet.cell(row=rowNum, column=<span class="number">2</span>).value = update_data[updatedName]</span><br><span class="line">wb.save(<span class="string">'C:\\Users\\test_updated.xlsx'</span>)</span><br></pre></td></tr></table></figure><h4 id="字体风格"><a href="#字体风格" class="headerlink" title="字体风格"></a>字体风格</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> openpyxl.styles <span class="keyword">import</span> Font</span><br><span class="line">fontObj = Font(name=<span class="string">'Calibri'</span>, size=<span class="number">12</span>, bold=true, italic=true)</span><br><span class="line">sheet[<span class="string">'A1'</span>].font = fontObj</span><br></pre></td></tr></table></figure><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">sheet.row_dimensions[<span class="number">1</span>].height = <span class="number">0</span><span class="number">-409</span></span><br><span class="line">sheet.column_dimensions[<span class="string">'A'</span>].width = <span class="number">0</span><span class="number">-255</span></span><br></pre></td></tr></table></figure><h4 id="行高列宽"><a href="#行高列宽" class="headerlink" title="行高列宽"></a>行高列宽</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">sheet.row_dimensions[<span class="number">1</span>].height = <span class="number">0</span><span class="number">-409</span></span><br><span class="line">sheet.column_dimensions[<span class="string">'A'</span>].width = <span class="number">0</span><span class="number">-255</span></span><br></pre></td></tr></table></figure><h4 id="合并拆分冻结"><a href="#合并拆分冻结" class="headerlink" title="合并拆分冻结"></a>合并拆分冻结</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">sheet.merge_cells(<span class="string">'A1:D3'</span>)</span><br><span class="line">sheet[<span class="string">'A1'</span>] = <span class="string">'merged'</span>                                    <span class="comment">#合并后单元格的值</span></span><br><span class="line"></span><br><span class="line">sheet.unmerge_cells(<span class="string">'A1:A2'</span>)</span><br><span class="line"></span><br><span class="line">sheet.freeze_panes = <span class="string">'A2'</span>                                 <span class="comment">#冻结左上部分</span></span><br></pre></td></tr></table></figure><h2 id="python处理CSV文件"><a href="#python处理CSV文件" class="headerlink" title="python处理CSV文件"></a>python处理CSV文件</h2><h3 id="CSV文件"><a href="#CSV文件" class="headerlink" title="CSV文件"></a>CSV文件</h3><p>csv全称’Comma-Separated Values‘，每行对应电子表格中的一行，单元格之间用逗号分隔。</p><ul><li>值没有类型，全为字符串</li><li>无多个工作表</li><li>不能嵌入图像或图表</li><li>无字体大小和颜色设置</li><li>无法操作单元格，宽高合并等</li></ul><h3 id="Reader对象"><a href="#Reader对象" class="headerlink" title="Reader对象"></a>Reader对象</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">openFile = open(<span class="string">'test.csv'</span>)                               <span class="comment">#返回一个File对象</span></span><br><span class="line">fileReader = csv.reader(openFile)                         <span class="comment">#返回一个Reader对象，按行读取</span></span><br><span class="line">fileData = list(fileReader)                               <span class="comment">#fileData[0][0]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> fileReader:</span><br><span class="line">    print(<span class="string">'row'</span>+str(fileReader.line_num)+str(row))        <span class="comment">#对于大型CSV文件，避免将文件一次性装入内存</span></span><br></pre></td></tr></table></figure><h3 id="Write对象"><a href="#Write对象" class="headerlink" title="Write对象"></a>Write对象</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">openFile = open(<span class="string">'test.csv'</span>, <span class="string">'w'</span>, newline=<span class="string">''</span>)              <span class="comment">#返回一个File对象</span></span><br><span class="line">fileWrite = csv.writer(openFile)                          <span class="comment">#返回一个Reader对象，按行读取</span></span><br><span class="line">fileWrite.writerow([<span class="number">1</span>, <span class="string">'a,b'</span>, <span class="number">2</span>, <span class="string">'b'</span>])</span><br><span class="line">openfile.close()</span><br><span class="line"></span><br><span class="line">fileWrite = csv.writer(openFile, delimiter=<span class="string">'\t'</span>, lineterminator=<span class="string">'\n\n'</span>)</span><br><span class="line"><span class="comment">#delimiter指定分隔符 lineterminator指定行终止符，默认\n</span></span><br></pre></td></tr></table></figure><h2 id="python处理JSON文件"><a href="#python处理JSON文件" class="headerlink" title="python处理JSON文件"></a>python处理JSON文件</h2><p>JSON文件，全称JavaScript Object Notation，是JavaScript程序编写数据结构的原生方式，<br>JSON不能表示Pyton特有的对象，可以存储：字符串、整型、浮点型、布尔型、列表、字典和NoneType。</p><h3 id="loads"><a href="#loads" class="headerlink" title="loads()"></a>loads()</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">jsonString = <span class="string">'&#123;"name":"asher", "isCat":true, "num":1&#125;'</span></span><br><span class="line">jsonData = json.loads(jsonString)                         <span class="comment">#返回一个Python字典</span></span><br></pre></td></tr></table></figure><h3 id="dumps"><a href="#dumps" class="headerlink" title="dumps()"></a>dumps()</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">pythonData = <span class="string">'&#123;'</span>name<span class="string">':'</span>ashe<span class="string">r', '</span>isCat<span class="string">':True, '</span>num<span class="string">':1&#125;'</span></span><br><span class="line">jsonString = json.dumps(json)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Excel</tag>
        <tag>CSV</tag>
        <tag>JSON</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式复习</title>
    <url>/post/tech/regex-review/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>正则表达式复习<a id="more"></a></p><h2 id="常用匹配规则"><a href="#常用匹配规则" class="headerlink" title="常用匹配规则"></a>常用匹配规则</h2><p><img src="https://s3.ax1x.com/2021/01/10/sl3TEQ.png" alt=""></p><h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="match"><a href="#match" class="headerlink" title="match()"></a>match()</h3><p>从字符串的开头开始匹配，不匹配则失败。</p><ul><li>贪婪与非贪婪<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re  </span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span>  </span><br><span class="line"></span><br><span class="line"><span class="comment">#.*匹配任意个字符（除了换行符），默认贪心，加上?为非贪心，一般用非贪心.*?</span></span><br><span class="line"><span class="comment">#但要注意在末尾匹配任意字符时，应用贪心匹配.*，否则匹配不到任何内容</span></span><br><span class="line">result = re.match(<span class="string">'^He.*?(\d+).*Demo$'</span>, content)</span><br><span class="line">result1 = re.match(<span class="string">'http.*?comment/(.*?)'</span>, content)  </span><br><span class="line"></span><br><span class="line">print(result)</span><br><span class="line">print(result.group(<span class="number">1</span>))  <span class="comment">#提取第一个括号内匹配的内容</span></span><br><span class="line">print(result.span())    <span class="comment">#匹配到的范围</span></span><br><span class="line">print(<span class="string">'result1'</span>, result1.group(<span class="number">1</span>))  </span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">&lt;re.Match object; span=(<span class="number">0</span>, <span class="number">40</span>), match=<span class="string">'Hello 1234567 World_This is a Regex Demo'</span>&gt;</span><br><span class="line"><span class="number">1234567</span></span><br><span class="line">(<span class="number">0</span>, <span class="number">40</span>)</span><br><span class="line">result1</span><br></pre></td></tr></table></figure></li><li>修饰符<br><code>result = re.match(regex, content, re.S | re.I)</code><br><code>re.S</code>和<code>re.I</code>十分常用。<br><code>re.DOTALL</code>可以匹配换行符。<br><code>re.VERBOSE</code>忽略regex中的空白符和注释。<br><img src="https://s3.ax1x.com/2021/01/10/slBzqA.png" alt=""></li><li>转义匹配<br>在前面加上反斜线，如<code>\.</code>匹配<code>.</code></li></ul><h3 id="search"><a href="#search" class="headerlink" title="search()"></a>search()</h3><p>返回第一个匹配成功的结果。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">result = re.search(<span class="string">'^He.*?(\d+).*Demo$'</span>, content, re.S)</span><br></pre></td></tr></table></figure><h3 id="findall"><a href="#findall" class="headerlink" title="findall()"></a>findall()</h3><p>返回匹配正则表达式的所有内容。<br>返回结果为列表类型，列表中的元素为元组类型。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">results = re.findall(<span class="string">'&lt;li.*?href="(.*?)".*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>, html, re.S)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br><span class="line">    print(result[<span class="number">0</span>], result[<span class="number">1</span>], result[<span class="number">2</span>])</span><br></pre></td></tr></table></figure><h3 id="sub"><a href="#sub" class="headerlink" title="sub()"></a>sub()</h3><p>用来修改文本。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'54aK54yr5oiR54ix5L2g'</span></span><br><span class="line">content = re.sub(<span class="string">'\d+'</span>, <span class="string">''</span>, content)</span><br><span class="line">print(content)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">aKyroiRixLg</span><br></pre></td></tr></table></figure><h3 id="compile"><a href="#compile" class="headerlink" title="compile()"></a>compile()</h3><p>将正则字符串编译成正则表达式对象，以便日后复用。</p><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pattern = re.compile(<span class="string">'\d&#123;2&#125;:\d&#123;2&#125;'</span>)</span><br><span class="line">result1 = re.sub(pattern, <span class="string">''</span>, content)</span><br></pre></td></tr></table></figure><h2 id="常用Regex"><a href="#常用Regex" class="headerlink" title="常用Regex"></a>常用Regex</h2><h3 id="email"><a href="#email" class="headerlink" title="email"></a>email</h3><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">emailRegex</span> = re.compile(r<span class="string">'''(</span></span><br><span class="line"><span class="string">    [a-zA-Z0-9._%+-]+</span></span><br><span class="line"><span class="string">    @</span></span><br><span class="line"><span class="string">    [a-zA-Z0-9.-]+</span></span><br><span class="line"><span class="string">    \.[a-zA-Z]&#123;2,4&#125;</span></span><br><span class="line"><span class="string">    )'''</span>, re.VERBOSE)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>正则</tag>
        <tag>Regex</tag>
      </tags>
  </entry>
  <entry>
    <title>Python知识点记录</title>
    <url>/post/Python/Python-basis/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>记录使用python遇到的一些工具和问题</p><a id="more"></a><p>学习Python之前，最好熟悉一下Python的语言和风格规范<br><a href="https://google-styleguide.readthedocs.io/zh_CN/latest/google-python-styleguide/contents.html" target="_blank" rel="noopener">https://google-styleguide.readthedocs.io/zh_CN/latest/google-python-styleguide/contents.html</a><br>原项目：<a href="https://github.com/google/styleguide" target="_blank" rel="noopener">https://github.com/google/styleguide</a><br>《PythonCookBook》：<br><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/index.html" target="_blank" rel="noopener">https://python3-cookbook.readthedocs.io/zh_CN/latest/index.html</a></p><h2 id="一些包和工具"><a href="#一些包和工具" class="headerlink" title="一些包和工具"></a>一些包和工具</h2><h3 id="虚拟环境（pipenv）"><a href="#虚拟环境（pipenv）" class="headerlink" title="虚拟环境（pipenv）"></a>虚拟环境（pipenv）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install pipenv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显式激活虚拟环境</span></span><br><span class="line">pipenv shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不显示激活虚拟环境而直接执行</span></span><br><span class="line">pipenv run python hello.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前环境下的依赖情况</span></span><br><span class="line">pipenv graph</span><br><span class="line">pip list    <span class="comment"># 在虚拟环境中使用查看依赖列表。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成requirements.txt文件</span></span><br><span class="line">pip freeze &gt;requirements.txt    <span class="comment"># 生成本地全部依赖，适用于虚拟环境</span></span><br><span class="line"></span><br><span class="line">pip install pipreqs</span><br><span class="line">pipreqs ./ --encoding=utf8  <span class="comment"># 只生成该目录内的依赖</span></span><br><span class="line"></span><br><span class="line">pipenv lock -r &gt; requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行requirement.txt</span></span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 Pipfile.lock 中指定的所有包。</span></span><br><span class="line"><span class="comment"># （适用于测试环境等，因为使用 pipenv install 会更新 Pipfile.lock，可能导致版本不一致）</span></span><br><span class="line">pipenv sync</span><br></pre></td></tr></table></figure><ul><li>默认情况下，Pipenv会统一管理所有虚拟环境。在Windows系统中，虚拟环境文件夹会在<code>C:\Users\Administrator\.virtualenvs\</code>目录下创建，而Linux或macOS会在<code>~/.local/share/virtualenvs/</code>目录下创建。</li><li>如果想在项目目录内创建虚拟环境文件夹.venv，有如下三种方法：<ul><li>linux设置环境变量<code>export PIPENV_VENV_IN_PROJECT=1</code></li><li>自己在项目目录下手动创建.venv的目录，然后运行<code>pipenv run</code>或者<code>pipenv shell pipenv</code>都会在.venv下创建虚拟环境。</li><li>设置WORKON_HOME到其他的地方 （如果当前目录下已经有.venv,此项设置失效）</li></ul></li><li>虚拟环境文件夹的目录名称的形式为“当前项目目录名+一串随机字符，比如helloflask-5Pa0ZfZw。</li></ul><h2 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h2><p>在写查询元素的代码时，通常会使用包含 yield 表达式的生成器函数。这样可以将搜索过程代码和使用搜索结果代码解耦。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="批量有序重命名文件"><a href="#批量有序重命名文件" class="headerlink" title="批量有序重命名文件"></a>批量有序重命名文件</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os,re</span><br><span class="line">num = <span class="number">1</span></span><br><span class="line">path = <span class="string">'C:\\Users\\Desktop\\test\\'</span></span><br><span class="line">prefix = <span class="string">'picture'</span></span><br><span class="line">suffix = <span class="string">'.py'</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(path):</span><br><span class="line">    fileregex = re.compile(<span class="string">r'\.\w*'</span>)                    <span class="comment">#取得后缀名</span></span><br><span class="line">    extension = <span class="string">''</span>.join(fileregex.findall(file)[<span class="number">-1</span>:])   <span class="comment">#列表转换为字符串</span></span><br><span class="line">    <span class="keyword">if</span> suffix:</span><br><span class="line">        newName = prefix+str(num)+suffix</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        newName = prefix+str(num)+extension</span><br><span class="line">    <span class="keyword">if</span> extension == <span class="string">''</span>:</span><br><span class="line">        <span class="keyword">continue</span>                                        <span class="comment">#过滤掉文件夹</span></span><br><span class="line">    os.rename(path+file, path+newName)</span><br><span class="line">    print(file+<span class="string">' to '</span>+newName+<span class="string">' rename succeed!'</span>)</span><br><span class="line">    num = num+<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo-NexT解决点进文章自动下滑到&lt;!-- more --&gt;后面</title>
    <url>/post/Hexo/hexo_issues/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Hexo 版本v4.0.0 | 主题 – NexT.Gemini v7.6.0<br>若使用<code>&lt;!-- more --&gt;</code>标签来截取文章概要，当点击全文时，文章链接后会加上#more使页面自动下滑到<code>&lt;!-- more --&gt;</code>的后面<a id="more"></a><br>不想使用这一特性，有以下2个办法<br>1.找到<code>themes\next\layout\_macro</code>下的post.swig文件，将</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"btn"</span> <span class="attr">href</span>=<span class="string">"&#123;&#123; url_for(post.path) &#125;&#125;#more"</span> <span class="attr">rel</span>=<span class="string">"contents"</span>&gt;</span></span><br></pre></td></tr></table></figure><p>里的#more删除，即</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"btn"</span> <span class="attr">href</span>=<span class="string">"&#123;&#123; url_for(post.path) &#125;&#125;"</span> <span class="attr">rel</span>=<span class="string">"contents"</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2.将主题配置文件中的<code>scroll_to_more: true</code>改为<code>scroll_to_more: false</code>这个方法对我没用，可能NexT在新版本中移除了这项配置<br>其他问题可以参考<a href="https://github.com/theme-next/hexo-theme-next/issues" target="_blank" rel="noopener">https://github.com/theme-next/hexo-theme-next/issues</a>以及官方文档<a href="https://hexo-theme-next.netlify.app/" target="_blank" rel="noopener">https://hexo-theme-next.netlify.app/</a></p>]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>死亡日记</title>
    <url>/post/Life/daliy/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">输入密码，查看文章</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="e7786f9bac6e8650951f22dd7fd09d22f9c7b9761a2eeca8c397778bdcc86fb3">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
</search>
